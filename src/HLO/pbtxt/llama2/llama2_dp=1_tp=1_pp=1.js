const text = `
HloModule SyncTensorsGraph.6428, input_output_alias={ {0}: (3, {}, must-alias), {1}: (10, {}, must-alias), {2}: (9, {}, must-alias), {3}: (6, {}, must-alias), {4}: (5, {}, must-alias), {5}: (13, {}, must-alias), {6}: (12, {}, must-alias), {7}: (4, {}, must-alias), {8}: (11, {}, must-alias), {9}: (18, {}, must-alias), {10}: (17, {}, must-alias), {11}: (16, {}, must-alias), {12}: (15, {}, must-alias), {13}: (21, {}, must-alias), {14}: (20, {}, must-alias), {15}: (14, {}, must-alias), {16}: (19, {}, must-alias), {17}: (26, {}, must-alias), {18}: (25, {}, must-alias), {19}: (24, {}, must-alias), {20}: (23, {}, must-alias), {21}: (29, {}, must-alias), {22}: (28, {}, must-alias), {23}: (22, {}, must-alias), {24}: (27, {}, must-alias), {25}: (34, {}, must-alias), {26}: (33, {}, must-alias), {27}: (32, {}, must-alias), {28}: (31, {}, must-alias), {29}: (42, {}, must-alias), {30}: (41, {}, must-alias), {31}: (30, {}, must-alias), {32}: (35, {}, must-alias), {33}: (43, {}, must-alias), {34}: (48, {}, must-alias), {35}: (87, {}, must-alias), {36}: (90, {}, must-alias), {37}: (93, {}, must-alias), {38}: (96, {}, must-alias), {39}: (99, {}, must-alias), {40}: (102, {}, must-alias), {41}: (105, {}, must-alias), {42}: (108, {}, must-alias), {43}: (111, {}, must-alias), {44}: (114, {}, must-alias), {45}: (117, {}, must-alias), {46}: (120, {}, must-alias), {47}: (123, {}, must-alias), {48}: (126, {}, must-alias), {49}: (129, {}, must-alias), {50}: (132, {}, must-alias), {51}: (135, {}, must-alias), {52}: (138, {}, must-alias), {53}: (141, {}, must-alias), {54}: (144, {}, must-alias), {55}: (147, {}, must-alias), {56}: (150, {}, must-alias), {57}: (153, {}, must-alias), {58}: (156, {}, must-alias), {59}: (159, {}, must-alias), {60}: (162, {}, must-alias), {61}: (165, {}, must-alias), {62}: (49, {}, must-alias), {63}: (70, {}, must-alias), {64}: (53, {}, must-alias), {65}: (54, {}, must-alias), {66}: (71, {}, must-alias), {67}: (55, {}, must-alias), {68}: (56, {}, must-alias), {69}: (72, {}, must-alias), {70}: (57, {}, must-alias), {71}: (58, {}, must-alias), {72}: (73, {}, must-alias), {73}: (59, {}, must-alias), {74}: (60, {}, must-alias), {75}: (74, {}, must-alias), {76}: (61, {}, must-alias), {77}: (62, {}, must-alias), {78}: (75, {}, must-alias), {79}: (63, {}, must-alias), {80}: (64, {}, must-alias), {81}: (76, {}, must-alias), {82}: (65, {}, must-alias), {83}: (66, {}, must-alias), {84}: (77, {}, must-alias), {85}: (67, {}, must-alias), {86}: (68, {}, must-alias), {87}: (78, {}, must-alias), {88}: (69, {}, must-alias), {89}: (84, {}, must-alias), {90}: (81, {}, must-alias), {91}: (89, {}, must-alias), {92}: (88, {}, must-alias), {93}: (92, {}, must-alias), {94}: (91, {}, must-alias), {95}: (95, {}, must-alias), {96}: (94, {}, must-alias), {97}: (98, {}, must-alias), {98}: (97, {}, must-alias), {99}: (101, {}, must-alias), {100}: (100, {}, must-alias), {101}: (104, {}, must-alias), {102}: (103, {}, must-alias), {103}: (107, {}, must-alias), {104}: (106, {}, must-alias), {105}: (110, {}, must-alias), {106}: (109, {}, must-alias), {107}: (113, {}, must-alias), {108}: (112, {}, must-alias), {109}: (116, {}, must-alias), {110}: (115, {}, must-alias), {111}: (119, {}, must-alias), {112}: (118, {}, must-alias), {113}: (122, {}, must-alias), {114}: (121, {}, must-alias), {115}: (125, {}, must-alias), {116}: (124, {}, must-alias), {117}: (128, {}, must-alias), {118}: (127, {}, must-alias), {119}: (131, {}, must-alias), {120}: (130, {}, must-alias), {121}: (134, {}, must-alias), {122}: (133, {}, must-alias), {123}: (137, {}, must-alias), {124}: (136, {}, must-alias), {125}: (140, {}, must-alias), {126}: (139, {}, must-alias), {127}: (143, {}, must-alias), {128}: (142, {}, must-alias), {129}: (146, {}, must-alias), {130}: (145, {}, must-alias), {131}: (149, {}, must-alias), {132}: (148, {}, must-alias), {133}: (152, {}, must-alias), {134}: (151, {}, must-alias), {135}: (155, {}, must-alias), {136}: (154, {}, must-alias), {137}: (158, {}, must-alias), {138}: (157, {}, must-alias), {139}: (161, {}, must-alias), {140}: (160, {}, must-alias), {141}: (164, {}, must-alias), {142}: (163, {}, must-alias), {143}: (2, {}, must-alias), {144}: (166, {}, must-alias), {145}: (46, {}, must-alias) }

%AddComputation.21 (x.22: f32[], y.23: f32[]) -> f32[] {
  %x.22 = f32[] parameter(0)
  %y.23 = f32[] parameter(1)
  ROOT %add.24 = f32[] add(f32[] %x.22, f32[] %y.23)
}

%AddComputation.254 (x.255: f32[], y.256: f32[]) -> f32[] {
  %x.255 = f32[] parameter(0)
  %y.256 = f32[] parameter(1)
  ROOT %add.257 = f32[] add(f32[] %x.255, f32[] %y.256)
}

%AddComputation.323 (x.324: f32[], y.325: f32[]) -> f32[] {
  %x.324 = f32[] parameter(0)
  %y.325 = f32[] parameter(1)
  ROOT %add.326 = f32[] add(f32[] %x.324, f32[] %y.325)
}

%AddComputation.552 (x.553: f32[], y.554: f32[]) -> f32[] {
  %x.553 = f32[] parameter(0)
  %y.554 = f32[] parameter(1)
  ROOT %add.555 = f32[] add(f32[] %x.553, f32[] %y.554)
}

%AddComputation.621 (x.622: f32[], y.623: f32[]) -> f32[] {
  %x.622 = f32[] parameter(0)
  %y.623 = f32[] parameter(1)
  ROOT %add.624 = f32[] add(f32[] %x.622, f32[] %y.623)
}

%AddComputation.850 (x.851: f32[], y.852: f32[]) -> f32[] {
  %x.851 = f32[] parameter(0)
  %y.852 = f32[] parameter(1)
  ROOT %add.853 = f32[] add(f32[] %x.851, f32[] %y.852)
}

%AddComputation.919 (x.920: f32[], y.921: f32[]) -> f32[] {
  %x.920 = f32[] parameter(0)
  %y.921 = f32[] parameter(1)
  ROOT %add.922 = f32[] add(f32[] %x.920, f32[] %y.921)
}

%AddComputation.1148 (x.1149: f32[], y.1150: f32[]) -> f32[] {
  %x.1149 = f32[] parameter(0)
  %y.1150 = f32[] parameter(1)
  ROOT %add.1151 = f32[] add(f32[] %x.1149, f32[] %y.1150)
}

%AddComputation.1304 (x.1305: f32[], y.1306: f32[]) -> f32[] {
  %x.1305 = f32[] parameter(0)
  %y.1306 = f32[] parameter(1)
  ROOT %add.1307 = f32[] add(f32[] %x.1305, f32[] %y.1306)
}

%MaxComputation.1411 (x.1412: f32[], y.1413: f32[]) -> f32[] {
  %x.1412 = f32[] parameter(0)
  %y.1413 = f32[] parameter(1)
  ROOT %maximum.1414 = f32[] maximum(f32[] %x.1412, f32[] %y.1413)
}

%AddComputation.1455 (x.1456: f32[], y.1457: f32[]) -> f32[] {
  %x.1456 = f32[] parameter(0)
  %y.1457 = f32[] parameter(1)
  ROOT %add.1458 = f32[] add(f32[] %x.1456, f32[] %y.1457)
}

%ScatterCombiner.1489 (p0.1490: f32[], p1.1491: f32[]) -> f32[] {
  %p0.1490 = f32[] parameter(0)
  ROOT %p1.1491 = f32[] parameter(1)
}

%AddComputation.1527 (x.1528: bf16[], y.1529: bf16[]) -> bf16[] {
  %x.1528 = bf16[] parameter(0)
  %y.1529 = bf16[] parameter(1)
  ROOT %add.1530 = bf16[] add(bf16[] %x.1528, bf16[] %y.1529)
}

%AddComputation.1538 (x.1539: bf16[], y.1540: bf16[]) -> bf16[] {
  %x.1539 = bf16[] parameter(0)
  %y.1540 = bf16[] parameter(1)
  ROOT %add.1541 = bf16[] add(bf16[] %x.1539, bf16[] %y.1540)
}

%AddComputation.1593 (x.1594: f32[], y.1595: f32[]) -> f32[] {
  %x.1594 = f32[] parameter(0)
  %y.1595 = f32[] parameter(1)
  ROOT %add.1596 = f32[] add(f32[] %x.1594, f32[] %y.1595)
}

%AddComputation.1624 (x.1625: bf16[], y.1626: bf16[]) -> bf16[] {
  %x.1625 = bf16[] parameter(0)
  %y.1626 = bf16[] parameter(1)
  ROOT %add.1627 = bf16[] add(bf16[] %x.1625, bf16[] %y.1626)
}

%AddComputation.1635 (x.1636: bf16[], y.1637: bf16[]) -> bf16[] {
  %x.1636 = bf16[] parameter(0)
  %y.1637 = bf16[] parameter(1)
  ROOT %add.1638 = bf16[] add(bf16[] %x.1636, bf16[] %y.1637)
}

%AddComputation.1707 (x.1708: bf16[], y.1709: bf16[]) -> bf16[] {
  %x.1708 = bf16[] parameter(0)
  %y.1709 = bf16[] parameter(1)
  ROOT %add.1710 = bf16[] add(bf16[] %x.1708, bf16[] %y.1709)
}

%AddComputation.1718 (x.1719: bf16[], y.1720: bf16[]) -> bf16[] {
  %x.1719 = bf16[] parameter(0)
  %y.1720 = bf16[] parameter(1)
  ROOT %add.1721 = bf16[] add(bf16[] %x.1719, bf16[] %y.1720)
}

%AddComputation.1775 (x.1776: bf16[], y.1777: bf16[]) -> bf16[] {
  %x.1776 = bf16[] parameter(0)
  %y.1777 = bf16[] parameter(1)
  ROOT %add.1778 = bf16[] add(bf16[] %x.1776, bf16[] %y.1777)
}

%AddComputation.1789 (x.1790: f32[], y.1791: f32[]) -> f32[] {
  %x.1790 = f32[] parameter(0)
  %y.1791 = f32[] parameter(1)
  ROOT %add.1792 = f32[] add(f32[] %x.1790, f32[] %y.1791)
}

%AddComputation.1822 (x.1823: bf16[], y.1824: bf16[]) -> bf16[] {
  %x.1823 = bf16[] parameter(0)
  %y.1824 = bf16[] parameter(1)
  ROOT %add.1825 = bf16[] add(bf16[] %x.1823, bf16[] %y.1824)
}

%AddComputation.1833 (x.1834: bf16[], y.1835: bf16[]) -> bf16[] {
  %x.1834 = bf16[] parameter(0)
  %y.1835 = bf16[] parameter(1)
  ROOT %add.1836 = bf16[] add(bf16[] %x.1834, bf16[] %y.1835)
}

%AddComputation.1978 (x.1979: bf16[], y.1980: bf16[]) -> bf16[] {
  %x.1979 = bf16[] parameter(0)
  %y.1980 = bf16[] parameter(1)
  ROOT %add.1981 = bf16[] add(bf16[] %x.1979, bf16[] %y.1980)
}

%AddComputation.1996 (x.1997: bf16[], y.1998: bf16[]) -> bf16[] {
  %x.1997 = bf16[] parameter(0)
  %y.1998 = bf16[] parameter(1)
  ROOT %add.1999 = bf16[] add(bf16[] %x.1997, bf16[] %y.1998)
}

%AddComputation.2042 (x.2043: bf16[], y.2044: bf16[]) -> bf16[] {
  %x.2043 = bf16[] parameter(0)
  %y.2044 = bf16[] parameter(1)
  ROOT %add.2045 = bf16[] add(bf16[] %x.2043, bf16[] %y.2044)
}

%AddComputation.2060 (x.2061: bf16[], y.2062: bf16[]) -> bf16[] {
  %x.2061 = bf16[] parameter(0)
  %y.2062 = bf16[] parameter(1)
  ROOT %add.2063 = bf16[] add(bf16[] %x.2061, bf16[] %y.2062)
}

%AddComputation.2102 (x.2103: bf16[], y.2104: bf16[]) -> bf16[] {
  %x.2103 = bf16[] parameter(0)
  %y.2104 = bf16[] parameter(1)
  ROOT %add.2105 = bf16[] add(bf16[] %x.2103, bf16[] %y.2104)
}

%AddComputation.2113 (x.2114: bf16[], y.2115: bf16[]) -> bf16[] {
  %x.2114 = bf16[] parameter(0)
  %y.2115 = bf16[] parameter(1)
  ROOT %add.2116 = bf16[] add(bf16[] %x.2114, bf16[] %y.2115)
}

%AddComputation.2167 (x.2168: bf16[], y.2169: bf16[]) -> bf16[] {
  %x.2168 = bf16[] parameter(0)
  %y.2169 = bf16[] parameter(1)
  ROOT %add.2170 = bf16[] add(bf16[] %x.2168, bf16[] %y.2169)
}

%AddComputation.2181 (x.2182: f32[], y.2183: f32[]) -> f32[] {
  %x.2182 = f32[] parameter(0)
  %y.2183 = f32[] parameter(1)
  ROOT %add.2184 = f32[] add(f32[] %x.2182, f32[] %y.2183)
}

%AddComputation.2214 (x.2215: bf16[], y.2216: bf16[]) -> bf16[] {
  %x.2215 = bf16[] parameter(0)
  %y.2216 = bf16[] parameter(1)
  ROOT %add.2217 = bf16[] add(bf16[] %x.2215, bf16[] %y.2216)
}

%AddComputation.2225 (x.2226: bf16[], y.2227: bf16[]) -> bf16[] {
  %x.2226 = bf16[] parameter(0)
  %y.2227 = bf16[] parameter(1)
  ROOT %add.2228 = bf16[] add(bf16[] %x.2226, bf16[] %y.2227)
}

%AddComputation.2297 (x.2298: bf16[], y.2299: bf16[]) -> bf16[] {
  %x.2298 = bf16[] parameter(0)
  %y.2299 = bf16[] parameter(1)
  ROOT %add.2300 = bf16[] add(bf16[] %x.2298, bf16[] %y.2299)
}

%AddComputation.2308 (x.2309: bf16[], y.2310: bf16[]) -> bf16[] {
  %x.2309 = bf16[] parameter(0)
  %y.2310 = bf16[] parameter(1)
  ROOT %add.2311 = bf16[] add(bf16[] %x.2309, bf16[] %y.2310)
}

%AddComputation.2365 (x.2366: bf16[], y.2367: bf16[]) -> bf16[] {
  %x.2366 = bf16[] parameter(0)
  %y.2367 = bf16[] parameter(1)
  ROOT %add.2368 = bf16[] add(bf16[] %x.2366, bf16[] %y.2367)
}

%AddComputation.2379 (x.2380: f32[], y.2381: f32[]) -> f32[] {
  %x.2380 = f32[] parameter(0)
  %y.2381 = f32[] parameter(1)
  ROOT %add.2382 = f32[] add(f32[] %x.2380, f32[] %y.2381)
}

%AddComputation.2412 (x.2413: bf16[], y.2414: bf16[]) -> bf16[] {
  %x.2413 = bf16[] parameter(0)
  %y.2414 = bf16[] parameter(1)
  ROOT %add.2415 = bf16[] add(bf16[] %x.2413, bf16[] %y.2414)
}

%AddComputation.2423 (x.2424: bf16[], y.2425: bf16[]) -> bf16[] {
  %x.2424 = bf16[] parameter(0)
  %y.2425 = bf16[] parameter(1)
  ROOT %add.2426 = bf16[] add(bf16[] %x.2424, bf16[] %y.2425)
}

%AddComputation.2568 (x.2569: bf16[], y.2570: bf16[]) -> bf16[] {
  %x.2569 = bf16[] parameter(0)
  %y.2570 = bf16[] parameter(1)
  ROOT %add.2571 = bf16[] add(bf16[] %x.2569, bf16[] %y.2570)
}

%AddComputation.2586 (x.2587: bf16[], y.2588: bf16[]) -> bf16[] {
  %x.2587 = bf16[] parameter(0)
  %y.2588 = bf16[] parameter(1)
  ROOT %add.2589 = bf16[] add(bf16[] %x.2587, bf16[] %y.2588)
}

%AddComputation.2632 (x.2633: bf16[], y.2634: bf16[]) -> bf16[] {
  %x.2633 = bf16[] parameter(0)
  %y.2634 = bf16[] parameter(1)
  ROOT %add.2635 = bf16[] add(bf16[] %x.2633, bf16[] %y.2634)
}

%AddComputation.2650 (x.2651: bf16[], y.2652: bf16[]) -> bf16[] {
  %x.2651 = bf16[] parameter(0)
  %y.2652 = bf16[] parameter(1)
  ROOT %add.2653 = bf16[] add(bf16[] %x.2651, bf16[] %y.2652)
}

%AddComputation.2692 (x.2693: bf16[], y.2694: bf16[]) -> bf16[] {
  %x.2693 = bf16[] parameter(0)
  %y.2694 = bf16[] parameter(1)
  ROOT %add.2695 = bf16[] add(bf16[] %x.2693, bf16[] %y.2694)
}

%AddComputation.2703 (x.2704: bf16[], y.2705: bf16[]) -> bf16[] {
  %x.2704 = bf16[] parameter(0)
  %y.2705 = bf16[] parameter(1)
  ROOT %add.2706 = bf16[] add(bf16[] %x.2704, bf16[] %y.2705)
}

%AddComputation.2757 (x.2758: bf16[], y.2759: bf16[]) -> bf16[] {
  %x.2758 = bf16[] parameter(0)
  %y.2759 = bf16[] parameter(1)
  ROOT %add.2760 = bf16[] add(bf16[] %x.2758, bf16[] %y.2759)
}

%AddComputation.2771 (x.2772: f32[], y.2773: f32[]) -> f32[] {
  %x.2772 = f32[] parameter(0)
  %y.2773 = f32[] parameter(1)
  ROOT %add.2774 = f32[] add(f32[] %x.2772, f32[] %y.2773)
}

%AddComputation.2804 (x.2805: bf16[], y.2806: bf16[]) -> bf16[] {
  %x.2805 = bf16[] parameter(0)
  %y.2806 = bf16[] parameter(1)
  ROOT %add.2807 = bf16[] add(bf16[] %x.2805, bf16[] %y.2806)
}

%AddComputation.2815 (x.2816: bf16[], y.2817: bf16[]) -> bf16[] {
  %x.2816 = bf16[] parameter(0)
  %y.2817 = bf16[] parameter(1)
  ROOT %add.2818 = bf16[] add(bf16[] %x.2816, bf16[] %y.2817)
}

%AddComputation.2887 (x.2888: bf16[], y.2889: bf16[]) -> bf16[] {
  %x.2888 = bf16[] parameter(0)
  %y.2889 = bf16[] parameter(1)
  ROOT %add.2890 = bf16[] add(bf16[] %x.2888, bf16[] %y.2889)
}

%AddComputation.2898 (x.2899: bf16[], y.2900: bf16[]) -> bf16[] {
  %x.2899 = bf16[] parameter(0)
  %y.2900 = bf16[] parameter(1)
  ROOT %add.2901 = bf16[] add(bf16[] %x.2899, bf16[] %y.2900)
}

%AddComputation.2955 (x.2956: bf16[], y.2957: bf16[]) -> bf16[] {
  %x.2956 = bf16[] parameter(0)
  %y.2957 = bf16[] parameter(1)
  ROOT %add.2958 = bf16[] add(bf16[] %x.2956, bf16[] %y.2957)
}

%AddComputation.2969 (x.2970: f32[], y.2971: f32[]) -> f32[] {
  %x.2970 = f32[] parameter(0)
  %y.2971 = f32[] parameter(1)
  ROOT %add.2972 = f32[] add(f32[] %x.2970, f32[] %y.2971)
}

%AddComputation.3002 (x.3003: bf16[], y.3004: bf16[]) -> bf16[] {
  %x.3003 = bf16[] parameter(0)
  %y.3004 = bf16[] parameter(1)
  ROOT %add.3005 = bf16[] add(bf16[] %x.3003, bf16[] %y.3004)
}

%AddComputation.3013 (x.3014: bf16[], y.3015: bf16[]) -> bf16[] {
  %x.3014 = bf16[] parameter(0)
  %y.3015 = bf16[] parameter(1)
  ROOT %add.3016 = bf16[] add(bf16[] %x.3014, bf16[] %y.3015)
}

%AddComputation.3158 (x.3159: bf16[], y.3160: bf16[]) -> bf16[] {
  %x.3159 = bf16[] parameter(0)
  %y.3160 = bf16[] parameter(1)
  ROOT %add.3161 = bf16[] add(bf16[] %x.3159, bf16[] %y.3160)
}

%AddComputation.3176 (x.3177: bf16[], y.3178: bf16[]) -> bf16[] {
  %x.3177 = bf16[] parameter(0)
  %y.3178 = bf16[] parameter(1)
  ROOT %add.3179 = bf16[] add(bf16[] %x.3177, bf16[] %y.3178)
}

%AddComputation.3222 (x.3223: bf16[], y.3224: bf16[]) -> bf16[] {
  %x.3223 = bf16[] parameter(0)
  %y.3224 = bf16[] parameter(1)
  ROOT %add.3225 = bf16[] add(bf16[] %x.3223, bf16[] %y.3224)
}

%AddComputation.3240 (x.3241: bf16[], y.3242: bf16[]) -> bf16[] {
  %x.3241 = bf16[] parameter(0)
  %y.3242 = bf16[] parameter(1)
  ROOT %add.3243 = bf16[] add(bf16[] %x.3241, bf16[] %y.3242)
}

%AddComputation.3282 (x.3283: bf16[], y.3284: bf16[]) -> bf16[] {
  %x.3283 = bf16[] parameter(0)
  %y.3284 = bf16[] parameter(1)
  ROOT %add.3285 = bf16[] add(bf16[] %x.3283, bf16[] %y.3284)
}

%AddComputation.3293 (x.3294: bf16[], y.3295: bf16[]) -> bf16[] {
  %x.3294 = bf16[] parameter(0)
  %y.3295 = bf16[] parameter(1)
  ROOT %add.3296 = bf16[] add(bf16[] %x.3294, bf16[] %y.3295)
}

%AddComputation.3347 (x.3348: bf16[], y.3349: bf16[]) -> bf16[] {
  %x.3348 = bf16[] parameter(0)
  %y.3349 = bf16[] parameter(1)
  ROOT %add.3350 = bf16[] add(bf16[] %x.3348, bf16[] %y.3349)
}

%AddComputation.3361 (x.3362: f32[], y.3363: f32[]) -> f32[] {
  %x.3362 = f32[] parameter(0)
  %y.3363 = f32[] parameter(1)
  ROOT %add.3364 = f32[] add(f32[] %x.3362, f32[] %y.3363)
}

%AddComputation.3394 (x.3395: bf16[], y.3396: bf16[]) -> bf16[] {
  %x.3395 = bf16[] parameter(0)
  %y.3396 = bf16[] parameter(1)
  ROOT %add.3397 = bf16[] add(bf16[] %x.3395, bf16[] %y.3396)
}

%AddComputation.3405 (x.3406: bf16[], y.3407: bf16[]) -> bf16[] {
  %x.3406 = bf16[] parameter(0)
  %y.3407 = bf16[] parameter(1)
  ROOT %add.3408 = bf16[] add(bf16[] %x.3406, bf16[] %y.3407)
}

%AddComputation.3477 (x.3478: bf16[], y.3479: bf16[]) -> bf16[] {
  %x.3478 = bf16[] parameter(0)
  %y.3479 = bf16[] parameter(1)
  ROOT %add.3480 = bf16[] add(bf16[] %x.3478, bf16[] %y.3479)
}

%AddComputation.3488 (x.3489: bf16[], y.3490: bf16[]) -> bf16[] {
  %x.3489 = bf16[] parameter(0)
  %y.3490 = bf16[] parameter(1)
  ROOT %add.3491 = bf16[] add(bf16[] %x.3489, bf16[] %y.3490)
}

%AddComputation.3545 (x.3546: bf16[], y.3547: bf16[]) -> bf16[] {
  %x.3546 = bf16[] parameter(0)
  %y.3547 = bf16[] parameter(1)
  ROOT %add.3548 = bf16[] add(bf16[] %x.3546, bf16[] %y.3547)
}

%AddComputation.3559 (x.3560: f32[], y.3561: f32[]) -> f32[] {
  %x.3560 = f32[] parameter(0)
  %y.3561 = f32[] parameter(1)
  ROOT %add.3562 = f32[] add(f32[] %x.3560, f32[] %y.3561)
}

%AddComputation.3592 (x.3593: bf16[], y.3594: bf16[]) -> bf16[] {
  %x.3593 = bf16[] parameter(0)
  %y.3594 = bf16[] parameter(1)
  ROOT %add.3595 = bf16[] add(bf16[] %x.3593, bf16[] %y.3594)
}

%AddComputation.3603 (x.3604: bf16[], y.3605: bf16[]) -> bf16[] {
  %x.3604 = bf16[] parameter(0)
  %y.3605 = bf16[] parameter(1)
  ROOT %add.3606 = bf16[] add(bf16[] %x.3604, bf16[] %y.3605)
}

%AddComputation.3748 (x.3749: bf16[], y.3750: bf16[]) -> bf16[] {
  %x.3749 = bf16[] parameter(0)
  %y.3750 = bf16[] parameter(1)
  ROOT %add.3751 = bf16[] add(bf16[] %x.3749, bf16[] %y.3750)
}

%AddComputation.3766 (x.3767: bf16[], y.3768: bf16[]) -> bf16[] {
  %x.3767 = bf16[] parameter(0)
  %y.3768 = bf16[] parameter(1)
  ROOT %add.3769 = bf16[] add(bf16[] %x.3767, bf16[] %y.3768)
}

%AddComputation.3812 (x.3813: bf16[], y.3814: bf16[]) -> bf16[] {
  %x.3813 = bf16[] parameter(0)
  %y.3814 = bf16[] parameter(1)
  ROOT %add.3815 = bf16[] add(bf16[] %x.3813, bf16[] %y.3814)
}

%AddComputation.3830 (x.3831: bf16[], y.3832: bf16[]) -> bf16[] {
  %x.3831 = bf16[] parameter(0)
  %y.3832 = bf16[] parameter(1)
  ROOT %add.3833 = bf16[] add(bf16[] %x.3831, bf16[] %y.3832)
}

%AddComputation.3872 (x.3873: bf16[], y.3874: bf16[]) -> bf16[] {
  %x.3873 = bf16[] parameter(0)
  %y.3874 = bf16[] parameter(1)
  ROOT %add.3875 = bf16[] add(bf16[] %x.3873, bf16[] %y.3874)
}

%AddComputation.3883 (x.3884: bf16[], y.3885: bf16[]) -> bf16[] {
  %x.3884 = bf16[] parameter(0)
  %y.3885 = bf16[] parameter(1)
  ROOT %add.3886 = bf16[] add(bf16[] %x.3884, bf16[] %y.3885)
}

%AddComputation.3941 (x.3942: bf16[], y.3943: bf16[]) -> bf16[] {
  %x.3942 = bf16[] parameter(0)
  %y.3943 = bf16[] parameter(1)
  ROOT %add.3944 = bf16[] add(bf16[] %x.3942, bf16[] %y.3943)
}

%AddComputation.3955 (x.3956: f32[], y.3957: f32[]) -> f32[] {
  %x.3956 = f32[] parameter(0)
  %y.3957 = f32[] parameter(1)
  ROOT %add.3958 = f32[] add(f32[] %x.3956, f32[] %y.3957)
}

%ScatterCombiner.4007 (p0.4008: bf16[], p1.4009: bf16[]) -> bf16[] {
  %p0.4008 = bf16[] parameter(0)
  %p1.4009 = bf16[] parameter(1)
  ROOT %add.4010 = bf16[] add(bf16[] %p0.4008, bf16[] %p1.4009)
}

%AddComputation.4020 (x.4021: bf16[], y.4022: bf16[]) -> bf16[] {
  %x.4021 = bf16[] parameter(0)
  %y.4022 = bf16[] parameter(1)
  ROOT %add.4023 = bf16[] add(bf16[] %x.4021, bf16[] %y.4022)
}

%AddComputation.4031 (x.4032: bf16[], y.4033: bf16[]) -> bf16[] {
  %x.4032 = bf16[] parameter(0)
  %y.4033 = bf16[] parameter(1)
  ROOT %add.4034 = bf16[] add(bf16[] %x.4032, bf16[] %y.4033)
}

%AddComputation.4050 (x.4051: bf16[], y.4052: bf16[]) -> bf16[] {
  %x.4051 = bf16[] parameter(0)
  %y.4052 = bf16[] parameter(1)
  ROOT %add.4053 = bf16[] add(bf16[] %x.4051, bf16[] %y.4052)
}

%AddComputation.4064 (x.4065: bf16[], y.4066: bf16[]) -> bf16[] {
  %x.4065 = bf16[] parameter(0)
  %y.4066 = bf16[] parameter(1)
  ROOT %add.4067 = bf16[] add(bf16[] %x.4065, bf16[] %y.4066)
}

%AddComputation.4075 (x.4076: bf16[], y.4077: bf16[]) -> bf16[] {
  %x.4076 = bf16[] parameter(0)
  %y.4077 = bf16[] parameter(1)
  ROOT %add.4078 = bf16[] add(bf16[] %x.4076, bf16[] %y.4077)
}

%AddComputation.4093 (x.4094: bf16[], y.4095: bf16[]) -> bf16[] {
  %x.4094 = bf16[] parameter(0)
  %y.4095 = bf16[] parameter(1)
  ROOT %add.4096 = bf16[] add(bf16[] %x.4094, bf16[] %y.4095)
}

%AddComputation.4107 (x.4108: bf16[], y.4109: bf16[]) -> bf16[] {
  %x.4108 = bf16[] parameter(0)
  %y.4109 = bf16[] parameter(1)
  ROOT %add.4110 = bf16[] add(bf16[] %x.4108, bf16[] %y.4109)
}

%AddComputation.4118 (x.4119: bf16[], y.4120: bf16[]) -> bf16[] {
  %x.4119 = bf16[] parameter(0)
  %y.4120 = bf16[] parameter(1)
  ROOT %add.4121 = bf16[] add(bf16[] %x.4119, bf16[] %y.4120)
}

%AddComputation.4136 (x.4137: bf16[], y.4138: bf16[]) -> bf16[] {
  %x.4137 = bf16[] parameter(0)
  %y.4138 = bf16[] parameter(1)
  ROOT %add.4139 = bf16[] add(bf16[] %x.4137, bf16[] %y.4138)
}

%AddComputation.4150 (x.4151: bf16[], y.4152: bf16[]) -> bf16[] {
  %x.4151 = bf16[] parameter(0)
  %y.4152 = bf16[] parameter(1)
  ROOT %add.4153 = bf16[] add(bf16[] %x.4151, bf16[] %y.4152)
}

%AddComputation.4161 (x.4162: bf16[], y.4163: bf16[]) -> bf16[] {
  %x.4162 = bf16[] parameter(0)
  %y.4163 = bf16[] parameter(1)
  ROOT %add.4164 = bf16[] add(bf16[] %x.4162, bf16[] %y.4163)
}

%AddComputation.4179 (x.4180: bf16[], y.4181: bf16[]) -> bf16[] {
  %x.4180 = bf16[] parameter(0)
  %y.4181 = bf16[] parameter(1)
  ROOT %add.4182 = bf16[] add(bf16[] %x.4180, bf16[] %y.4181)
}

%AddComputation.4193 (x.4194: bf16[], y.4195: bf16[]) -> bf16[] {
  %x.4194 = bf16[] parameter(0)
  %y.4195 = bf16[] parameter(1)
  ROOT %add.4196 = bf16[] add(bf16[] %x.4194, bf16[] %y.4195)
}

%AddComputation.4204 (x.4205: bf16[], y.4206: bf16[]) -> bf16[] {
  %x.4205 = bf16[] parameter(0)
  %y.4206 = bf16[] parameter(1)
  ROOT %add.4207 = bf16[] add(bf16[] %x.4205, bf16[] %y.4206)
}

%AddComputation.4222 (x.4223: bf16[], y.4224: bf16[]) -> bf16[] {
  %x.4223 = bf16[] parameter(0)
  %y.4224 = bf16[] parameter(1)
  ROOT %add.4225 = bf16[] add(bf16[] %x.4223, bf16[] %y.4224)
}

%AddComputation.4236 (x.4237: bf16[], y.4238: bf16[]) -> bf16[] {
  %x.4237 = bf16[] parameter(0)
  %y.4238 = bf16[] parameter(1)
  ROOT %add.4239 = bf16[] add(bf16[] %x.4237, bf16[] %y.4238)
}

%AddComputation.4247 (x.4248: bf16[], y.4249: bf16[]) -> bf16[] {
  %x.4248 = bf16[] parameter(0)
  %y.4249 = bf16[] parameter(1)
  ROOT %add.4250 = bf16[] add(bf16[] %x.4248, bf16[] %y.4249)
}

%AddComputation.4265 (x.4266: bf16[], y.4267: bf16[]) -> bf16[] {
  %x.4266 = bf16[] parameter(0)
  %y.4267 = bf16[] parameter(1)
  ROOT %add.4268 = bf16[] add(bf16[] %x.4266, bf16[] %y.4267)
}

%AddComputation.4279 (x.4280: bf16[], y.4281: bf16[]) -> bf16[] {
  %x.4280 = bf16[] parameter(0)
  %y.4281 = bf16[] parameter(1)
  ROOT %add.4282 = bf16[] add(bf16[] %x.4280, bf16[] %y.4281)
}

%AddComputation.4290 (x.4291: bf16[], y.4292: bf16[]) -> bf16[] {
  %x.4291 = bf16[] parameter(0)
  %y.4292 = bf16[] parameter(1)
  ROOT %add.4293 = bf16[] add(bf16[] %x.4291, bf16[] %y.4292)
}

%AddComputation.4308 (x.4309: bf16[], y.4310: bf16[]) -> bf16[] {
  %x.4309 = bf16[] parameter(0)
  %y.4310 = bf16[] parameter(1)
  ROOT %add.4311 = bf16[] add(bf16[] %x.4309, bf16[] %y.4310)
}

%AddComputation.4322 (x.4323: bf16[], y.4324: bf16[]) -> bf16[] {
  %x.4323 = bf16[] parameter(0)
  %y.4324 = bf16[] parameter(1)
  ROOT %add.4325 = bf16[] add(bf16[] %x.4323, bf16[] %y.4324)
}

%AddComputation.4333 (x.4334: bf16[], y.4335: bf16[]) -> bf16[] {
  %x.4334 = bf16[] parameter(0)
  %y.4335 = bf16[] parameter(1)
  ROOT %add.4336 = bf16[] add(bf16[] %x.4334, bf16[] %y.4335)
}

%AddComputation.4351 (x.4352: bf16[], y.4353: bf16[]) -> bf16[] {
  %x.4352 = bf16[] parameter(0)
  %y.4353 = bf16[] parameter(1)
  ROOT %add.4354 = bf16[] add(bf16[] %x.4352, bf16[] %y.4353)
}

%AddComputation.4365 (x.4366: bf16[], y.4367: bf16[]) -> bf16[] {
  %x.4366 = bf16[] parameter(0)
  %y.4367 = bf16[] parameter(1)
  ROOT %add.4368 = bf16[] add(bf16[] %x.4366, bf16[] %y.4367)
}

%AddComputation.4376 (x.4377: bf16[], y.4378: bf16[]) -> bf16[] {
  %x.4377 = bf16[] parameter(0)
  %y.4378 = bf16[] parameter(1)
  ROOT %add.4379 = bf16[] add(bf16[] %x.4377, bf16[] %y.4378)
}

%AddComputation.4394 (x.4395: bf16[], y.4396: bf16[]) -> bf16[] {
  %x.4395 = bf16[] parameter(0)
  %y.4396 = bf16[] parameter(1)
  ROOT %add.4397 = bf16[] add(bf16[] %x.4395, bf16[] %y.4396)
}

%AddComputation.4408 (x.4409: bf16[], y.4410: bf16[]) -> bf16[] {
  %x.4409 = bf16[] parameter(0)
  %y.4410 = bf16[] parameter(1)
  ROOT %add.4411 = bf16[] add(bf16[] %x.4409, bf16[] %y.4410)
}

%AddComputation.4419 (x.4420: bf16[], y.4421: bf16[]) -> bf16[] {
  %x.4420 = bf16[] parameter(0)
  %y.4421 = bf16[] parameter(1)
  ROOT %add.4422 = bf16[] add(bf16[] %x.4420, bf16[] %y.4421)
}

%AddComputation.6413 (x.6414: f32[], y.6415: f32[]) -> f32[] {
  %x.6414 = f32[] parameter(0)
  %y.6415 = f32[] parameter(1)
  ROOT %add.6416 = f32[] add(f32[] %x.6414, f32[] %y.6415)
}

%scalar_add_computation (scalar_lhs: f32[], scalar_rhs: f32[]) -> f32[] {
  %scalar_lhs = f32[] parameter(0)
  %scalar_rhs = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %scalar_lhs, f32[] %scalar_rhs)
}

%scalar_add_computation.1 (scalar_lhs.1: f32[], scalar_rhs.1: f32[]) -> f32[] {
  %scalar_lhs.1 = f32[] parameter(0)
  %scalar_rhs.1 = f32[] parameter(1)
  ROOT %add.1 = f32[] add(f32[] %scalar_lhs.1, f32[] %scalar_rhs.1)
}

ENTRY %SyncTensorsGraph.6428 (p0.1: f32[], p1.3: f32[], p2.10: s64[1024,4096], p3.12: bf16[32000,32], p4.45: bf16[32], p5.68: bf16[32,32], p6.70: bf16[96,32], p7.101: bf16[], p8.109: s64[], p9.121: bf16[1,1,4096,2], p10.159: bf16[1,1,4096,2], p11.278: bf16[32], p12.301: bf16[32,32], p13.303: bf16[64,32], p14.347: bf16[32], p15.370: bf16[32,32], p16.372: bf16[96,32], p17.419: bf16[1,1,4096,2], p18.457: bf16[1,1,4096,2], p19.576: bf16[32], p20.599: bf16[32,32], p21.601: bf16[64,32], p22.645: bf16[32], p23.668: bf16[32,32], p24.670: bf16[96,32], p25.717: bf16[1,1,4096,2], p26.755: bf16[1,1,4096,2], p27.874: bf16[32], p28.897: bf16[32,32], p29.899: bf16[64,32], p30.943: bf16[32], p31.966: bf16[32,32], p32.968: bf16[96,32], p33.1015: bf16[1,1,4096,2], p34.1053: bf16[1,1,4096,2], p35.1172: bf16[32], p36.1247: f32[], p37.1249: bf16[], p38.1251: bf16[], p39.1252: f32[], p40.1258: bf16[], p41.1282: bf16[32,32], p42.1284: bf16[64,32], p43.1328: bf16[32], p44.1333: f32[], p45.1348: s64[], p46.1349: s64[1024,4096], p47.1381: s64[], p48.1399: bf16[32000,32], p49.1521: bf16[32000,32], p50.1559: f32[], p51.1571: f32[], p52.1578: f32[], p53.1618: bf16[32,32], p54.1701: bf16[64,32], p55.1816: bf16[32,32], p56.2096: bf16[96,32], p57.2208: bf16[32,32], p58.2291: bf16[64,32], p59.2406: bf16[32,32], p60.2686: bf16[96,32], p61.2798: bf16[32,32], p62.2881: bf16[64,32], p63.2996: bf16[32,32], p64.3276: bf16[96,32], p65.3388: bf16[32,32], p66.3471: bf16[64,32], p67.3586: bf16[32,32], p68.3866: bf16[96,32], p69.4014: bf16[32000,32], p70.4058: bf16[32], p71.4101: bf16[32], p72.4144: bf16[32], p73.4187: bf16[32], p74.4230: bf16[32], p75.4273: bf16[32], p76.4316: bf16[32], p77.4359: bf16[32], p78.4402: bf16[32], p79.4427: bf16[1], p80.4504: bf16[], p81.4505: bf16[32000,32], p82.4518: bf16[], p83.4524: bf16[], p84.4525: bf16[32000,32], p85.4534: bf16[], p86.4535: bf16[], p87.4536: bf16[32000,32], p88.4574: bf16[96,32], p89.4592: bf16[96,32], p90.4601: bf16[96,32], p91.4639: bf16[32,32], p92.4657: bf16[32,32], p93.4666: bf16[32,32], p94.4704: bf16[64,32], p95.4722: bf16[64,32], p96.4731: bf16[64,32], p97.4769: bf16[32,32], p98.4787: bf16[32,32], p99.4796: bf16[32,32], p100.4833: bf16[32], p101.4851: bf16[32], p102.4860: bf16[32], p103.4897: bf16[32], p104.4915: bf16[32], p105.4924: bf16[32], p106.4962: bf16[96,32], p107.4980: bf16[96,32], p108.4989: bf16[96,32], p109.5027: bf16[32,32], p110.5045: bf16[32,32], p111.5054: bf16[32,32], p112.5092: bf16[64,32], p113.5110: bf16[64,32], p114.5119: bf16[64,32], p115.5157: bf16[32,32], p116.5175: bf16[32,32], p117.5184: bf16[32,32], p118.5221: bf16[32], p119.5239: bf16[32], p120.5248: bf16[32], p121.5285: bf16[32], p122.5303: bf16[32], p123.5312: bf16[32], p124.5350: bf16[96,32], p125.5368: bf16[96,32], p126.5377: bf16[96,32], p127.5415: bf16[32,32], p128.5433: bf16[32,32], p129.5442: bf16[32,32], p130.5480: bf16[64,32], p131.5498: bf16[64,32], p132.5507: bf16[64,32], p133.5545: bf16[32,32], p134.5563: bf16[32,32], p135.5572: bf16[32,32], p136.5609: bf16[32], p137.5627: bf16[32], p138.5636: bf16[32], p139.5673: bf16[32], p140.5691: bf16[32], p141.5700: bf16[32], p142.5738: bf16[96,32], p143.5756: bf16[96,32], p144.5765: bf16[96,32], p145.5803: bf16[32,32], p146.5821: bf16[32,32], p147.5830: bf16[32,32], p148.5868: bf16[64,32], p149.5886: bf16[64,32], p150.5895: bf16[64,32], p151.5933: bf16[32,32], p152.5951: bf16[32,32], p153.5960: bf16[32,32], p154.5997: bf16[32], p155.6015: bf16[32], p156.6024: bf16[32], p157.6061: bf16[32], p158.6079: bf16[32], p159.6088: bf16[32], p160.6125: bf16[32], p161.6143: bf16[32], p162.6152: bf16[32], p163.6190: bf16[32000,32], p164.6208: bf16[32000,32], p165.6217: bf16[32000,32], p166.6373: s64[1024,4096]) -> (bf16[32000,32], bf16[1,1,4096,2], bf16[1,1,4096,2], bf16[96,32], bf16[32,32], /*index=5*/bf16[64,32], bf16[32,32], bf16[32], bf16[32], bf16[1,1,4096,2], /*index=10*/bf16[1,1,4096,2], bf16[96,32], bf16[32,32], bf16[64,32], bf16[32,32], /*index=15*/bf16[32], bf16[32], bf16[1,1,4096,2], bf16[1,1,4096,2], bf16[96,32], /*index=20*/bf16[32,32], bf16[64,32], bf16[32,32], bf16[32], bf16[32], /*index=25*/bf16[1,1,4096,2], bf16[1,1,4096,2], bf16[96,32], bf16[32,32], bf16[64,32], /*index=30*/bf16[32,32], bf16[32], bf16[32], bf16[32], bf16[32000,32], /*index=35*/bf16[32000,32], bf16[96,32], bf16[32,32], bf16[64,32], bf16[32,32], /*index=40*/bf16[32], bf16[32], bf16[96,32], bf16[32,32], bf16[64,32], /*index=45*/bf16[32,32], bf16[32], bf16[32], bf16[96,32], bf16[32,32], /*index=50*/bf16[64,32], bf16[32,32], bf16[32], bf16[32], bf16[96,32], /*index=55*/bf16[32,32], bf16[64,32], bf16[32,32], bf16[32], bf16[32], /*index=60*/bf16[32], bf16[32000,32], bf16[32000,32], bf16[32], bf16[32,32], /*index=65*/bf16[64,32], bf16[32], bf16[32,32], bf16[96,32], bf16[32], /*index=70*/bf16[32,32], bf16[64,32], bf16[32], bf16[32,32], bf16[96,32], /*index=75*/bf16[32], bf16[32,32], bf16[64,32], bf16[32], bf16[32,32], /*index=80*/bf16[96,32], bf16[32], bf16[32,32], bf16[64,32], bf16[32], /*index=85*/bf16[32,32], bf16[96,32], bf16[32], bf16[32000,32], bf16[32000,32], /*index=90*/bf16[32000,32], bf16[96,32], bf16[96,32], bf16[32,32], bf16[32,32], /*index=95*/bf16[64,32], bf16[64,32], bf16[32,32], bf16[32,32], bf16[32], /*index=100*/bf16[32], bf16[32], bf16[32], bf16[96,32], bf16[96,32], /*index=105*/bf16[32,32], bf16[32,32], bf16[64,32], bf16[64,32], bf16[32,32], /*index=110*/bf16[32,32], bf16[32], bf16[32], bf16[32], bf16[32], /*index=115*/bf16[96,32], bf16[96,32], bf16[32,32], bf16[32,32], bf16[64,32], /*index=120*/bf16[64,32], bf16[32,32], bf16[32,32], bf16[32], bf16[32], /*index=125*/bf16[32], bf16[32], bf16[96,32], bf16[96,32], bf16[32,32], /*index=130*/bf16[32,32], bf16[64,32], bf16[64,32], bf16[32,32], bf16[32,32], /*index=135*/bf16[32], bf16[32], bf16[32], bf16[32], bf16[32], /*index=140*/bf16[32], bf16[32000,32], bf16[32000,32], s64[1024,4096], s64[1024,4096], /*index=145*/s64[1024,4096], f32[], bf16[1]) {
  %p8.109 = s64[] parameter(8), frontend_attributes={neff_input_names="input8"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %p47.1381 = s64[] parameter(47), frontend_attributes={neff_input_names="input47"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %p87.4536 = bf16[32000,32]{1,0} parameter(87), frontend_attributes={neff_input_names="input87"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p86.4535 = bf16[] parameter(86), frontend_attributes={neff_input_names="input86"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4537 = bf16[32000,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4538 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %p87.4536, bf16[32000,32]{1,0} %broadcast.4537), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p85.4534 = bf16[] parameter(85), frontend_attributes={neff_input_names="input85"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4539 = bf16[32000,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4541 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.4538, bf16[32000,32]{1,0} %broadcast.4539), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4542 = bf16[32000,32]{1,0} subtract(bf16[32000,32]{1,0} %p87.4536, bf16[32000,32]{1,0} %multiply.4541), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p84.4525 = bf16[32000,32]{1,0} parameter(84), frontend_attributes={neff_input_names="input84"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p83.4524 = bf16[] parameter(83), frontend_attributes={neff_input_names="input83"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4526 = bf16[32000,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4527 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %p84.4525, bf16[32000,32]{1,0} %broadcast.4526), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p69.4014 = bf16[32000,32]{1,0} parameter(69), frontend_attributes={neff_input_names="input69"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %p2.10 = s64[1024,4096]{1,0} parameter(2), frontend_attributes={neff_input_names="input2"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="run_llama_nxd.py" source_line=271}
  %reshape.3981 = s64[4194304]{0} reshape(s64[1024,4096]{1,0} %p2.10), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.3995 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.3996 = s64[4194304]{0} broadcast(s64[] %constant.3995), dimensions={}, metadata={op_type="aten__lt" op_name="aten__lt"}
  %compare.3997 = pred[4194304]{0} compare(s64[4194304]{0} %reshape.3981, s64[4194304]{0} %broadcast.3996), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt"}
  %p45.1348 = s64[] parameter(45), frontend_attributes={neff_input_names="input45"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %broadcast.3993 = s64[4194304]{0} broadcast(s64[] %p45.1348), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %add.3994 = s64[4194304]{0} add(s64[4194304]{0} %reshape.3981, s64[4194304]{0} %broadcast.3993), metadata={op_type="aten__add" op_name="aten__add"}
  %select.3998 = s64[4194304]{0} select(pred[4194304]{0} %compare.3997, s64[4194304]{0} %add.3994, s64[4194304]{0} %reshape.3981), metadata={op_type="aten__where" op_name="aten__where"}
  %reshape.3999 = s64[4194304,1]{1,0} reshape(s64[4194304]{0} %select.3998), metadata={op_type="aten__stack" op_name="aten__stack"}
  %convert.3982 = f32[4194304]{0} convert(s64[4194304]{0} %reshape.3981), metadata={op_type="aten__ne" op_name="aten__ne"}
  %constant.3980 = f32[] constant(-1), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.3983 = f32[4194304]{0} broadcast(f32[] %constant.3980), dimensions={}, metadata={op_type="aten__ne" op_name="aten__ne"}
  %compare.3984 = pred[4194304]{0} compare(f32[4194304]{0} %convert.3982, f32[4194304]{0} %broadcast.3983), direction=NE, metadata={op_type="aten__ne" op_name="aten__ne"}
  %broadcast.3988 = pred[4194304,32]{1,0} broadcast(pred[4194304]{0} %compare.3984), dimensions={0}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %constant.51 = pred[4096]{0} constant({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0})
  %broadcast = pred[1024,4096,32000]{2,1,0} broadcast(pred[4096]{0} %constant.51), dimensions={1}, metadata={op_type="xla__unselect" op_name="xla__unselect"}
  %constant.52 = pred[4096]{0} constant({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0})
  %broadcast.1 = pred[1024,4096,32000]{2,1,0} broadcast(pred[4096]{0} %constant.52), dimensions={1}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %p43.1328 = bf16[32]{0} parameter(43), frontend_attributes={neff_input_names="input43"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.1329 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p43.1328), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %p3.12 = bf16[32000,32]{1,0} parameter(3), frontend_attributes={neff_input_names="input3"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2233}
  %convert.8 = u32[1024,4096]{1,0} convert(s64[1024,4096]{1,0} %p2.10)
  %reshape.1370 = u32[4194304]{0} reshape(u32[1024,4096]{1,0} %convert.8), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2233}
  %gather.14 = bf16[4194304,32]{1,0} gather(bf16[32000,32]{1,0} %p3.12, u32[4194304]{0} %reshape.1370), offset_dims={1}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,32}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2233}
  %reshape.15 = bf16[1024,4096,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %gather.14), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %transpose.16 = bf16[4096,1024,32]{2,0,1} transpose(bf16[1024,4096,32]{2,1,0} %reshape.15), dimensions={1,0,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %iota = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.1 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.94 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota, s32[4096,4096]{1,0} %iota.1), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.85 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.553 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.85), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.80 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.555 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.80), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.9 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.94, bf16[4096,4096]{1,0} %broadcast.553, bf16[4096,4096]{1,0} %broadcast.555), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.40 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.9)
  %broadcast.4 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.40), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.1/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.227 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.1/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.228 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.227), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.1/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %p4.45 = bf16[32]{0} parameter(4), frontend_attributes={neff_input_names="input4"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.46 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p4.45), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.17 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %transpose.16), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.41 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.17, f32[4096,1024,32]{2,0,1} %convert.17), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.19 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.25 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.41, f32[] %constant.19), dimensions={2}, to_apply=%AddComputation.21, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.57 = f32[] constant(0.03125)
  %broadcast.33 = f32[4096,1024]{1,0} broadcast(f32[] %constant.57), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.34 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.25, f32[4096,1024]{1,0} %broadcast.33), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %p1.3 = f32[] parameter(1), frontend_attributes={neff_input_names="input1"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.557 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.3 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.34, f32[4096,1024]{1,0} %broadcast.557), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.0 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.3), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.42 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.0), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.43 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.17, f32[4096,1024,32]{2,1,0} %broadcast.42), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.44 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.43), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.47 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.46, bf16[4096,1024,32]{2,0,1} %convert.44), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %p0.1 = f32[] parameter(0), frontend_attributes={neff_input_names="input0"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=430}
  %convert.48 = bf16[] convert(f32[] %p0.1), metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.52 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.47, bf16[] %convert.48), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.53 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.52), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.72 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.53), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p6.70 = bf16[96,32]{1,0} parameter(6), frontend_attributes={neff_input_names="input6"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.71 = bf16[32,96]{0,1} transpose(bf16[96,32]{1,0} %p6.70), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.73 = bf16[4194304,96]{1,0} dot(bf16[4194304,32]{1,0} %reshape.72, bf16[32,96]{0,1} %transpose.71), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.206 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.73), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %slice.207 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.206), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.208 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.207), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %transpose.209 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.208), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.214 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.215 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.214), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p10.159 = bf16[1,1,4096,2]{3,2,1,0} parameter(10), frontend_attributes={neff_input_names="input10"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.163 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p10.159), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %iota.4 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.164 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.163, s64[1,4096,1]{2,1,0} %iota.4), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %reshape.211 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.164), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.212 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.211), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.216 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.215, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.212), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.191 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.73), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.192 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.191), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.17 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.192), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2172 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.17), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.7 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2172), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.202 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.7), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.203 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.202), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p9.121 = bf16[1,1,4096,2]{3,2,1,0} parameter(9), frontend_attributes={neff_input_names="input9"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.125 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p9.121), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %iota.5 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.126 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.125, s64[1,4096,1]{2,1,0} %iota.5), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %reshape.199 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.126), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.200 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.199), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.205 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.203, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.200), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %add.217 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.216, bf16[1024,32,4096,2]{3,2,1,0} %multiply.205), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.219 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %add.217), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.166 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.73), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %slice.167 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.166), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.168 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.167), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.169 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.168), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.174 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.169), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.175 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.174), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.171 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.164), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.172 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.171), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.176 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.175, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.172), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.133 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.73), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.134 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.133), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.16 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.134), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2170 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.16), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.5 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2170), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.144 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.5), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.145 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.144), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.141 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.126), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.142 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.141), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.147 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.145, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.142), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %add.177 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.176, bf16[1024,32,4096,2]{3,2,1,0} %multiply.147), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.178 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %add.177), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.180 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.178), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.220 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.219, bf16[32768,2,4096]{2,1,0} %reshape.180), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %p7.101 = bf16[] parameter(7), frontend_attributes={neff_input_names="input7"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.559 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.10 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.220, bf16[32768,4096,4096]{2,1,0} %broadcast.559), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1810 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.10), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.229 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.4, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.228, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1810), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.1/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.230 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.229), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.12 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.230), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.238 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.12), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.240 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.238), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.74 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.73), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.75 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.74), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.76 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.75), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.77 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.76), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.62 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.77)
  %broadcast.206 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.62), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %multiply.20 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.240, bf16[32768,4096,4096]{2,1,0} %broadcast.206)
  %convert = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.20)
  %constant.42 = f32[] constant(0)
  %reduce = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert, f32[] %constant.42), dimensions={2}, to_apply=%scalar_add_computation
  %convert.1 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.242 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.1), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.243 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.242), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.245 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.243), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p5.68 = bf16[32,32]{1,0} parameter(5), frontend_attributes={neff_input_names="input5"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.69 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p5.68), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.246 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.245, bf16[32,32]{0,1} %transpose.69), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.247 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.246), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %add.249 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %transpose.16, bf16[4096,1024,32]{2,1,0} %reshape.247), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %p11.278 = bf16[32]{0} parameter(11), frontend_attributes={neff_input_names="input11"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.279 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p11.278), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.250 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.249), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.42 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.250, f32[4096,1024,32]{2,0,1} %convert.250), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.252 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.258 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.42, f32[] %constant.252), dimensions={2}, to_apply=%AddComputation.254, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.62 = f32[] constant(0.03125)
  %broadcast.266 = f32[4096,1024]{1,0} broadcast(f32[] %constant.62), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.267 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.258, f32[4096,1024]{1,0} %broadcast.266), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.561 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.4 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.267, f32[4096,1024]{1,0} %broadcast.561), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.1 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.4), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.275 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.1), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.276 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.250, f32[4096,1024,32]{2,1,0} %broadcast.275), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.277 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.276), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.280 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.279, bf16[4096,1024,32]{2,0,1} %convert.277), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.284 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.52), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.285 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.280, bf16[] %get-tuple-element.284), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.286 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.285), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.305 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.286), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p13.303 = bf16[64,32]{1,0} parameter(13), frontend_attributes={neff_input_names="input13"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.304 = bf16[32,64]{0,1} transpose(bf16[64,32]{1,0} %p13.303), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.306 = bf16[4194304,64]{1,0} dot(bf16[4194304,32]{1,0} %reshape.305, bf16[32,64]{0,1} %transpose.304), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.309 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.306), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.310 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.309), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %logistic.311 = bf16[4096,1024,32]{2,1,0} logistic(bf16[4096,1024,32]{2,1,0} %slice.310), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.312 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.310, bf16[4096,1024,32]{2,1,0} %logistic.311), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.307 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.306), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %slice.308 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.307), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.313 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.312, bf16[4096,1024,32]{2,1,0} %slice.308), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %reshape.314 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.313), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p12.301 = bf16[32,32]{1,0} parameter(12), frontend_attributes={neff_input_names="input12"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.302 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p12.301), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.315 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.314, bf16[32,32]{0,1} %transpose.302), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.316 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.315), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %add.318 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.249, bf16[4096,1024,32]{2,1,0} %reshape.316), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %iota.6 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.7 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.396 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.6, s32[4096,4096]{1,0} %iota.7), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.387 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.563 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.387), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.382 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.566 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.382), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.10 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.396, bf16[4096,4096]{1,0} %broadcast.563, bf16[4096,4096]{1,0} %broadcast.566), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.39 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.10)
  %broadcast.13 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.39), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.2/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.525 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.2/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.526 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.525), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.2/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %p14.347 = bf16[32]{0} parameter(14), frontend_attributes={neff_input_names="input14"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.348 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p14.347), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.319 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.318), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.44 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.319, f32[4096,1024,32]{2,0,1} %convert.319), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.321 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.327 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.44, f32[] %constant.321), dimensions={2}, to_apply=%AddComputation.323, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.67 = f32[] constant(0.03125)
  %broadcast.335 = f32[4096,1024]{1,0} broadcast(f32[] %constant.67), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.336 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.327, f32[4096,1024]{1,0} %broadcast.335), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.569 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.5 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.336, f32[4096,1024]{1,0} %broadcast.569), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.2 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.5), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.344 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.2), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.345 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.319, f32[4096,1024,32]{2,1,0} %broadcast.344), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.346 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.345), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.349 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.348, bf16[4096,1024,32]{2,0,1} %convert.346), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.353 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.285), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.354 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.349, bf16[] %get-tuple-element.353), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.355 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.354), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.374 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.355), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p16.372 = bf16[96,32]{1,0} parameter(16), frontend_attributes={neff_input_names="input16"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.373 = bf16[32,96]{0,1} transpose(bf16[96,32]{1,0} %p16.372), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.375 = bf16[4194304,96]{1,0} dot(bf16[4194304,32]{1,0} %reshape.374, bf16[32,96]{0,1} %transpose.373), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.504 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.375), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %slice.505 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.504), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.506 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.505), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %transpose.507 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.506), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.512 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.507), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.513 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.512), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p18.457 = bf16[1,1,4096,2]{3,2,1,0} parameter(18), frontend_attributes={neff_input_names="input18"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.461 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p18.457), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %iota.8 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.462 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.461, s64[1,4096,1]{2,1,0} %iota.8), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %reshape.509 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.462), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.510 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.509), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.514 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.513, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.510), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.489 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.375), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.490 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.489), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.19 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.490), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2176 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.19), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.11 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2176), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.500 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.11), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.501 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.500), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p17.419 = bf16[1,1,4096,2]{3,2,1,0} parameter(17), frontend_attributes={neff_input_names="input17"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.423 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p17.419), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %iota.9 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.424 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.423, s64[1,4096,1]{2,1,0} %iota.9), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %reshape.497 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.424), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.498 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.497), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.503 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.501, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.498), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %add.515 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.514, bf16[1024,32,4096,2]{3,2,1,0} %multiply.503), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.517 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %add.515), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.464 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.375), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %slice.465 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.464), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.466 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.465), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.467 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.466), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.472 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.467), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.473 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.472), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.469 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.462), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.470 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.469), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.474 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.473, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.470), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.431 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.375), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.432 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.431), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.18 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.432), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2174 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.18), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.9 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2174), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.442 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.9), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.443 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.442), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.439 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.424), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.440 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.439), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.445 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.443, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.440), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %add.475 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.474, bf16[1024,32,4096,2]{3,2,1,0} %multiply.445), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.476 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %add.475), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.478 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.476), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.518 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.517, bf16[32768,2,4096]{2,1,0} %reshape.478), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.572 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.11 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.518, bf16[32768,4096,4096]{2,1,0} %broadcast.572), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1824 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.11), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.527 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.13, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.526, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1824), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.2/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.528 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.527), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.13 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.528), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.536 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.13), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.538 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.536), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.376 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.375), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.377 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.376), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.378 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.377), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.379 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.378), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.143 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.379)
  %broadcast.207 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.143), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %multiply.25 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.538, bf16[32768,4096,4096]{2,1,0} %broadcast.207)
  %convert.2 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.25)
  %constant.43 = f32[] constant(0)
  %reduce.1 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.2, f32[] %constant.43), dimensions={2}, to_apply=%scalar_add_computation
  %convert.3 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.1), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.540 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.3), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.541 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.540), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.543 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.541), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p15.370 = bf16[32,32]{1,0} parameter(15), frontend_attributes={neff_input_names="input15"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.371 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p15.370), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.544 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.543, bf16[32,32]{0,1} %transpose.371), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.545 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.544), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %add.547 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.318, bf16[4096,1024,32]{2,1,0} %reshape.545), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %p19.576 = bf16[32]{0} parameter(19), frontend_attributes={neff_input_names="input19"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.577 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p19.576), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.548 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.547), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.45 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.548, f32[4096,1024,32]{2,0,1} %convert.548), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.550 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.556 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.45, f32[] %constant.550), dimensions={2}, to_apply=%AddComputation.552, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.71 = f32[] constant(0.03125)
  %broadcast.564 = f32[4096,1024]{1,0} broadcast(f32[] %constant.71), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.565 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.556, f32[4096,1024]{1,0} %broadcast.564), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.575 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.6 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.565, f32[4096,1024]{1,0} %broadcast.575), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.3 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.6), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.573 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.3), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.574 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.548, f32[4096,1024,32]{2,1,0} %broadcast.573), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.575 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.574), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.578 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.577, bf16[4096,1024,32]{2,0,1} %convert.575), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.582 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.354), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.583 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.578, bf16[] %get-tuple-element.582), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.584 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.583), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.603 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.584), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p21.601 = bf16[64,32]{1,0} parameter(21), frontend_attributes={neff_input_names="input21"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.602 = bf16[32,64]{0,1} transpose(bf16[64,32]{1,0} %p21.601), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.604 = bf16[4194304,64]{1,0} dot(bf16[4194304,32]{1,0} %reshape.603, bf16[32,64]{0,1} %transpose.602), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.607 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.604), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.608 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.607), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %logistic.609 = bf16[4096,1024,32]{2,1,0} logistic(bf16[4096,1024,32]{2,1,0} %slice.608), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.610 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.608, bf16[4096,1024,32]{2,1,0} %logistic.609), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.605 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.604), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %slice.606 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.605), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.611 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.610, bf16[4096,1024,32]{2,1,0} %slice.606), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %reshape.612 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.611), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p20.599 = bf16[32,32]{1,0} parameter(20), frontend_attributes={neff_input_names="input20"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.600 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p20.599), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.613 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.612, bf16[32,32]{0,1} %transpose.600), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.614 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.613), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %add.616 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.547, bf16[4096,1024,32]{2,1,0} %reshape.614), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %iota.10 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.11 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.694 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.10, s32[4096,4096]{1,0} %iota.11), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.685 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.578 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.685), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.680 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.580 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.680), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.11 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.694, bf16[4096,4096]{1,0} %broadcast.578, bf16[4096,4096]{1,0} %broadcast.580), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.38 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.11)
  %broadcast.20 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.38), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.3/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.823 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.3/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.824 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.823), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.3/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %p22.645 = bf16[32]{0} parameter(22), frontend_attributes={neff_input_names="input22"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.646 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p22.645), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.617 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.616), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.46 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.617, f32[4096,1024,32]{2,0,1} %convert.617), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.619 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.625 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.46, f32[] %constant.619), dimensions={2}, to_apply=%AddComputation.621, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.75 = f32[] constant(0.03125)
  %broadcast.633 = f32[4096,1024]{1,0} broadcast(f32[] %constant.75), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.634 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.625, f32[4096,1024]{1,0} %broadcast.633), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.582 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.7 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.634, f32[4096,1024]{1,0} %broadcast.582), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.4 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.7), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.642 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.4), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.643 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.617, f32[4096,1024,32]{2,1,0} %broadcast.642), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.644 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.643), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.647 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.646, bf16[4096,1024,32]{2,0,1} %convert.644), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.651 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.583), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.652 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.647, bf16[] %get-tuple-element.651), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.653 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.652), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.672 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.653), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p24.670 = bf16[96,32]{1,0} parameter(24), frontend_attributes={neff_input_names="input24"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.671 = bf16[32,96]{0,1} transpose(bf16[96,32]{1,0} %p24.670), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.673 = bf16[4194304,96]{1,0} dot(bf16[4194304,32]{1,0} %reshape.672, bf16[32,96]{0,1} %transpose.671), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.802 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.673), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %slice.803 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.802), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.804 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.803), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %transpose.805 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.804), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.810 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.805), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.811 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.810), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p26.755 = bf16[1,1,4096,2]{3,2,1,0} parameter(26), frontend_attributes={neff_input_names="input26"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.759 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p26.755), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %iota.12 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.760 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.759, s64[1,4096,1]{2,1,0} %iota.12), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %reshape.807 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.760), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.808 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.807), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.812 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.811, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.808), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.787 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.673), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.788 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.787), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.21 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.788), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2180 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.21), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.15 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2180), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.798 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.15), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.799 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.798), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p25.717 = bf16[1,1,4096,2]{3,2,1,0} parameter(25), frontend_attributes={neff_input_names="input25"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.721 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p25.717), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %iota.13 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.722 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.721, s64[1,4096,1]{2,1,0} %iota.13), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %reshape.795 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.722), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.796 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.795), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.801 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.799, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.796), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %add.813 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.812, bf16[1024,32,4096,2]{3,2,1,0} %multiply.801), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.815 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %add.813), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.762 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.673), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %slice.763 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.762), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.764 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.763), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.765 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.764), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.770 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.765), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.771 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.770), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.767 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.760), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.768 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.767), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.772 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.771, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.768), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.729 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.673), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.730 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.729), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.20 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.730), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2178 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.20), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.13 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2178), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.740 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.13), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.741 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.740), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.737 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.722), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.738 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.737), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.743 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.741, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.738), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %add.773 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.772, bf16[1024,32,4096,2]{3,2,1,0} %multiply.743), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.774 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %add.773), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.776 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.774), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.816 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.815, bf16[32768,2,4096]{2,1,0} %reshape.776), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.584 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.12 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.816, bf16[32768,4096,4096]{2,1,0} %broadcast.584), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1837 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.12), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.825 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.20, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.824, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1837), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.3/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.826 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.825), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.14 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.826), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.834 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.14), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.836 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.834), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.674 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.673), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.675 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.674), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.676 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.675), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.677 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.676), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.227 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.677)
  %broadcast.208 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.227), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %multiply.30 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.836, bf16[32768,4096,4096]{2,1,0} %broadcast.208)
  %convert.4 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.30)
  %constant.44 = f32[] constant(0)
  %reduce.2 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.4, f32[] %constant.44), dimensions={2}, to_apply=%scalar_add_computation
  %convert.5 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.2), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.838 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.5), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.839 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.838), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.841 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.839), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p23.668 = bf16[32,32]{1,0} parameter(23), frontend_attributes={neff_input_names="input23"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.669 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p23.668), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.842 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.841, bf16[32,32]{0,1} %transpose.669), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.843 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.842), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %add.845 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.616, bf16[4096,1024,32]{2,1,0} %reshape.843), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %p27.874 = bf16[32]{0} parameter(27), frontend_attributes={neff_input_names="input27"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.875 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p27.874), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.846 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.845), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.48 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.846, f32[4096,1024,32]{2,0,1} %convert.846), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.848 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.854 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.48, f32[] %constant.848), dimensions={2}, to_apply=%AddComputation.850, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.79 = f32[] constant(0.03125)
  %broadcast.862 = f32[4096,1024]{1,0} broadcast(f32[] %constant.79), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.863 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.854, f32[4096,1024]{1,0} %broadcast.862), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.586 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.8 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.863, f32[4096,1024]{1,0} %broadcast.586), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.5 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.8), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.871 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.5), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.872 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.846, f32[4096,1024,32]{2,1,0} %broadcast.871), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.873 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.872), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.876 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.875, bf16[4096,1024,32]{2,0,1} %convert.873), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.880 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.652), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.881 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.876, bf16[] %get-tuple-element.880), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.882 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.881), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.901 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.882), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p29.899 = bf16[64,32]{1,0} parameter(29), frontend_attributes={neff_input_names="input29"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.900 = bf16[32,64]{0,1} transpose(bf16[64,32]{1,0} %p29.899), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.902 = bf16[4194304,64]{1,0} dot(bf16[4194304,32]{1,0} %reshape.901, bf16[32,64]{0,1} %transpose.900), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.905 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.902), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.906 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.905), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %logistic.907 = bf16[4096,1024,32]{2,1,0} logistic(bf16[4096,1024,32]{2,1,0} %slice.906), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.908 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.906, bf16[4096,1024,32]{2,1,0} %logistic.907), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.903 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.902), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %slice.904 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.903), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.909 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.908, bf16[4096,1024,32]{2,1,0} %slice.904), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %reshape.910 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.909), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p28.897 = bf16[32,32]{1,0} parameter(28), frontend_attributes={neff_input_names="input28"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.898 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p28.897), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.911 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.910, bf16[32,32]{0,1} %transpose.898), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.912 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.911), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %add.914 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.845, bf16[4096,1024,32]{2,1,0} %reshape.912), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %iota.14 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.15 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.992 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.14, s32[4096,4096]{1,0} %iota.15), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.983 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.588 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.983), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.978 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.590 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.978), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.12 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.992, bf16[4096,4096]{1,0} %broadcast.588, bf16[4096,4096]{1,0} %broadcast.590), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.37 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.12)
  %broadcast.27 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.37), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.4/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.1121 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.4/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.1122 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.1121), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.4/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %p30.943 = bf16[32]{0} parameter(30), frontend_attributes={neff_input_names="input30"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.944 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p30.943), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.915 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.914), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.49 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.915, f32[4096,1024,32]{2,0,1} %convert.915), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.917 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.923 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.49, f32[] %constant.917), dimensions={2}, to_apply=%AddComputation.919, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.84 = f32[] constant(0.03125)
  %broadcast.931 = f32[4096,1024]{1,0} broadcast(f32[] %constant.84), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.932 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.923, f32[4096,1024]{1,0} %broadcast.931), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.594 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.9 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.932, f32[4096,1024]{1,0} %broadcast.594), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.6 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.9), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.940 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.6), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.941 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.915, f32[4096,1024,32]{2,1,0} %broadcast.940), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.942 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.941), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.945 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.944, bf16[4096,1024,32]{2,0,1} %convert.942), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.949 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.881), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.950 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.945, bf16[] %get-tuple-element.949), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.951 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.950), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.970 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.951), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p32.968 = bf16[96,32]{1,0} parameter(32), frontend_attributes={neff_input_names="input32"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.969 = bf16[32,96]{0,1} transpose(bf16[96,32]{1,0} %p32.968), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.971 = bf16[4194304,96]{1,0} dot(bf16[4194304,32]{1,0} %reshape.970, bf16[32,96]{0,1} %transpose.969), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.1100 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.971), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %slice.1101 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.1100), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.1102 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.1101), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %transpose.1103 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.1102), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.1108 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.1103), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.1109 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.1108), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p34.1053 = bf16[1,1,4096,2]{3,2,1,0} parameter(34), frontend_attributes={neff_input_names="input34"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.1057 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p34.1053), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %iota.16 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.1058 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.1057, s64[1,4096,1]{2,1,0} %iota.16), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=184}
  %reshape.1105 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1058), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.1106 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.1105), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.1110 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1109, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1106), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.1085 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.971), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.1086 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.1085), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.23 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.1086), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2184 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.23), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.20 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2184), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.1096 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.20), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.1097 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.1096), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %p33.1015 = bf16[1,1,4096,2]{3,2,1,0} parameter(33), frontend_attributes={neff_input_names="input33"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/modules/module.py" source_line=1158}
  %reshape.1019 = bf16[4096,2]{1,0} reshape(bf16[1,1,4096,2]{3,2,1,0} %p33.1015), metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %iota.17 = s64[1,4096,1]{2,1,0} iota(), iota_dimension=1, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=501}
  %gather.1020 = bf16[1,4096,2]{2,1,0} gather(bf16[4096,2]{1,0} %reshape.1019, s64[1,4096,1]{2,1,0} %iota.17), offset_dims={2}, collapsed_slice_dims={0}, start_index_map={0}, index_vector_dim=2, slice_sizes={1,2}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=185}
  %reshape.1093 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1020), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %broadcast.1094 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.1093), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %multiply.1099 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1097, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1094), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %add.1111 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.1110, bf16[1024,32,4096,2]{3,2,1,0} %multiply.1099), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=186}
  %reshape.1113 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %add.1111), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1060 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.971), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %slice.1061 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.1060), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.1062 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.1061), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.1063 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.1062), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.1068 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.1063), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.1069 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.1068), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.1065 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1058), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.1066 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.1065), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.1070 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1069, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1066), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.1027 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.971), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %slice.1028 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.1027), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %negate.22 = bf16[4096,1024,32]{2,1,0} negate(bf16[4096,1024,32]{2,1,0} %slice.1028), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.2182 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %negate.22), metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %transpose.18 = bf16[1024,32,4096,1]{3,2,1,0} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2182), dimensions={1,2,0,3}, metadata={op_type="aten__neg" op_name="aten__neg" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=177}
  %reshape.1038 = bf16[1024,32,4096]{2,1,0} reshape(bf16[1024,32,4096,1]{3,2,1,0} %transpose.18), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.1039 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[1024,32,4096]{2,1,0} %reshape.1038), dimensions={0,1,2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %reshape.1035 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1020), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %broadcast.1036 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.1035), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %multiply.1041 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1039, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1036), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %add.1071 = bf16[1024,32,4096,2]{3,2,1,0} add(bf16[1024,32,4096,2]{3,2,1,0} %multiply.1070, bf16[1024,32,4096,2]{3,2,1,0} %multiply.1041), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=187}
  %transpose.1072 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %add.1071), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1074 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.1072), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.1114 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.1113, bf16[32768,2,4096]{2,1,0} %reshape.1074), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.597 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.13 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.1114, bf16[32768,4096,4096]{2,1,0} %broadcast.597), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1856 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.13), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.1123 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.27, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.1122, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1856), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.4/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.1124 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.1123), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.15 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.1124), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.1132 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.15), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.1134 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.1132), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.972 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.971), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.973 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %reshape.972), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.974 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.973), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.975 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.974), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.293 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.975)
  %broadcast.209 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.293), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %multiply.36 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.1134, bf16[32768,4096,4096]{2,1,0} %broadcast.209)
  %convert.6 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.36)
  %constant.45 = f32[] constant(0)
  %reduce.3 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.6, f32[] %constant.45), dimensions={2}, to_apply=%scalar_add_computation
  %convert.7 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.3), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.1136 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.7), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.1137 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.1136), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.1139 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.1137), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p31.966 = bf16[32,32]{1,0} parameter(31), frontend_attributes={neff_input_names="input31"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.967 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p31.966), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.1140 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.1139, bf16[32,32]{0,1} %transpose.967), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.1141 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.1140), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %add.1143 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.914, bf16[4096,1024,32]{2,1,0} %reshape.1141), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=416}
  %p35.1172 = bf16[32]{0} parameter(35), frontend_attributes={neff_input_names="input35"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %broadcast.1173 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p35.1172), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %convert.1144 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.1143), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.50 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1144, f32[4096,1024,32]{2,0,1} %convert.1144), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.1146 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.1152 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.50, f32[] %constant.1146), dimensions={2}, to_apply=%AddComputation.1148, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.90 = f32[] constant(0.03125)
  %broadcast.1160 = f32[4096,1024]{1,0} broadcast(f32[] %constant.90), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.1161 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.1152, f32[4096,1024]{1,0} %broadcast.1160), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.600 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.10 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.1161, f32[4096,1024]{1,0} %broadcast.600), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.7 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.10), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.1169 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.7), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.1170 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1144, f32[4096,1024,32]{2,1,0} %broadcast.1169), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.1171 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.1170), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.1174 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.1173, bf16[4096,1024,32]{2,0,1} %convert.1171), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %get-tuple-element.1178 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.950), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.1179 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.1174, bf16[] %get-tuple-element.1178), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1180 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1179), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.1286 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1180), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p42.1284 = bf16[64,32]{1,0} parameter(42), frontend_attributes={neff_input_names="input42"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.1285 = bf16[32,64]{0,1} transpose(bf16[64,32]{1,0} %p42.1284), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.1287 = bf16[4194304,64]{1,0} dot(bf16[4194304,32]{1,0} %reshape.1286, bf16[32,64]{0,1} %transpose.1285), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.1290 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.1287), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.1291 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.1290), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %logistic.1292 = bf16[4096,1024,32]{2,1,0} logistic(bf16[4096,1024,32]{2,1,0} %slice.1291), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.1293 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.1291, bf16[4096,1024,32]{2,1,0} %logistic.1292), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.1288 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.1287), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %slice.1289 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %reshape.1288), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.1294 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.1293, bf16[4096,1024,32]{2,1,0} %slice.1289), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %reshape.1295 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.1294), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p41.1282 = bf16[32,32]{1,0} parameter(41), frontend_attributes={neff_input_names="input41"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.1283 = bf16[32,32]{0,1} transpose(bf16[32,32]{1,0} %p41.1282), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.1296 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.1295, bf16[32,32]{0,1} %transpose.1283), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.1297 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.1296), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %add.1299 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.1143, bf16[4096,1024,32]{2,1,0} %reshape.1297), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py" source_line=422}
  %convert.1300 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %add.1299), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=125}
  %multiply.51 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1300, f32[4096,1024,32]{2,0,1} %convert.1300), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.1302 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %reduce.1308 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.51, f32[] %constant.1302), dimensions={2}, to_apply=%AddComputation.1304, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %constant.94 = f32[] constant(0.03125)
  %broadcast.1316 = f32[4096,1024]{1,0} broadcast(f32[] %constant.94), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %multiply.1317 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.1308, f32[4096,1024]{1,0} %broadcast.1316), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=127}
  %broadcast.602 = f32[4096,1024]{1,0} broadcast(f32[] %p1.3), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %add.11 = f32[4096,1024]{1,0} add(f32[4096,1024]{1,0} %multiply.1317, f32[4096,1024]{1,0} %broadcast.602), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %rsqrt.8 = f32[4096,1024]{1,0} rsqrt(f32[4096,1024]{1,0} %add.11), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %broadcast.1325 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.8), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.1326 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1300, f32[4096,1024,32]{2,1,0} %broadcast.1325), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %convert.1327 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %multiply.1326), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %multiply.1330 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %broadcast.1329, bf16[4096,1024,32]{2,0,1} %convert.1327), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=129}
  %transpose.1401 = bf16[1024,4096,32]{2,0,1} transpose(bf16[4096,1024,32]{2,1,0} %multiply.1330), dimensions={1,0,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %reshape.1402 = bf16[4194304,32]{1,0} reshape(bf16[1024,4096,32]{2,0,1} %transpose.1401), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %p48.1399 = bf16[32000,32]{1,0} parameter(48), frontend_attributes={neff_input_names="input48"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %transpose.1400 = bf16[32,32000]{0,1} transpose(bf16[32000,32]{1,0} %p48.1399), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %dot.1403 = bf16[4194304,32000]{1,0} dot(bf16[4194304,32]{1,0} %reshape.1402, bf16[32,32000]{0,1} %transpose.1400), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=239}
  %convert.286 = f32[4194304,32000]{1,0} convert(bf16[4194304,32000]{1,0} %dot.1403), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=686}
  %reshape.2820 = f32[1024,4096,32000]{2,1,0} reshape(f32[4194304,32000]{1,0} %convert.286), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=686}
  %slice.1407 = f32[1024,4095,32000]{2,1,0} slice(f32[1024,4096,32000]{2,1,0} %reshape.2820), slice={[0:1024], [0:4095], [0:32000]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=17}
  %reshape.1408 = f32[4193280,32000]{1,0} reshape(f32[1024,4095,32000]{2,1,0} %slice.1407), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=17}
  %constant.1409 = f32[] constant(-3.40282347e+38), metadata={op_type="aten__max" op_name="aten__max" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=17}
  %reduce.1415 = f32[4193280]{0} reduce(f32[4193280,32000]{1,0} %reshape.1408, f32[] %constant.1409), dimensions={1}, to_apply=%MaxComputation.1411, metadata={op_type="aten__max" op_name="aten__max" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=17}
  %broadcast.1438 = f32[4193280,32000]{1,0} broadcast(f32[4193280]{0} %reduce.1415), dimensions={0}, metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=24}
  %subtract.1439 = f32[4193280,32000]{1,0} subtract(f32[4193280,32000]{1,0} %reshape.1408, f32[4193280,32000]{1,0} %broadcast.1438), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=24}
  %reshape.1440 = f32[1024,4095,32000]{2,1,0} reshape(f32[4193280,32000]{1,0} %subtract.1439), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %constant.1445 = f32[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %pad.1446 = f32[1024,4096,32000]{2,1,0} pad(f32[1024,4095,32000]{2,1,0} %reshape.1440, f32[] %constant.1445), padding=0_0x0_1x0_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %select.1448 = f32[1024,4096,32000]{2,1,0} select(pred[1024,4096,32000]{2,1,0} %broadcast.1, f32[1024,4096,32000]{2,1,0} %pad.1446, f32[1024,4096,32000]{2,1,0} %reshape.2820), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %slice.1450 = f32[1024,4095,32000]{2,1,0} slice(f32[1024,4096,32000]{2,1,0} %select.1448), slice={[0:1024], [0:4095], [0:32000]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=65}
  %exponential.0 = f32[1024,4095,32000]{2,1,0} exponential(f32[1024,4095,32000]{2,1,0} %slice.1450), metadata={op_type="aten__exp" op_name="aten__exp" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=65}
  %reshape.1873 = f32[4193280,32000]{1,0} reshape(f32[1024,4095,32000]{2,1,0} %exponential.0), metadata={op_type="aten__exp" op_name="aten__exp" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=65}
  %constant.1453 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=66}
  %reduce.1459 = f32[4193280]{0} reduce(f32[4193280,32000]{1,0} %reshape.1873, f32[] %constant.1453), dimensions={1}, to_apply=%AddComputation.1455, metadata={op_type="aten__sum" op_name="aten__sum" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=66}
  %broadcast.1463 = f32[4193280,32000]{1,0} broadcast(f32[4193280]{0} %reduce.1459), dimensions={0}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=77}
  %divide.1464 = f32[4193280,32000]{1,0} divide(f32[4193280,32000]{1,0} %reshape.1873, f32[4193280,32000]{1,0} %broadcast.1463), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=77}
  %iota.19 = s64[4193280,1]{1,0} iota(), iota_dimension=0, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=116}
  %p46.1349 = s64[1024,4096]{1,0} parameter(46), frontend_attributes={neff_input_names="input46"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="run_llama_nxd.py" source_line=273}
  %slice.1350 = s64[1024,4095]{1,0} slice(s64[1024,4096]{1,0} %p46.1349), slice={[0:1024], [1:4096]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %reshape.1351 = s64[4193280]{0} reshape(s64[1024,4095]{1,0} %slice.1350), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %constant.1354 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %broadcast.1355 = s64[4193280]{0} broadcast(s64[] %constant.1354), dimensions={}, metadata={op_type="aten__ge" op_name="aten__ge" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %compare.1356 = pred[4193280]{0} compare(s64[4193280]{0} %reshape.1351, s64[4193280]{0} %broadcast.1355), direction=GE, metadata={op_type="aten__ge" op_name="aten__ge" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %convert.1357 = u8[4193280]{0} convert(pred[4193280]{0} %compare.1356), metadata={op_type="aten__bitwise_and" op_name="aten__bitwise_and" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %broadcast.1352 = s64[4193280]{0} broadcast(s64[] %p45.1348), dimensions={}, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %compare.1353 = pred[4193280]{0} compare(s64[4193280]{0} %reshape.1351, s64[4193280]{0} %broadcast.1352), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %convert.1358 = u8[4193280]{0} convert(pred[4193280]{0} %compare.1353), metadata={op_type="aten__bitwise_and" op_name="aten__bitwise_and" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %and.1359 = u8[4193280]{0} and(u8[4193280]{0} %convert.1357, u8[4193280]{0} %convert.1358), metadata={op_type="aten__bitwise_and" op_name="aten__bitwise_and" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %convert.1360 = pred[4193280]{0} convert(u8[4193280]{0} %and.1359), metadata={op_type="aten__bitwise_and" op_name="aten__bitwise_and" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=37}
  %constant.40 = s64[] constant(0)
  %broadcast.35 = s64[4193280]{0} broadcast(s64[] %constant.40), dimensions={}
  %select = s64[4193280]{0} select(pred[4193280]{0} %convert.1360, s64[4193280]{0} %reshape.1351, s64[4193280]{0} %broadcast.35), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=39}
  %constant.1472 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %broadcast.1473 = s64[4193280]{0} broadcast(s64[] %constant.1472), dimensions={}, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %compare.1474 = pred[4193280]{0} compare(s64[4193280]{0} %select, s64[4193280]{0} %broadcast.1473), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %broadcast.1470 = s64[4193280]{0} broadcast(s64[] %p45.1348), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %add.1471 = s64[4193280]{0} add(s64[4193280]{0} %select, s64[4193280]{0} %broadcast.1470), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %select.1475 = s64[4193280]{0} select(pred[4193280]{0} %compare.1474, s64[4193280]{0} %add.1471, s64[4193280]{0} %select), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %reshape.1486 = s64[4193280,1]{1,0} reshape(s64[4193280]{0} %select.1475), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %concatenate.1487 = s64[4193280,2]{1,0} concatenate(s64[4193280,1]{1,0} %iota.19, s64[4193280,1]{1,0} %reshape.1486), dimensions={1}, metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %iota.20 = s64[4193280,1]{1,0} iota(), iota_dimension=0, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=116}
  %constant.1376 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %broadcast.1377 = s64[4193280]{0} broadcast(s64[] %constant.1376), dimensions={}, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %compare.1378 = pred[4193280]{0} compare(s64[4193280]{0} %select, s64[4193280]{0} %broadcast.1377), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %broadcast.1374 = s64[4193280]{0} broadcast(s64[] %p45.1348), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %add.1375 = s64[4193280]{0} add(s64[4193280]{0} %select, s64[4193280]{0} %broadcast.1374), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %select.1379 = s64[4193280]{0} select(pred[4193280]{0} %compare.1378, s64[4193280]{0} %add.1375, s64[4193280]{0} %select), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %reshape.1392 = s64[4193280,1]{1,0} reshape(s64[4193280]{0} %select.1379), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %concatenate.1393 = s64[4193280,2]{1,0} concatenate(s64[4193280,1]{1,0} %iota.20, s64[4193280,1]{1,0} %reshape.1392), dimensions={1}, metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %gather.1465 = f32[4193280]{0} gather(f32[4193280,32000]{1,0} %divide.1464, s64[4193280,2]{1,0} %concatenate.1393), offset_dims={}, collapsed_slice_dims={0,1}, start_index_map={0,1}, index_vector_dim=1, slice_sizes={1,1}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %convert.1362 = f32[4193280]{0} convert(pred[4193280]{0} %convert.1360), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %subtract.1466 = f32[4193280]{0} subtract(f32[4193280]{0} %gather.1465, f32[4193280]{0} %convert.1362), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %scatter.1492 = f32[4193280,32000]{1,0} scatter(f32[4193280,32000]{1,0} %divide.1464, s64[4193280,2]{1,0} %concatenate.1487, f32[4193280]{0} %subtract.1466), update_window_dims={}, inserted_window_dims={0,1}, scatter_dims_to_operand_dims={0,1}, index_vector_dim=1, to_apply=%ScatterCombiner.1489, metadata={op_type="aten__index_put" op_name="aten__index_put" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=125}
  %constant.95 = f32[] constant(1)
  %p44.1333 = f32[] parameter(44), frontend_attributes={neff_input_names="input44"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %divide.0 = f32[] divide(f32[] %constant.95, f32[] %p44.1333), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.1496 = f32[4193280,32000]{1,0} broadcast(f32[] %divide.0), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=128}
  %multiply.1497 = f32[4193280,32000]{1,0} multiply(f32[4193280,32000]{1,0} %scatter.1492, f32[4193280,32000]{1,0} %broadcast.1496), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=128}
  %reshape.1498 = f32[1024,4095,32000]{2,1,0} reshape(f32[4193280,32000]{1,0} %multiply.1497), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.1512 = f32[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect"}
  %pad.1513 = f32[1024,4096,32000]{2,1,0} pad(f32[1024,4095,32000]{2,1,0} %reshape.1498, f32[] %constant.1512), padding=0_0x0_1x0_0, metadata={op_type="xla__unselect" op_name="xla__unselect"}
  %constant.1504 = f32[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.1508 = f32[1024,4096,32000]{2,1,0} broadcast(f32[] %constant.1504), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %select.1515 = f32[1024,4096,32000]{2,1,0} select(pred[1024,4096,32000]{2,1,0} %broadcast, f32[1024,4096,32000]{2,1,0} %pad.1513, f32[1024,4096,32000]{2,1,0} %broadcast.1508), metadata={op_type="xla__unselect" op_name="xla__unselect"}
  %convert.1516 = bf16[1024,4096,32000]{2,1,0} convert(f32[1024,4096,32000]{2,1,0} %select.1515), metadata={op_type="xla__cast" op_name="xla__cast"}
  %reshape.1583 = bf16[4194304,32000]{1,0} reshape(bf16[1024,4096,32000]{2,1,0} %convert.1516), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.1584 = bf16[4194304,32]{1,0} dot(bf16[4194304,32000]{1,0} %reshape.1583, bf16[32000,32]{1,0} %p48.1399), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.1585 = bf16[1024,4096,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.1584), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.1586 = bf16[4096,1024,32]{2,0,1} transpose(bf16[1024,4096,32]{2,1,0} %reshape.1585), dimensions={1,0,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %broadcast.1587 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p43.1328), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1588 = bf16[4096,1024,32]{2,0,1} multiply(bf16[4096,1024,32]{2,0,1} %transpose.1586, bf16[4096,1024,32]{2,1,0} %broadcast.1587), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.1589 = f32[4096,1024,32]{2,0,1} convert(bf16[4096,1024,32]{2,0,1} %multiply.1588), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.1610 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.8), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1611 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1589, f32[4096,1024,32]{2,1,0} %broadcast.1610), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1590 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1589, f32[4096,1024,32]{2,0,1} %convert.1300), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.1591 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.1597 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,0,1} %multiply.1590, f32[] %constant.1591), dimensions={2}, to_apply=%AddComputation.1593, metadata={op_type="aten__sum" op_name="aten__sum"}
  %p52.1578 = f32[] parameter(52), frontend_attributes={neff_input_names="input52"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %broadcast.604 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.79 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.1597, f32[4096,1024]{1,0} %broadcast.604), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1876 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.79), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2131 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.8), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.52 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2131, f32[4096,1024,1]{2,1,0} %reshape.2131), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.53 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2131, f32[4096,1024,1]{2,1,0} %multiply.52), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.1600 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1876, f32[4096,1024,1]{2,1,0} %multiply.53), metadata={op_type="aten__mul" op_name="aten__mul"}
  %p51.1571 = f32[] parameter(51), frontend_attributes={neff_input_names="input51"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %broadcast.606 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.18 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.1600, f32[4096,1024,1]{2,1,0} %broadcast.606), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1977 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.18), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.48 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1977), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %p50.1559 = f32[] parameter(50), frontend_attributes={neff_input_names="input50"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %broadcast.1563 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.1570 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1300, f32[4096,1024,32]{2,1,0} %broadcast.1563), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1607 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.48, f32[4096,1024,32]{2,0,1} %multiply.1570), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.1612 = f32[4096,1024,32]{2,0,1} add(f32[4096,1024,32]{2,0,1} %multiply.1611, f32[4096,1024,32]{2,1,0} %multiply.1607), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.1613 = bf16[4096,1024,32]{2,0,1} convert(f32[4096,1024,32]{2,0,1} %add.1612), metadata={op_type="xla__cast" op_name="xla__cast"}
  %reshape.1651 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %convert.1613), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.1652 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.1651, bf16[32,32]{1,0} %p41.1282), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.1653 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.1652), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.1654 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) tuple(bf16[4096,1024,32]{2,1,0} %slice.1291, bf16[4096,1024,32]{2,1,0} %slice.1289, bf16[4096,1024,32]{2,1,0} %reshape.1653), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.1655 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) opt-barrier((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %tuple.1654), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.1658 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.1655), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.96 = pred[64]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.49 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.96), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.1657 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.1655), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.1672 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.1673 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1657, bf16[] %constant.1672), padding=0_0x0_0x32_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.97 = pred[64]{0} constant({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0})
  %broadcast.50 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.97), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.1656 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.1655), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.1663 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.1664 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1656, bf16[] %constant.1663), padding=0_0x0_0x0_32, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.1659 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.1287), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.1666 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.50, bf16[4096,1024,64]{2,1,0} %pad.1664, bf16[4096,1024,64]{2,1,0} %reshape.1659), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.1675 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.49, bf16[4096,1024,64]{2,1,0} %pad.1673, bf16[4096,1024,64]{2,1,0} %select.1666), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.1685 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.1675), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.1686 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1658, bf16[4096,1024,32]{2,1,0} %slice.1685), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %logistic.0 = bf16[4096,1024,64]{2,1,0} logistic(bf16[4096,1024,64]{2,1,0} %select.1675), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.1 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.0), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.1687 = bf16[] constant(1), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.1692 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.1687), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.1678 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.1675), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %broadcast.1689 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.1687), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %subtract.1690 = bf16[4096,1024,32]{2,1,0} subtract(bf16[4096,1024,32]{2,1,0} %broadcast.1689, bf16[4096,1024,32]{2,1,0} %slice.1), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.1691 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.1678, bf16[4096,1024,32]{2,1,0} %subtract.1690), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %add.1693 = bf16[4096,1024,32]{2,1,0} add(bf16[4096,1024,32]{2,1,0} %broadcast.1692, bf16[4096,1024,32]{2,1,0} %multiply.1691), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.1694 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.1, bf16[4096,1024,32]{2,1,0} %add.1693), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.1695 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.1686, bf16[4096,1024,32]{2,1,0} %multiply.1694), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.0 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.0), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.1680 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.1678, bf16[4096,1024,32]{2,1,0} %slice.0), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.1683 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1658, bf16[4096,1024,32]{2,1,0} %multiply.1680), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %concatenate.1696 = bf16[4096,1024,64]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %multiply.1695, bf16[4096,1024,32]{2,1,0} %multiply.1683), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.1768 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.1696), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.1769 = bf16[4194304,32]{1,0} dot(bf16[4194304,64]{1,0} %reshape.1768, bf16[64,32]{1,0} %p42.1284), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.1770 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.1769), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.1186 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1179), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.1187 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.1174, bf16[] %get-tuple-element.1186), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1194 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1187), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.1779 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.1770, bf16[] %get-tuple-element.1194), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.1775, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.1780 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.1779), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.1783 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p35.1172), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1784 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1780, bf16[4096,1024,32]{2,1,0} %broadcast.1783), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.1785 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.1784), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.1806 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.7), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1807 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.1785, f32[4096,1024,32]{2,1,0} %broadcast.1806), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1786 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.1785, f32[4096,1024,32]{2,0,1} %convert.1144), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.1787 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.1793 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.1786, f32[] %constant.1787), dimensions={2}, to_apply=%AddComputation.1789, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.608 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.80 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.1793, f32[4096,1024]{1,0} %broadcast.608), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1879 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.80), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2127 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.7), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.54 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2127, f32[4096,1024,1]{2,1,0} %reshape.2127), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.55 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2127, f32[4096,1024,1]{2,1,0} %multiply.54), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.1796 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1879, f32[4096,1024,1]{2,1,0} %multiply.55), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.610 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.19 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.1796, f32[4096,1024,1]{2,1,0} %broadcast.610), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1980 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.19), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.54 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1980), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.1750 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.1757 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.1144, f32[4096,1024,32]{2,1,0} %broadcast.1750), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1803 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.54, f32[4096,1024,32]{2,0,1} %multiply.1757), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.1808 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.1807, f32[4096,1024,32]{2,1,0} %multiply.1803), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.1809 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.1808), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.1811 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %convert.1613, bf16[4096,1024,32]{2,1,0} %convert.1809), metadata={op_type="aten__add" op_name="aten__add"}
  %iota.21 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.22 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.1881 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.21, s32[4096,4096]{1,0} %iota.22), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.1872 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.612 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.1872), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.1867 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.614 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.1867), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.13 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.1881, bf16[4096,4096]{1,0} %broadcast.612, bf16[4096,4096]{1,0} %broadcast.614), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.35 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.13)
  %broadcast.57 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.35), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.10/aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.1961 = bf16[] constant(0), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.10/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.1962 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.1961), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.10/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.1849 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.1811), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.1850 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.1849, bf16[32,32]{1,0} %p31.966), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.1852 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4194304,32]{1,0} %dot.1850), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.1853 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.1852), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.1854 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) tuple(bf16[1024,32,4096,2]{3,2,1,0} %add.1111, bf16[1024,32,4096,2]{3,2,1,0} %add.1071, bf16[1024,32,4096,1]{3,1,0,2} %transpose.975, bf16[1024,32,4096,1]{3,1,0,2} %transpose.1853), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.1855 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) opt-barrier((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %tuple.1854), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.1859 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.1855), index=3, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.412 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.1859)
  %broadcast.303 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.412), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.98 = pred[96]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.58 = pred[4096,1024,96]{2,1,0} broadcast(pred[96]{0} %constant.98), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %get-tuple-element.1858 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.1855), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.1927 = bf16[4096,1024,32,1]{3,2,1,0} transpose(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.1858), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.1928 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,2,1,0} %transpose.1927), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %constant.1933 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %pad.1934 = bf16[4096,1024,96]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %reshape.1928, bf16[] %constant.1933), padding=0_0x0_0x64_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.1929 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.971), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %select.1936 = bf16[4096,1024,96]{2,1,0} select(pred[4096,1024,96]{2,1,0} %broadcast.58, bf16[4096,1024,96]{2,1,0} %pad.1934, bf16[4096,1024,96]{2,1,0} %reshape.1929), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.1939 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %select.1936), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.1940 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.1939), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.1941 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.1940), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.413 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.1941)
  %broadcast.304 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.413), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.56 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %broadcast.303, bf16[32768,4096,4096]{2,1,0} %broadcast.304), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %convert.287 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.56), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2822 = f32[1024,32,4096,4096]{3,2,1,0} reshape(f32[32768,4096,4096]{2,1,0} %convert.287), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.61 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.35), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.9/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.1900 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.9/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.1901 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.1900), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.9/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %get-tuple-element.1856 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.1855), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.1892 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.1856), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %get-tuple-element.1857 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.1855), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.1888 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.1857), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1890 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.1888), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.1893 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.1892, bf16[32768,2,4096]{2,1,0} %reshape.1890), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.616 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.14 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.1893, bf16[32768,4096,4096]{2,1,0} %broadcast.616), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1887 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.14), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.1902 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.61, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.1901, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1887), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.9/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.1903 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.1902), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.16 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.1903), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %custom-call.17 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %reshape.2822, f32[1024,32,4096,4096]{3,2,1,0} %custom-call.16), custom_call_target="AwsNeuronSoftmaxBackward", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxBackwardImpl" op_name="xla___op_SoftmaxBackwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.1957 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.17), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %select.1963 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.57, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.1962, bf16[1024,32,4096,4096]{3,2,1,0} %convert.1957), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.10/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.1964 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %divide.1965 = bf16[1024,32,4096,4096]{3,2,1,0} divide(bf16[1024,32,4096,4096]{3,2,1,0} %select.1963, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.1964), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.1966 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %divide.1965), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2032 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.1888), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.2033 = bf16[32768,4096,2]{1,2,0} transpose(bf16[32768,2,4096]{2,1,0} %reshape.2032), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.2034 = bf16[32768,4096,2]{2,1,0} dot(bf16[32768,4096,4096]{2,1,0} %reshape.1966, bf16[32768,4096,2]{1,2,0} %transpose.2033), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2035 = bf16[1024,32,4096,2]{3,2,1,0} reshape(bf16[32768,4096,2]{2,1,0} %dot.2034), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.2055 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1020), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.2056 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.2055), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2057 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.2035, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.2056), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2058 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2064 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.2057, bf16[] %constant.2058), dimensions={3}, to_apply=%AddComputation.2060, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.9 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.2064), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.2037 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1058), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.2038 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.2037), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2039 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.2035, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.2038), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2040 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2046 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.2039, bf16[] %constant.2040), dimensions={3}, to_apply=%AddComputation.2042, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.13 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.9, bf16[1024,32,4096]{2,1,0} %reduce.2046), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2142 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.13), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.2089 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2142), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.2090 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2089), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.1967 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.1856), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.1968 = bf16[32768,2,4096]{1,2,0} transpose(bf16[32768,4096,2]{2,1,0} %reshape.1967), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.1969 = bf16[32768,2,4096]{2,1,0} dot(bf16[32768,2,4096]{1,2,0} %transpose.1968, bf16[32768,4096,4096]{2,1,0} %reshape.1966), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.1970 = bf16[1024,32,2,4096]{3,2,1,0} reshape(bf16[32768,2,4096]{2,1,0} %dot.1969), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.1971 = bf16[1024,32,4096,2]{2,3,1,0} transpose(bf16[1024,32,2,4096]{3,2,1,0} %reshape.1970), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.1991 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1020), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.1992 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.1991), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1993 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.1971, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1992), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.1994 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2000 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.1993, bf16[] %constant.1994), dimensions={3}, to_apply=%AddComputation.1996, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.8 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.2000), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.1973 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.1058), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.1974 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.1973), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.1975 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.1971, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.1974), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.1976 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.1982 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.1975, bf16[] %constant.1976), dimensions={3}, to_apply=%AddComputation.1978, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.12 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.8, bf16[1024,32,4096]{2,1,0} %reduce.1982), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2136 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.12), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.2025 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2136), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.2026 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2025), metadata={op_type="aten__view" op_name="aten__view"}
  %convert.1911 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.16), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.1913 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.1911), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.457 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.1859)
  %broadcast.618 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.457), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.88 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.1913, bf16[32768,4096,4096]{2,1,0} %broadcast.618)
  %convert.41 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.88)
  %constant.47 = f32[] constant(0)
  %reduce.4 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.41, f32[] %constant.47), dimensions={1}, to_apply=%scalar_add_computation.1
  %convert.10 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.4), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.1916 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.10), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.1917 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.1916), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.1918 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.1917), metadata={op_type="aten__view" op_name="aten__view"}
  %concatenate.2091 = bf16[4096,1024,96]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %reshape.2090, bf16[4096,1024,32]{2,1,0} %reshape.2026, bf16[4096,1024,32]{2,1,0} %reshape.1918), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.2160 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.2091), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2161 = bf16[4194304,32]{1,0} dot(bf16[4194304,96]{1,0} %reshape.2160, bf16[96,32]{1,0} %p32.968), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2162 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2161), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1195 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.945, bf16[] %get-tuple-element.1194), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1202 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1195), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.2171 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.2162, bf16[] %get-tuple-element.1202), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2167, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.2172 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.2171), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.2175 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p30.943), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2176 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2172, bf16[4096,1024,32]{2,1,0} %broadcast.2175), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.2177 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.2176), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.2198 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.6), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2199 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2177, f32[4096,1024,32]{2,1,0} %broadcast.2198), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2178 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2177, f32[4096,1024,32]{2,0,1} %convert.915), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2179 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2185 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.2178, f32[] %constant.2179), dimensions={2}, to_apply=%AddComputation.2181, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.619 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.81 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.2185, f32[4096,1024]{1,0} %broadcast.619), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1899 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.81), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2123 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.6), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.59 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2123, f32[4096,1024,1]{2,1,0} %reshape.2123), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.60 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2123, f32[4096,1024,1]{2,1,0} %multiply.59), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.2188 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1899, f32[4096,1024,1]{2,1,0} %multiply.60), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.621 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.20 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.2188, f32[4096,1024,1]{2,1,0} %broadcast.621), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1984 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.20), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.85 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1984), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2142 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2149 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.915, f32[4096,1024,32]{2,1,0} %broadcast.2142), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2195 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.85, f32[4096,1024,32]{2,0,1} %multiply.2149), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2200 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.2199, f32[4096,1024,32]{2,1,0} %multiply.2195), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.2201 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.2200), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.2203 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.1811, bf16[4096,1024,32]{2,1,0} %convert.2201), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2241 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2203), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2242 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.2241, bf16[32,32]{1,0} %p28.897), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2243 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2242), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.2244 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) tuple(bf16[4096,1024,32]{2,1,0} %slice.906, bf16[4096,1024,32]{2,1,0} %slice.904, bf16[4096,1024,32]{2,1,0} %reshape.2243), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.2245 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) opt-barrier((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %tuple.2244), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.2248 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.2245), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.116 = pred[64]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.87 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.116), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.2247 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.2245), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.2262 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.2263 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2247, bf16[] %constant.2262), padding=0_0x0_0x32_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.117 = pred[64]{0} constant({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0})
  %broadcast.88 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.117), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.2246 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.2245), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.2253 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.2254 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2246, bf16[] %constant.2253), padding=0_0x0_0x0_32, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.2249 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.902), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.2256 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.88, bf16[4096,1024,64]{2,1,0} %pad.2254, bf16[4096,1024,64]{2,1,0} %reshape.2249), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.2265 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.87, bf16[4096,1024,64]{2,1,0} %pad.2263, bf16[4096,1024,64]{2,1,0} %select.2256), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.2275 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.2265), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.2276 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2248, bf16[4096,1024,32]{2,1,0} %slice.2275), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %logistic.1 = bf16[4096,1024,64]{2,1,0} logistic(bf16[4096,1024,64]{2,1,0} %select.2265), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.3 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.1), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.2277 = bf16[] constant(1), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.2282 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.2277), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.2268 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.2265), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %broadcast.2279 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.2277), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %subtract.2280 = bf16[4096,1024,32]{2,1,0} subtract(bf16[4096,1024,32]{2,1,0} %broadcast.2279, bf16[4096,1024,32]{2,1,0} %slice.3), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.2281 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.2268, bf16[4096,1024,32]{2,1,0} %subtract.2280), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %add.2283 = bf16[4096,1024,32]{2,1,0} add(bf16[4096,1024,32]{2,1,0} %broadcast.2282, bf16[4096,1024,32]{2,1,0} %multiply.2281), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.2284 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.3, bf16[4096,1024,32]{2,1,0} %add.2283), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.2285 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.2276, bf16[4096,1024,32]{2,1,0} %multiply.2284), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.2 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.1), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.2270 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.2268, bf16[4096,1024,32]{2,1,0} %slice.2), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.2273 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2248, bf16[4096,1024,32]{2,1,0} %multiply.2270), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %concatenate.2286 = bf16[4096,1024,64]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %multiply.2285, bf16[4096,1024,32]{2,1,0} %multiply.2273), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.2358 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.2286), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2359 = bf16[4194304,32]{1,0} dot(bf16[4194304,64]{1,0} %reshape.2358, bf16[64,32]{1,0} %p29.899), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2360 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2359), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1203 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.876, bf16[] %get-tuple-element.1202), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1210 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1203), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.2369 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.2360, bf16[] %get-tuple-element.1210), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2365, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.2370 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.2369), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.2373 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p27.874), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2374 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2370, bf16[4096,1024,32]{2,1,0} %broadcast.2373), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.2375 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.2374), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.2396 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.5), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2397 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2375, f32[4096,1024,32]{2,1,0} %broadcast.2396), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2376 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2375, f32[4096,1024,32]{2,0,1} %convert.846), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2377 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2383 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.2376, f32[] %constant.2377), dimensions={2}, to_apply=%AddComputation.2379, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.623 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.82 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.2383, f32[4096,1024]{1,0} %broadcast.623), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1902 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.82), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2121 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.5), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.61 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2121, f32[4096,1024,1]{2,1,0} %reshape.2121), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.62 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2121, f32[4096,1024,1]{2,1,0} %multiply.61), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.2386 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1902, f32[4096,1024,1]{2,1,0} %multiply.62), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.625 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.21 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.2386, f32[4096,1024,1]{2,1,0} %broadcast.625), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1989 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.21), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.96 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1989), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2340 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2347 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.846, f32[4096,1024,32]{2,1,0} %broadcast.2340), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2393 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.96, f32[4096,1024,32]{2,0,1} %multiply.2347), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2398 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.2397, f32[4096,1024,32]{2,1,0} %multiply.2393), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.2399 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.2398), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.2401 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.2203, bf16[4096,1024,32]{2,1,0} %convert.2399), metadata={op_type="aten__add" op_name="aten__add"}
  %iota.23 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.24 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.2471 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.23, s32[4096,4096]{1,0} %iota.24), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.2462 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.627 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.2462), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.2457 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.629 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.2457), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.14 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.2471, bf16[4096,4096]{1,0} %broadcast.627, bf16[4096,4096]{1,0} %broadcast.629), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.34 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.14)
  %broadcast.101 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.34), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.12/aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.2551 = bf16[] constant(0), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.12/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.2552 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.2551), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.12/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2439 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2401), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2440 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.2439, bf16[32,32]{1,0} %p23.668), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2442 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2440), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.2443 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2442), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.2444 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) tuple(bf16[1024,32,4096,2]{3,2,1,0} %add.813, bf16[1024,32,4096,2]{3,2,1,0} %add.773, bf16[1024,32,4096,1]{3,1,0,2} %transpose.677, bf16[1024,32,4096,1]{3,1,0,2} %transpose.2443), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.2445 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) opt-barrier((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %tuple.2444), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.2449 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.2445), index=3, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.536 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.2449)
  %broadcast.327 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.536), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.118 = pred[96]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.102 = pred[4096,1024,96]{2,1,0} broadcast(pred[96]{0} %constant.118), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %get-tuple-element.2448 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.2445), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.2517 = bf16[4096,1024,32,1]{3,2,1,0} transpose(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.2448), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.2518 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,2,1,0} %transpose.2517), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %constant.2523 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %pad.2524 = bf16[4096,1024,96]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %reshape.2518, bf16[] %constant.2523), padding=0_0x0_0x64_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.2519 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.673), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %select.2526 = bf16[4096,1024,96]{2,1,0} select(pred[4096,1024,96]{2,1,0} %broadcast.102, bf16[4096,1024,96]{2,1,0} %pad.2524, bf16[4096,1024,96]{2,1,0} %reshape.2519), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.2529 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %select.2526), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.2530 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.2529), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.2531 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.2530), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.537 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.2531)
  %broadcast.328 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.537), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.63 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %broadcast.327, bf16[32768,4096,4096]{2,1,0} %broadcast.328), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %convert.289 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.63), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2824 = f32[1024,32,4096,4096]{3,2,1,0} reshape(f32[32768,4096,4096]{2,1,0} %convert.289), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.105 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.34), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.11/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.2490 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.11/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.2491 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.2490), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.11/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %get-tuple-element.2446 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.2445), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.2482 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.2446), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %get-tuple-element.2447 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.2445), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.2478 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.2447), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.2480 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.2478), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.2483 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.2482, bf16[32768,2,4096]{2,1,0} %reshape.2480), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.631 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.15 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.2483, bf16[32768,4096,4096]{2,1,0} %broadcast.631), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1909 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.15), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.2492 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.105, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.2491, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1909), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.11/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.2493 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.2492), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.18 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.2493), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %custom-call.19 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %reshape.2824, f32[1024,32,4096,4096]{3,2,1,0} %custom-call.18), custom_call_target="AwsNeuronSoftmaxBackward", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxBackwardImpl" op_name="xla___op_SoftmaxBackwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.2547 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.19), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %select.2553 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.101, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.2552, bf16[1024,32,4096,4096]{3,2,1,0} %convert.2547), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.12/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.2554 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %divide.2555 = bf16[1024,32,4096,4096]{3,2,1,0} divide(bf16[1024,32,4096,4096]{3,2,1,0} %select.2553, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.2554), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2556 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %divide.2555), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2622 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.2478), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.2623 = bf16[32768,4096,2]{1,2,0} transpose(bf16[32768,2,4096]{2,1,0} %reshape.2622), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.2624 = bf16[32768,4096,2]{2,1,0} dot(bf16[32768,4096,4096]{2,1,0} %reshape.2556, bf16[32768,4096,2]{1,2,0} %transpose.2623), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2625 = bf16[1024,32,4096,2]{3,2,1,0} reshape(bf16[32768,4096,2]{2,1,0} %dot.2624), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.2645 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.722), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.2646 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.2645), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2647 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.2625, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.2646), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2648 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2654 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.2647, bf16[] %constant.2648), dimensions={3}, to_apply=%AddComputation.2650, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.11 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.2654), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.2627 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.760), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.2628 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.2627), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2629 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.2625, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.2628), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2630 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2636 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.2629, bf16[] %constant.2630), dimensions={3}, to_apply=%AddComputation.2632, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.15 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.11, bf16[1024,32,4096]{2,1,0} %reduce.2636), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2150 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.15), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.2679 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2150), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.2680 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2679), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.2557 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.2446), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.2558 = bf16[32768,2,4096]{1,2,0} transpose(bf16[32768,4096,2]{2,1,0} %reshape.2557), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.2559 = bf16[32768,2,4096]{2,1,0} dot(bf16[32768,2,4096]{1,2,0} %transpose.2558, bf16[32768,4096,4096]{2,1,0} %reshape.2556), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2560 = bf16[1024,32,2,4096]{3,2,1,0} reshape(bf16[32768,2,4096]{2,1,0} %dot.2559), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.2561 = bf16[1024,32,4096,2]{2,3,1,0} transpose(bf16[1024,32,2,4096]{3,2,1,0} %reshape.2560), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.2581 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.722), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.2582 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.2581), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2583 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.2561, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.2582), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2584 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2590 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.2583, bf16[] %constant.2584), dimensions={3}, to_apply=%AddComputation.2586, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.10 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.2590), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.2563 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.760), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.2564 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.2563), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2565 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.2561, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.2564), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2566 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2572 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.2565, bf16[] %constant.2566), dimensions={3}, to_apply=%AddComputation.2568, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.14 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.10, bf16[1024,32,4096]{2,1,0} %reduce.2572), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2147 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.14), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.2615 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2147), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.2616 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2615), metadata={op_type="aten__view" op_name="aten__view"}
  %convert.2501 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.18), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.2503 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.2501), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.575 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.2449)
  %broadcast.634 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.575), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.89 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.2503, bf16[32768,4096,4096]{2,1,0} %broadcast.634)
  %convert.42 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.89)
  %constant.48 = f32[] constant(0)
  %reduce.5 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.42, f32[] %constant.48), dimensions={1}, to_apply=%scalar_add_computation.1
  %convert.12 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.5), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2506 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.12), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.2507 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2506), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.2508 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2507), metadata={op_type="aten__view" op_name="aten__view"}
  %concatenate.2681 = bf16[4096,1024,96]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %reshape.2680, bf16[4096,1024,32]{2,1,0} %reshape.2616, bf16[4096,1024,32]{2,1,0} %reshape.2508), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.2750 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.2681), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2751 = bf16[4194304,32]{1,0} dot(bf16[4194304,96]{1,0} %reshape.2750, bf16[96,32]{1,0} %p24.670), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2752 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2751), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1211 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.647, bf16[] %get-tuple-element.1210), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1218 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1211), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.2761 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.2752, bf16[] %get-tuple-element.1218), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2757, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.2762 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.2761), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.2765 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p22.645), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2766 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2762, bf16[4096,1024,32]{2,1,0} %broadcast.2765), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.2767 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.2766), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.2788 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.4), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2789 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2767, f32[4096,1024,32]{2,1,0} %broadcast.2788), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2768 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2767, f32[4096,1024,32]{2,0,1} %convert.617), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2769 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2775 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.2768, f32[] %constant.2769), dimensions={2}, to_apply=%AddComputation.2771, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.635 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.83 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.2775, f32[4096,1024]{1,0} %broadcast.635), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1923 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.83), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2119 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.4), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.65 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2119, f32[4096,1024,1]{2,1,0} %reshape.2119), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.66 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2119, f32[4096,1024,1]{2,1,0} %multiply.65), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.2778 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1923, f32[4096,1024,1]{2,1,0} %multiply.66), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.638 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.22 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.2778, f32[4096,1024,1]{2,1,0} %broadcast.638), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1993 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.22), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.127 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1993), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2732 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2739 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.617, f32[4096,1024,32]{2,1,0} %broadcast.2732), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2785 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.127, f32[4096,1024,32]{2,0,1} %multiply.2739), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2790 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.2789, f32[4096,1024,32]{2,1,0} %multiply.2785), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.2791 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.2790), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.2793 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.2401, bf16[4096,1024,32]{2,1,0} %convert.2791), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2831 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2793), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2832 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.2831, bf16[32,32]{1,0} %p20.599), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2833 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2832), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.2834 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) tuple(bf16[4096,1024,32]{2,1,0} %slice.608, bf16[4096,1024,32]{2,1,0} %slice.606, bf16[4096,1024,32]{2,1,0} %reshape.2833), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.2835 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) opt-barrier((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %tuple.2834), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.2838 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.2835), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.133 = pred[64]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.128 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.133), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.2837 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.2835), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.2852 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.2853 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2837, bf16[] %constant.2852), padding=0_0x0_0x32_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.134 = pred[64]{0} constant({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0})
  %broadcast.129 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.134), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.2836 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.2835), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.2843 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.2844 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2836, bf16[] %constant.2843), padding=0_0x0_0x0_32, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.2839 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.604), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.2846 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.129, bf16[4096,1024,64]{2,1,0} %pad.2844, bf16[4096,1024,64]{2,1,0} %reshape.2839), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.2855 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.128, bf16[4096,1024,64]{2,1,0} %pad.2853, bf16[4096,1024,64]{2,1,0} %select.2846), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.2865 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.2855), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.2866 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2838, bf16[4096,1024,32]{2,1,0} %slice.2865), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %logistic.2 = bf16[4096,1024,64]{2,1,0} logistic(bf16[4096,1024,64]{2,1,0} %select.2855), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.5 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.2), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.2867 = bf16[] constant(1), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.2872 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.2867), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.2858 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.2855), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %broadcast.2869 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.2867), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %subtract.2870 = bf16[4096,1024,32]{2,1,0} subtract(bf16[4096,1024,32]{2,1,0} %broadcast.2869, bf16[4096,1024,32]{2,1,0} %slice.5), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.2871 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.2858, bf16[4096,1024,32]{2,1,0} %subtract.2870), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %add.2873 = bf16[4096,1024,32]{2,1,0} add(bf16[4096,1024,32]{2,1,0} %broadcast.2872, bf16[4096,1024,32]{2,1,0} %multiply.2871), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.2874 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.5, bf16[4096,1024,32]{2,1,0} %add.2873), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.2875 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.2866, bf16[4096,1024,32]{2,1,0} %multiply.2874), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.4 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.2), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.2860 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.2858, bf16[4096,1024,32]{2,1,0} %slice.4), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.2863 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2838, bf16[4096,1024,32]{2,1,0} %multiply.2860), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %concatenate.2876 = bf16[4096,1024,64]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %multiply.2875, bf16[4096,1024,32]{2,1,0} %multiply.2863), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.2948 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.2876), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.2949 = bf16[4194304,32]{1,0} dot(bf16[4194304,64]{1,0} %reshape.2948, bf16[64,32]{1,0} %p21.601), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.2950 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.2949), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1219 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.578, bf16[] %get-tuple-element.1218), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1226 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1219), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.2959 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.2950, bf16[] %get-tuple-element.1226), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2955, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.2960 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.2959), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.2963 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p19.576), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2964 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2960, bf16[4096,1024,32]{2,1,0} %broadcast.2963), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.2965 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.2964), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.2986 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.3), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2987 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2965, f32[4096,1024,32]{2,1,0} %broadcast.2986), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2966 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.2965, f32[4096,1024,32]{2,0,1} %convert.548), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.2967 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.2973 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.2966, f32[] %constant.2967), dimensions={2}, to_apply=%AddComputation.2969, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.641 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.84 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.2973, f32[4096,1024]{1,0} %broadcast.641), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1927 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.84), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2117 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.3), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.67 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2117, f32[4096,1024,1]{2,1,0} %reshape.2117), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.68 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2117, f32[4096,1024,1]{2,1,0} %multiply.67), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.2976 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1927, f32[4096,1024,1]{2,1,0} %multiply.68), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.644 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.23 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.2976, f32[4096,1024,1]{2,1,0} %broadcast.644), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1996 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.23), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.133 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1996), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.2930 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.2937 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.548, f32[4096,1024,32]{2,1,0} %broadcast.2930), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.2983 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.133, f32[4096,1024,32]{2,0,1} %multiply.2937), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.2988 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.2987, f32[4096,1024,32]{2,1,0} %multiply.2983), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.2989 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.2988), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.2991 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.2793, bf16[4096,1024,32]{2,1,0} %convert.2989), metadata={op_type="aten__add" op_name="aten__add"}
  %iota.25 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.26 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.3061 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.25, s32[4096,4096]{1,0} %iota.26), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.3052 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.647 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.3052), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.3047 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.649 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.3047), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.15 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.3061, bf16[4096,4096]{1,0} %broadcast.647, bf16[4096,4096]{1,0} %broadcast.649), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.33 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.15)
  %broadcast.136 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.33), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.14/aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.3141 = bf16[] constant(0), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.14/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.3142 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.3141), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.14/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3029 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2991), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.3030 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.3029, bf16[32,32]{1,0} %p15.370), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.3032 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4194304,32]{1,0} %dot.3030), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.3033 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.3032), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.3034 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) tuple(bf16[1024,32,4096,2]{3,2,1,0} %add.515, bf16[1024,32,4096,2]{3,2,1,0} %add.475, bf16[1024,32,4096,1]{3,1,0,2} %transpose.379, bf16[1024,32,4096,1]{3,1,0,2} %transpose.3033), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.3035 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) opt-barrier((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %tuple.3034), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.3039 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3035), index=3, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.643 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.3039)
  %broadcast.356 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.643), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.135 = pred[96]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.137 = pred[4096,1024,96]{2,1,0} broadcast(pred[96]{0} %constant.135), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %get-tuple-element.3038 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3035), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.3107 = bf16[4096,1024,32,1]{3,2,1,0} transpose(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.3038), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.3108 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,2,1,0} %transpose.3107), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %constant.3113 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %pad.3114 = bf16[4096,1024,96]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %reshape.3108, bf16[] %constant.3113), padding=0_0x0_0x64_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.3109 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.375), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %select.3116 = bf16[4096,1024,96]{2,1,0} select(pred[4096,1024,96]{2,1,0} %broadcast.137, bf16[4096,1024,96]{2,1,0} %pad.3114, bf16[4096,1024,96]{2,1,0} %reshape.3109), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.3119 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %select.3116), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.3120 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.3119), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.3121 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.3120), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.644 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.3121)
  %broadcast.357 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.644), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.69 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %broadcast.356, bf16[32768,4096,4096]{2,1,0} %broadcast.357), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %convert.290 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.69), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2827 = f32[1024,32,4096,4096]{3,2,1,0} reshape(f32[32768,4096,4096]{2,1,0} %convert.290), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.139 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.33), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.13/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.3080 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.13/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.3081 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.3080), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.13/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %get-tuple-element.3036 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3035), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.3072 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.3036), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %get-tuple-element.3037 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3035), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.3068 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.3037), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.3070 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.3068), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.3073 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.3072, bf16[32768,2,4096]{2,1,0} %reshape.3070), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.651 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.16 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.3073, bf16[32768,4096,4096]{2,1,0} %broadcast.651), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1936 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.16), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.3082 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.139, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.3081, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1936), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.13/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.3083 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.3082), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.20 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.3083), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %custom-call.21 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %reshape.2827, f32[1024,32,4096,4096]{3,2,1,0} %custom-call.20), custom_call_target="AwsNeuronSoftmaxBackward", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxBackwardImpl" op_name="xla___op_SoftmaxBackwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.3137 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.21), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %select.3143 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.136, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.3142, bf16[1024,32,4096,4096]{3,2,1,0} %convert.3137), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.14/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.3144 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %divide.3145 = bf16[1024,32,4096,4096]{3,2,1,0} divide(bf16[1024,32,4096,4096]{3,2,1,0} %select.3143, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.3144), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3146 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %divide.3145), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3212 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.3068), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.3213 = bf16[32768,4096,2]{1,2,0} transpose(bf16[32768,2,4096]{2,1,0} %reshape.3212), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.3214 = bf16[32768,4096,2]{2,1,0} dot(bf16[32768,4096,4096]{2,1,0} %reshape.3146, bf16[32768,4096,2]{1,2,0} %transpose.3213), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3215 = bf16[1024,32,4096,2]{3,2,1,0} reshape(bf16[32768,4096,2]{2,1,0} %dot.3214), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.3235 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.424), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3236 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3235), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3237 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.3215, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3236), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3238 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3244 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.3237, bf16[] %constant.3238), dimensions={3}, to_apply=%AddComputation.3240, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.13 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.3244), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.3217 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.462), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3218 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3217), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3219 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.3215, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3218), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3220 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3226 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.3219, bf16[] %constant.3220), dimensions={3}, to_apply=%AddComputation.3222, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.17 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.13, bf16[1024,32,4096]{2,1,0} %reduce.3226), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2161 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.17), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.3269 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2161), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.3270 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3269), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.3147 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.3036), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.3148 = bf16[32768,2,4096]{1,2,0} transpose(bf16[32768,4096,2]{2,1,0} %reshape.3147), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.3149 = bf16[32768,2,4096]{2,1,0} dot(bf16[32768,2,4096]{1,2,0} %transpose.3148, bf16[32768,4096,4096]{2,1,0} %reshape.3146), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3150 = bf16[1024,32,2,4096]{3,2,1,0} reshape(bf16[32768,2,4096]{2,1,0} %dot.3149), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.3151 = bf16[1024,32,4096,2]{2,3,1,0} transpose(bf16[1024,32,2,4096]{3,2,1,0} %reshape.3150), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.3171 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.424), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3172 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3171), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3173 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.3151, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3172), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3174 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3180 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.3173, bf16[] %constant.3174), dimensions={3}, to_apply=%AddComputation.3176, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.12 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.3180), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.3153 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.462), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3154 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3153), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3155 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.3151, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3154), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3156 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3162 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.3155, bf16[] %constant.3156), dimensions={3}, to_apply=%AddComputation.3158, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.16 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.12, bf16[1024,32,4096]{2,1,0} %reduce.3162), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2155 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.16), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.3205 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2155), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.3206 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3205), metadata={op_type="aten__view" op_name="aten__view"}
  %convert.3091 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.20), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.3093 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.3091), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.685 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.3039)
  %broadcast.653 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.685), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.90 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.3093, bf16[32768,4096,4096]{2,1,0} %broadcast.653)
  %convert.43 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.90)
  %constant.49 = f32[] constant(0)
  %reduce.6 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.43, f32[] %constant.49), dimensions={1}, to_apply=%scalar_add_computation.1
  %convert.15 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.6), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3096 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.15), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.3097 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.3096), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.3098 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3097), metadata={op_type="aten__view" op_name="aten__view"}
  %concatenate.3271 = bf16[4096,1024,96]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %reshape.3270, bf16[4096,1024,32]{2,1,0} %reshape.3206, bf16[4096,1024,32]{2,1,0} %reshape.3098), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.3340 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.3271), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.3341 = bf16[4194304,32]{1,0} dot(bf16[4194304,96]{1,0} %reshape.3340, bf16[96,32]{1,0} %p16.372), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.3342 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.3341), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1227 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.349, bf16[] %get-tuple-element.1226), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1234 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1227), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.3351 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.3342, bf16[] %get-tuple-element.1234), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3347, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.3352 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.3351), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.3355 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p14.347), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3356 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3352, bf16[4096,1024,32]{2,1,0} %broadcast.3355), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.3357 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.3356), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.3378 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.2), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3379 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.3357, f32[4096,1024,32]{2,1,0} %broadcast.3378), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3358 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.3357, f32[4096,1024,32]{2,0,1} %convert.319), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3359 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3365 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.3358, f32[] %constant.3359), dimensions={2}, to_apply=%AddComputation.3361, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.654 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.85 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.3365, f32[4096,1024]{1,0} %broadcast.654), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1951 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.85), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2115 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.2), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.71 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2115, f32[4096,1024,1]{2,1,0} %reshape.2115), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.72 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2115, f32[4096,1024,1]{2,1,0} %multiply.71), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.3368 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1951, f32[4096,1024,1]{2,1,0} %multiply.72), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.656 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.24 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.3368, f32[4096,1024,1]{2,1,0} %broadcast.656), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.1999 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.24), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.164 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.1999), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3322 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3329 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.319, f32[4096,1024,32]{2,1,0} %broadcast.3322), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3375 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.164, f32[4096,1024,32]{2,0,1} %multiply.3329), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3380 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.3379, f32[4096,1024,32]{2,1,0} %multiply.3375), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.3381 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.3380), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.3383 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.2991, bf16[4096,1024,32]{2,1,0} %convert.3381), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.3421 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.3383), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.3422 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.3421, bf16[32,32]{1,0} %p12.301), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.3423 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.3422), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.3424 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) tuple(bf16[4096,1024,32]{2,1,0} %slice.310, bf16[4096,1024,32]{2,1,0} %slice.308, bf16[4096,1024,32]{2,1,0} %reshape.3423), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.3425 = (bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) opt-barrier((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %tuple.3424), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.3428 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.3425), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.150 = pred[64]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.165 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.150), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.3427 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.3425), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.3442 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.3443 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3427, bf16[] %constant.3442), padding=0_0x0_0x32_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.151 = pred[64]{0} constant({1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0})
  %broadcast.166 = pred[4096,1024,64]{2,1,0} broadcast(pred[64]{0} %constant.151), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %get-tuple-element.3426 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}, bf16[4096,1024,32]{2,1,0}) %opt-barrier.3425), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %constant.3433 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %pad.3434 = bf16[4096,1024,64]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3426, bf16[] %constant.3433), padding=0_0x0_0x0_32, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %reshape.3429 = bf16[4096,1024,64]{2,1,0} reshape(bf16[4194304,64]{1,0} %dot.306), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.3436 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.166, bf16[4096,1024,64]{2,1,0} %pad.3434, bf16[4096,1024,64]{2,1,0} %reshape.3429), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %select.3445 = bf16[4096,1024,64]{2,1,0} select(pred[4096,1024,64]{2,1,0} %broadcast.165, bf16[4096,1024,64]{2,1,0} %pad.3443, bf16[4096,1024,64]{2,1,0} %select.3436), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.3455 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.3445), slice={[0:4096], [0:1024], [32:64]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=180}
  %multiply.3456 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3428, bf16[4096,1024,32]{2,1,0} %slice.3455), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %logistic.3 = bf16[4096,1024,64]{2,1,0} logistic(bf16[4096,1024,64]{2,1,0} %select.3445), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %slice.7 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.3), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %constant.3457 = bf16[] constant(1), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.3462 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.3457), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.3448 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %select.3445), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %broadcast.3459 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[] %constant.3457), dimensions={}, metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %subtract.3460 = bf16[4096,1024,32]{2,1,0} subtract(bf16[4096,1024,32]{2,1,0} %broadcast.3459, bf16[4096,1024,32]{2,1,0} %slice.7), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.3461 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.3448, bf16[4096,1024,32]{2,1,0} %subtract.3460), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %add.3463 = bf16[4096,1024,32]{2,1,0} add(bf16[4096,1024,32]{2,1,0} %broadcast.3462, bf16[4096,1024,32]{2,1,0} %multiply.3461), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.3464 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.7, bf16[4096,1024,32]{2,1,0} %add.3463), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.3465 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %multiply.3456, bf16[4096,1024,32]{2,1,0} %multiply.3464), metadata={op_type="aten__silu_backward" op_name="aten__silu_backward" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %slice.6 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,64]{2,1,0} %logistic.3), slice={[0:4096], [0:1024], [0:32]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.3450 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %slice.3448, bf16[4096,1024,32]{2,1,0} %slice.6), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/home/ubuntu/kahfi/pytorch/torch/nn/functional.py" source_line=2072}
  %multiply.3453 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3428, bf16[4096,1024,32]{2,1,0} %multiply.3450), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %concatenate.3466 = bf16[4096,1024,64]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %multiply.3465, bf16[4096,1024,32]{2,1,0} %multiply.3453), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.3538 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.3466), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.3539 = bf16[4194304,32]{1,0} dot(bf16[4194304,64]{1,0} %reshape.3538, bf16[64,32]{1,0} %p13.303), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.3540 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.3539), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1235 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.280, bf16[] %get-tuple-element.1234), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.1242 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1235), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.3549 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.3540, bf16[] %get-tuple-element.1242), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3545, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.3550 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.3549), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.3553 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p11.278), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3554 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3550, bf16[4096,1024,32]{2,1,0} %broadcast.3553), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.3555 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.3554), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.3576 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.1), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3577 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.3555, f32[4096,1024,32]{2,1,0} %broadcast.3576), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3556 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.3555, f32[4096,1024,32]{2,0,1} %convert.250), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3557 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3563 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.3556, f32[] %constant.3557), dimensions={2}, to_apply=%AddComputation.3559, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.658 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.86 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.3563, f32[4096,1024]{1,0} %broadcast.658), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1954 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.86), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2113 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.1), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.73 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2113, f32[4096,1024,1]{2,1,0} %reshape.2113), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.74 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2113, f32[4096,1024,1]{2,1,0} %multiply.73), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.3566 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1954, f32[4096,1024,1]{2,1,0} %multiply.74), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.661 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.25 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.3566, f32[4096,1024,1]{2,1,0} %broadcast.661), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.2003 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.25), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.171 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.2003), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3520 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3527 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.250, f32[4096,1024,32]{2,1,0} %broadcast.3520), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3573 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.171, f32[4096,1024,32]{2,0,1} %multiply.3527), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3578 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.3577, f32[4096,1024,32]{2,1,0} %multiply.3573), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.3579 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.3578), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.3581 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.3383, bf16[4096,1024,32]{2,1,0} %convert.3579), metadata={op_type="aten__add" op_name="aten__add"}
  %iota.27 = s32[4096,4096]{1,0} iota(), iota_dimension=0, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %iota.28 = s32[4096,4096]{1,0} iota(), iota_dimension=1, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %compare.3651 = pred[4096,4096]{1,0} compare(s32[4096,4096]{1,0} %iota.27, s32[4096,4096]{1,0} %iota.28), direction=GE, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.3642 = bf16[] constant(0), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.664 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.3642), dimensions={}, metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %constant.3637 = bf16[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %broadcast.668 = bf16[4096,4096]{1,0} broadcast(bf16[] %constant.3637), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %select.16 = bf16[4096,4096]{1,0} select(pred[4096,4096]{1,0} %compare.3651, bf16[4096,4096]{1,0} %broadcast.664, bf16[4096,4096]{1,0} %broadcast.668), metadata={op_type="aten__triu" op_name="aten__triu" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=208}
  %convert.32 = pred[4096,4096]{1,0} convert(bf16[4096,4096]{1,0} %select.16)
  %broadcast.177 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.32), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.16/aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.3731 = bf16[] constant(0), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.16/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.3732 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.3731), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.16/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3619 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.3581), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.3620 = bf16[4194304,32]{1,0} dot(bf16[4194304,32]{1,0} %reshape.3619, bf16[32,32]{1,0} %p5.68), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.3622 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4194304,32]{1,0} %dot.3620), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.3623 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.3622), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %tuple.3624 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) tuple(bf16[1024,32,4096,2]{3,2,1,0} %add.217, bf16[1024,32,4096,2]{3,2,1,0} %add.177, bf16[1024,32,4096,1]{3,1,0,2} %transpose.77, bf16[1024,32,4096,1]{3,1,0,2} %transpose.3623), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %opt-barrier.3625 = (bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) opt-barrier((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %tuple.3624), metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %get-tuple-element.3629 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3625), index=3, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.765 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.3629)
  %broadcast.387 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.765), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %constant.152 = pred[96]{0} constant({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1})
  %broadcast.178 = pred[4096,1024,96]{2,1,0} broadcast(pred[96]{0} %constant.152), dimensions={2}, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %get-tuple-element.3628 = bf16[1024,32,4096,1]{3,1,0,2} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3625), index=2, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.3697 = bf16[4096,1024,32,1]{3,2,1,0} transpose(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.3628), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.3698 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,2,1,0} %transpose.3697), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %constant.3703 = bf16[] constant(0), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %pad.3704 = bf16[4096,1024,96]{2,1,0} pad(bf16[4096,1024,32]{2,1,0} %reshape.3698, bf16[] %constant.3703), padding=0_0x0_0x64_0, metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.3699 = bf16[4096,1024,96]{2,1,0} reshape(bf16[4194304,96]{1,0} %dot.73), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %select.3706 = bf16[4096,1024,96]{2,1,0} select(pred[4096,1024,96]{2,1,0} %broadcast.178, bf16[4096,1024,96]{2,1,0} %pad.3704, bf16[4096,1024,96]{2,1,0} %reshape.3699), metadata={op_type="xla__unselect" op_name="xla__unselect" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %slice.3709 = bf16[4096,1024,32]{2,1,0} slice(bf16[4096,1024,96]{2,1,0} %select.3706), slice={[0:4096], [0:1024], [64:96]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.3710 = bf16[4096,1024,32,1]{3,2,1,0} reshape(bf16[4096,1024,32]{2,1,0} %slice.3709), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %transpose.3711 = bf16[1024,32,4096,1]{3,1,0,2} transpose(bf16[4096,1024,32,1]{3,2,1,0} %reshape.3710), dimensions={1,2,0,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=213}
  %reshape.766 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %transpose.3711)
  %broadcast.389 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.766), dimensions={0,2}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.75 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %broadcast.387, bf16[32768,4096,4096]{2,1,0} %broadcast.389), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %convert.291 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.75), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.2832 = f32[1024,32,4096,4096]{3,2,1,0} reshape(f32[32768,4096,4096]{2,1,0} %convert.291), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.181 = pred[1024,32,4096,4096]{3,2,1,0} broadcast(pred[4096,4096]{1,0} %convert.32), dimensions={2,3}, metadata={op_type="aten__expand" op_name="aten__masked_fill.15/aten__expand" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %constant.3670 = bf16[] constant(-9984), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.15/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %broadcast.3671 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %constant.3670), dimensions={}, metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.15/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %get-tuple-element.3626 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3625), index=0, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %reshape.3662 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.3626), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %get-tuple-element.3627 = bf16[1024,32,4096,2]{3,2,1,0} get-tuple-element((bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,2]{3,2,1,0}, bf16[1024,32,4096,1]{3,1,0,2}, bf16[1024,32,4096,1]{3,1,0,2}) %opt-barrier.3625), index=1, metadata={op_type="xla__optimization_barrier" op_name="xla__optimization_barrier" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=1408}
  %transpose.3658 = bf16[1024,32,2,4096]{2,3,1,0} transpose(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.3627), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.3660 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.3658), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %dot.3663 = bf16[32768,4096,4096]{2,1,0} dot(bf16[32768,4096,2]{2,1,0} %reshape.3662, bf16[32768,2,4096]{2,1,0} %reshape.3660), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %broadcast.670 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %divide.17 = bf16[32768,4096,4096]{2,1,0} divide(bf16[32768,4096,4096]{2,1,0} %dot.3663, bf16[32768,4096,4096]{2,1,0} %broadcast.670), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %reshape.1961 = bf16[1024,32,4096,4096]{3,2,1,0} reshape(bf16[32768,4096,4096]{2,1,0} %divide.17), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=200}
  %select.3672 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.181, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.3671, bf16[1024,32,4096,4096]{3,2,1,0} %reshape.1961), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.15/aten__masked_fill" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=209}
  %convert.3673 = f32[1024,32,4096,4096]{3,2,1,0} convert(bf16[1024,32,4096,4096]{3,2,1,0} %select.3672), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/xla_impl/ops.py" source_line=1322}
  %custom-call.22 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %convert.3673), custom_call_target="AwsNeuronSoftmax", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxForwardImpl" op_name="xla___op_SoftmaxForwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %custom-call.23 = f32[1024,32,4096,4096]{3,2,1,0} custom-call(f32[1024,32,4096,4096]{3,2,1,0} %reshape.2832, f32[1024,32,4096,4096]{3,2,1,0} %custom-call.22), custom_call_target="AwsNeuronSoftmaxBackward", api_version=API_VERSION_UNSPECIFIED, metadata={op_type="xla___op_SoftmaxBackwardImpl" op_name="xla___op_SoftmaxBackwardImpl" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}, backend_config="3"
  %convert.3727 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.23), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %select.3733 = bf16[1024,32,4096,4096]{3,2,1,0} select(pred[1024,32,4096,4096]{3,2,1,0} %broadcast.177, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.3732, bf16[1024,32,4096,4096]{3,2,1,0} %convert.3727), metadata={op_type="aten__masked_fill" op_name="aten__masked_fill.16/aten__masked_fill" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %broadcast.3734 = bf16[1024,32,4096,4096]{3,2,1,0} broadcast(bf16[] %p7.101), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %divide.3735 = bf16[1024,32,4096,4096]{3,2,1,0} divide(bf16[1024,32,4096,4096]{3,2,1,0} %select.3733, bf16[1024,32,4096,4096]{3,2,1,0} %broadcast.3734), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3736 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %divide.3735), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3802 = bf16[32768,2,4096]{2,1,0} reshape(bf16[1024,32,2,4096]{2,3,1,0} %transpose.3658), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.3803 = bf16[32768,4096,2]{1,2,0} transpose(bf16[32768,2,4096]{2,1,0} %reshape.3802), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.3804 = bf16[32768,4096,2]{2,1,0} dot(bf16[32768,4096,4096]{2,1,0} %reshape.3736, bf16[32768,4096,2]{1,2,0} %transpose.3803), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3805 = bf16[1024,32,4096,2]{3,2,1,0} reshape(bf16[32768,4096,2]{2,1,0} %dot.3804), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.3825 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.126), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3826 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3825), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3827 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.3805, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3826), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3828 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3834 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.3827, bf16[] %constant.3828), dimensions={3}, to_apply=%AddComputation.3830, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.15 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.3834), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.3807 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.164), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3808 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3807), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3809 = bf16[1024,32,4096,2]{3,2,1,0} multiply(bf16[1024,32,4096,2]{3,2,1,0} %reshape.3805, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3808), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3810 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3816 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{3,2,1,0} %multiply.3809, bf16[] %constant.3810), dimensions={3}, to_apply=%AddComputation.3812, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.19 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.15, bf16[1024,32,4096]{2,1,0} %reduce.3816), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2168 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.19), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.3859 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2168), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.3860 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3859), metadata={op_type="aten__view" op_name="aten__view"}
  %reshape.3737 = bf16[32768,4096,2]{2,1,0} reshape(bf16[1024,32,4096,2]{3,2,1,0} %get-tuple-element.3626), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %transpose.3738 = bf16[32768,2,4096]{1,2,0} transpose(bf16[32768,4096,2]{2,1,0} %reshape.3737), dimensions={0,2,1}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %dot.3739 = bf16[32768,2,4096]{2,1,0} dot(bf16[32768,2,4096]{1,2,0} %transpose.3738, bf16[32768,4096,4096]{2,1,0} %reshape.3736), lhs_batch_dims={0}, lhs_contracting_dims={2}, rhs_batch_dims={0}, rhs_contracting_dims={1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3740 = bf16[1024,32,2,4096]{3,2,1,0} reshape(bf16[32768,2,4096]{2,1,0} %dot.3739), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.3741 = bf16[1024,32,4096,2]{2,3,1,0} transpose(bf16[1024,32,2,4096]{3,2,1,0} %reshape.3740), dimensions={0,1,3,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.3761 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.126), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3762 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3761), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3763 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.3741, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3762), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3764 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3770 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.3763, bf16[] %constant.3764), dimensions={3}, to_apply=%AddComputation.3766, metadata={op_type="aten__sum" op_name="aten__sum"}
  %negate.14 = bf16[1024,32,4096]{2,1,0} negate(bf16[1024,32,4096]{2,1,0} %reduce.3770), metadata={op_type="aten__neg" op_name="aten__neg"}
  %reshape.3743 = bf16[4096,2]{1,0} reshape(bf16[1,4096,2]{2,1,0} %gather.164), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.3744 = bf16[1024,32,4096,2]{3,2,1,0} broadcast(bf16[4096,2]{1,0} %reshape.3743), dimensions={2,3}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3745 = bf16[1024,32,4096,2]{2,3,1,0} multiply(bf16[1024,32,4096,2]{2,3,1,0} %transpose.3741, bf16[1024,32,4096,2]{3,2,1,0} %broadcast.3744), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3746 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3752 = bf16[1024,32,4096]{2,1,0} reduce(bf16[1024,32,4096,2]{2,3,1,0} %multiply.3745, bf16[] %constant.3746), dimensions={3}, to_apply=%AddComputation.3748, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.18 = bf16[1024,32,4096]{2,1,0} add(bf16[1024,32,4096]{2,1,0} %negate.14, bf16[1024,32,4096]{2,1,0} %reduce.3752), metadata={op_type="aten__add" op_name="aten__add"}
  %reshape.2165 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[1024,32,4096]{2,1,0} %add.18), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.3795 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2165), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.3796 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3795), metadata={op_type="aten__view" op_name="aten__view"}
  %convert.3681 = bf16[1024,32,4096,4096]{3,2,1,0} convert(f32[1024,32,4096,4096]{3,2,1,0} %custom-call.22), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=211}
  %reshape.3683 = bf16[32768,4096,4096]{2,1,0} reshape(bf16[1024,32,4096,4096]{3,2,1,0} %convert.3681), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.812 = bf16[32768,4096]{1,0} reshape(bf16[1024,32,4096,1]{3,1,0,2} %get-tuple-element.3629)
  %broadcast.672 = bf16[32768,4096,4096]{2,1,0} broadcast(bf16[32768,4096]{1,0} %reshape.812), dimensions={0,1}, metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %multiply.91 = bf16[32768,4096,4096]{2,1,0} multiply(bf16[32768,4096,4096]{2,1,0} %reshape.3683, bf16[32768,4096,4096]{2,1,0} %broadcast.672)
  %convert.45 = f32[32768,4096,4096]{2,1,0} convert(bf16[32768,4096,4096]{2,1,0} %multiply.91)
  %constant.50 = f32[] constant(0)
  %reduce.7 = f32[32768,4096]{1,0} reduce(f32[32768,4096,4096]{2,1,0} %convert.45, f32[] %constant.50), dimensions={1}, to_apply=%scalar_add_computation.1
  %convert.18 = bf16[32768,4096]{1,0} convert(f32[32768,4096]{1,0} %reduce.7), metadata={op_type="aten__matmul" op_name="aten__matmul" source_file="/home/ubuntu/kahfi/pytorch/torch/autograd/__init__.py" source_line=251}
  %reshape.3686 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.18), metadata={op_type="aten__view" op_name="aten__view"}
  %transpose.3687 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.3686), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute"}
  %reshape.3688 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3687), metadata={op_type="aten__view" op_name="aten__view"}
  %concatenate.3861 = bf16[4096,1024,96]{2,1,0} concatenate(bf16[4096,1024,32]{2,1,0} %reshape.3860, bf16[4096,1024,32]{2,1,0} %reshape.3796, bf16[4096,1024,32]{2,1,0} %reshape.3688), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat"}
  %reshape.3934 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.3861), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %dot.3935 = bf16[4194304,32]{1,0} dot(bf16[4194304,96]{1,0} %reshape.3934, bf16[96,32]{1,0} %p6.70), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=265}
  %reshape.3936 = bf16[4096,1024,32]{2,1,0} reshape(bf16[4194304,32]{1,0} %dot.3935), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %all-gather.1243 = (bf16[4096,1024,32]{2,1,0}, bf16[]) all-gather(bf16[4096,1024,32]{2,1,0} %multiply.47, bf16[] %get-tuple-element.1242), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4551 = bf16[] get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1243), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reduce-scatter.3945 = (bf16[4096,1024,32]{2,1,0}, bf16[]) reduce-scatter(bf16[4096,1024,32]{2,1,0} %reshape.3936, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3941, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %get-tuple-element.3946 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %reduce-scatter.3945), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=819}
  %broadcast.3949 = bf16[4096,1024,32]{2,1,0} broadcast(bf16[32]{0} %p4.45), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3950 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3946, bf16[4096,1024,32]{2,1,0} %broadcast.3949), metadata={op_type="aten__mul" op_name="aten__mul"}
  %convert.3951 = f32[4096,1024,32]{2,1,0} convert(bf16[4096,1024,32]{2,1,0} %multiply.3950), metadata={op_type="xla__cast" op_name="xla__cast"}
  %broadcast.3972 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %rsqrt.0), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3973 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.3951, f32[4096,1024,32]{2,1,0} %broadcast.3972), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3952 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %convert.3951, f32[4096,1024,32]{2,0,1} %convert.17), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.3953 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.3959 = f32[4096,1024]{1,0} reduce(f32[4096,1024,32]{2,1,0} %multiply.3952, f32[] %constant.3953), dimensions={2}, to_apply=%AddComputation.3955, metadata={op_type="aten__sum" op_name="aten__sum"}
  %broadcast.673 = f32[4096,1024]{1,0} broadcast(f32[] %p52.1578), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.87 = f32[4096,1024]{1,0} multiply(f32[4096,1024]{1,0} %reduce.3959, f32[4096,1024]{1,0} %broadcast.673), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.1974 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %multiply.87), metadata={op_type="aten__mul" op_name="aten__mul"}
  %reshape.2111 = f32[4096,1024,1]{2,1,0} reshape(f32[4096,1024]{1,0} %rsqrt.0), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=128}
  %multiply.77 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2111, f32[4096,1024,1]{2,1,0} %reshape.2111), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.78 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.2111, f32[4096,1024,1]{2,1,0} %multiply.77), metadata={op_type="aten__pow" op_name="aten__pow"}
  %multiply.3962 = f32[4096,1024,1]{2,1,0} multiply(f32[4096,1024,1]{2,1,0} %reshape.1974, f32[4096,1024,1]{2,1,0} %multiply.78), metadata={op_type="aten__mul" op_name="aten__mul"}
  %broadcast.675 = f32[4096,1024,1]{2,1,0} broadcast(f32[] %p51.1571), dimensions={}, metadata={op_type="aten__div" op_name="aten__div"}
  %divide.26 = f32[4096,1024,1]{2,1,0} divide(f32[4096,1024,1]{2,1,0} %multiply.3962, f32[4096,1024,1]{2,1,0} %broadcast.675), metadata={op_type="aten__div" op_name="aten__div"}
  %reshape.2008 = f32[4096,1024]{1,0} reshape(f32[4096,1024,1]{2,1,0} %divide.26), metadata={op_type="aten__div" op_name="aten__div"}
  %broadcast.205 = f32[4096,1024,32]{2,1,0} broadcast(f32[4096,1024]{1,0} %reshape.2008), dimensions={0,1}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %broadcast.3916 = f32[4096,1024,32]{2,1,0} broadcast(f32[] %p50.1559), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %multiply.3923 = f32[4096,1024,32]{2,0,1} multiply(f32[4096,1024,32]{2,0,1} %convert.17, f32[4096,1024,32]{2,1,0} %broadcast.3916), metadata={op_type="aten__mul" op_name="aten__mul"}
  %multiply.3969 = f32[4096,1024,32]{2,1,0} multiply(f32[4096,1024,32]{2,1,0} %broadcast.205, f32[4096,1024,32]{2,0,1} %multiply.3923), metadata={op_type="aten__mul" op_name="aten__mul"}
  %add.3974 = f32[4096,1024,32]{2,1,0} add(f32[4096,1024,32]{2,1,0} %multiply.3973, f32[4096,1024,32]{2,1,0} %multiply.3969), metadata={op_type="aten__add" op_name="aten__add"}
  %convert.3975 = bf16[4096,1024,32]{2,1,0} convert(f32[4096,1024,32]{2,1,0} %add.3974), metadata={op_type="xla__cast" op_name="xla__cast"}
  %add.3977 = bf16[4096,1024,32]{2,0,1} add(bf16[4096,1024,32]{2,0,1} %add.3581, bf16[4096,1024,32]{2,1,0} %convert.3975), metadata={op_type="aten__add" op_name="aten__add"}
  %transpose.3978 = bf16[1024,4096,32]{2,1,0} transpose(bf16[4096,1024,32]{2,0,1} %add.3977), dimensions={1,0,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided"}
  %reshape.3979 = bf16[4194304,32]{1,0} reshape(bf16[1024,4096,32]{2,1,0} %transpose.3978), metadata={op_type="aten__view" op_name="aten__view"}
  %constant.3898 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant"}
  %broadcast.3902 = bf16[4194304,32]{1,0} broadcast(bf16[] %constant.3898), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand"}
  %select.3989 = bf16[4194304,32]{1,0} select(pred[4194304,32]{1,0} %broadcast.3988, bf16[4194304,32]{1,0} %reshape.3979, bf16[4194304,32]{1,0} %broadcast.3902), metadata={op_type="aten__where" op_name="aten__where"}
  %scatter.4011 = bf16[32000,32]{1,0} scatter(bf16[32000,32]{1,0} %p69.4014, s64[4194304,1]{1,0} %reshape.3999, bf16[4194304,32]{1,0} %select.3989), update_window_dims={1}, inserted_window_dims={0}, scatter_dims_to_operand_dims={0}, index_vector_dim=1, to_apply=%ScatterCombiner.4007, metadata={op_type="aten__index_put" op_name="aten__index_put"}
  %reduce-scatter.4024 = (bf16[32000,32]{1,0}, bf16[]) reduce-scatter(bf16[32000,32]{1,0} %scatter.4011, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4020, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4025 = bf16[32000,32]{1,0} get-tuple-element((bf16[32000,32]{1,0}, bf16[]) %reduce-scatter.4024), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %constant.168 = bf16[1]{0} constant({1})
  %p79.4427 = bf16[1]{0} parameter(79), frontend_attributes={neff_input_names="input79"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=113}
  %p78.4402 = bf16[32]{0} parameter(78), frontend_attributes={neff_input_names="input78"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4391 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3946, bf16[4096,1024,32]{2,0,1} %convert.44), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4392 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4398 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4391, bf16[] %constant.4392), dimensions={0,1}, to_apply=%AddComputation.4394, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4403 = bf16[32]{0} add(bf16[32]{0} %p78.4402, bf16[32]{0} %reduce.4398), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4412 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4403, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4408, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4413 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4412), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4416 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4413, bf16[32]{0} %get-tuple-element.4413), metadata={op_type="aten__mul" op_name="aten__norm.5/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4417 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4423 = bf16[] reduce(bf16[32]{0} %multiply.4416, bf16[] %constant.4417), dimensions={0}, to_apply=%AddComputation.4419, metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4424 = bf16[] sqrt(bf16[] %reduce.4423), metadata={op_type="aten__sqrt" op_name="aten__norm.5/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4426 = bf16[] multiply(bf16[] %sqrt.4424, bf16[] %sqrt.4424), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.849 = bf16[1]{0} reshape(bf16[] %multiply.4426), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4429 = bf16[1]{0} add(bf16[1]{0} %p79.4427, bf16[1]{0} %reshape.849), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p77.4359 = bf16[32]{0} parameter(77), frontend_attributes={neff_input_names="input77"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4348 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3550, bf16[4096,1024,32]{2,0,1} %convert.277), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4349 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4355 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4348, bf16[] %constant.4349), dimensions={0,1}, to_apply=%AddComputation.4351, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4360 = bf16[32]{0} add(bf16[32]{0} %p77.4359, bf16[32]{0} %reduce.4355), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4369 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4360, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4365, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4370 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4369), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4373 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4370, bf16[32]{0} %get-tuple-element.4370), metadata={op_type="aten__mul" op_name="aten__norm.6/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4374 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4380 = bf16[] reduce(bf16[32]{0} %multiply.4373, bf16[] %constant.4374), dimensions={0}, to_apply=%AddComputation.4376, metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4381 = bf16[] sqrt(bf16[] %reduce.4380), metadata={op_type="aten__sqrt" op_name="aten__norm.6/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4383 = bf16[] multiply(bf16[] %sqrt.4381, bf16[] %sqrt.4381), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.853 = bf16[1]{0} reshape(bf16[] %multiply.4383), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4431 = bf16[1]{0} add(bf16[1]{0} %add.4429, bf16[1]{0} %reshape.853), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p76.4316 = bf16[32]{0} parameter(76), frontend_attributes={neff_input_names="input76"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4305 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.3352, bf16[4096,1024,32]{2,0,1} %convert.346), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4306 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4312 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4305, bf16[] %constant.4306), dimensions={0,1}, to_apply=%AddComputation.4308, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4317 = bf16[32]{0} add(bf16[32]{0} %p76.4316, bf16[32]{0} %reduce.4312), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4326 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4317, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4322, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4327 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4326), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4330 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4327, bf16[32]{0} %get-tuple-element.4327), metadata={op_type="aten__mul" op_name="aten__norm.7/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4331 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4337 = bf16[] reduce(bf16[32]{0} %multiply.4330, bf16[] %constant.4331), dimensions={0}, to_apply=%AddComputation.4333, metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4338 = bf16[] sqrt(bf16[] %reduce.4337), metadata={op_type="aten__sqrt" op_name="aten__norm.7/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4340 = bf16[] multiply(bf16[] %sqrt.4338, bf16[] %sqrt.4338), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.857 = bf16[1]{0} reshape(bf16[] %multiply.4340), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4433 = bf16[1]{0} add(bf16[1]{0} %add.4431, bf16[1]{0} %reshape.857), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p75.4273 = bf16[32]{0} parameter(75), frontend_attributes={neff_input_names="input75"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4262 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2960, bf16[4096,1024,32]{2,0,1} %convert.575), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4263 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4269 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4262, bf16[] %constant.4263), dimensions={0,1}, to_apply=%AddComputation.4265, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4274 = bf16[32]{0} add(bf16[32]{0} %p75.4273, bf16[32]{0} %reduce.4269), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4283 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4274, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4279, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4284 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4283), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4287 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4284, bf16[32]{0} %get-tuple-element.4284), metadata={op_type="aten__mul" op_name="aten__norm.8/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4288 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4294 = bf16[] reduce(bf16[32]{0} %multiply.4287, bf16[] %constant.4288), dimensions={0}, to_apply=%AddComputation.4290, metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4295 = bf16[] sqrt(bf16[] %reduce.4294), metadata={op_type="aten__sqrt" op_name="aten__norm.8/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4297 = bf16[] multiply(bf16[] %sqrt.4295, bf16[] %sqrt.4295), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.861 = bf16[1]{0} reshape(bf16[] %multiply.4297), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4435 = bf16[1]{0} add(bf16[1]{0} %add.4433, bf16[1]{0} %reshape.861), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p74.4230 = bf16[32]{0} parameter(74), frontend_attributes={neff_input_names="input74"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4219 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2762, bf16[4096,1024,32]{2,0,1} %convert.644), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4220 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4226 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4219, bf16[] %constant.4220), dimensions={0,1}, to_apply=%AddComputation.4222, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4231 = bf16[32]{0} add(bf16[32]{0} %p74.4230, bf16[32]{0} %reduce.4226), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4240 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4231, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4236, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4241 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4240), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4244 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4241, bf16[32]{0} %get-tuple-element.4241), metadata={op_type="aten__mul" op_name="aten__norm.9/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4245 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4251 = bf16[] reduce(bf16[32]{0} %multiply.4244, bf16[] %constant.4245), dimensions={0}, to_apply=%AddComputation.4247, metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4252 = bf16[] sqrt(bf16[] %reduce.4251), metadata={op_type="aten__sqrt" op_name="aten__norm.9/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4254 = bf16[] multiply(bf16[] %sqrt.4252, bf16[] %sqrt.4252), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.866 = bf16[1]{0} reshape(bf16[] %multiply.4254), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4437 = bf16[1]{0} add(bf16[1]{0} %add.4435, bf16[1]{0} %reshape.866), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p73.4187 = bf16[32]{0} parameter(73), frontend_attributes={neff_input_names="input73"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4176 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2370, bf16[4096,1024,32]{2,0,1} %convert.873), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4177 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4183 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4176, bf16[] %constant.4177), dimensions={0,1}, to_apply=%AddComputation.4179, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4188 = bf16[32]{0} add(bf16[32]{0} %p73.4187, bf16[32]{0} %reduce.4183), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4197 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4188, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4193, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4198 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4197), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4201 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4198, bf16[32]{0} %get-tuple-element.4198), metadata={op_type="aten__mul" op_name="aten__norm.10/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4202 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4208 = bf16[] reduce(bf16[32]{0} %multiply.4201, bf16[] %constant.4202), dimensions={0}, to_apply=%AddComputation.4204, metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4209 = bf16[] sqrt(bf16[] %reduce.4208), metadata={op_type="aten__sqrt" op_name="aten__norm.10/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4211 = bf16[] multiply(bf16[] %sqrt.4209, bf16[] %sqrt.4209), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.871 = bf16[1]{0} reshape(bf16[] %multiply.4211), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4439 = bf16[1]{0} add(bf16[1]{0} %add.4437, bf16[1]{0} %reshape.871), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p72.4144 = bf16[32]{0} parameter(72), frontend_attributes={neff_input_names="input72"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4133 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.2172, bf16[4096,1024,32]{2,0,1} %convert.942), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4134 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4140 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4133, bf16[] %constant.4134), dimensions={0,1}, to_apply=%AddComputation.4136, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4145 = bf16[32]{0} add(bf16[32]{0} %p72.4144, bf16[32]{0} %reduce.4140), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4154 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4145, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4150, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4155 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4154), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4158 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4155, bf16[32]{0} %get-tuple-element.4155), metadata={op_type="aten__mul" op_name="aten__norm.11/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4159 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4165 = bf16[] reduce(bf16[32]{0} %multiply.4158, bf16[] %constant.4159), dimensions={0}, to_apply=%AddComputation.4161, metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4166 = bf16[] sqrt(bf16[] %reduce.4165), metadata={op_type="aten__sqrt" op_name="aten__norm.11/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4168 = bf16[] multiply(bf16[] %sqrt.4166, bf16[] %sqrt.4166), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.875 = bf16[1]{0} reshape(bf16[] %multiply.4168), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4441 = bf16[1]{0} add(bf16[1]{0} %add.4439, bf16[1]{0} %reshape.875), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p71.4101 = bf16[32]{0} parameter(71), frontend_attributes={neff_input_names="input71"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4090 = bf16[4096,1024,32]{2,1,0} multiply(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1780, bf16[4096,1024,32]{2,0,1} %convert.1171), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4091 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4097 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,1,0} %multiply.4090, bf16[] %constant.4091), dimensions={0,1}, to_apply=%AddComputation.4093, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4102 = bf16[32]{0} add(bf16[32]{0} %p71.4101, bf16[32]{0} %reduce.4097), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4111 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4102, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4107, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4112 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4111), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4115 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4112, bf16[32]{0} %get-tuple-element.4112), metadata={op_type="aten__mul" op_name="aten__norm.12/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4116 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4122 = bf16[] reduce(bf16[32]{0} %multiply.4115, bf16[] %constant.4116), dimensions={0}, to_apply=%AddComputation.4118, metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4123 = bf16[] sqrt(bf16[] %reduce.4122), metadata={op_type="aten__sqrt" op_name="aten__norm.12/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4125 = bf16[] multiply(bf16[] %sqrt.4123, bf16[] %sqrt.4123), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.879 = bf16[1]{0} reshape(bf16[] %multiply.4125), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4443 = bf16[1]{0} add(bf16[1]{0} %add.4441, bf16[1]{0} %reshape.879), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %p70.4058 = bf16[32]{0} parameter(70), frontend_attributes={neff_input_names="input70"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %multiply.4047 = bf16[4096,1024,32]{2,0,1} multiply(bf16[4096,1024,32]{2,0,1} %transpose.1586, bf16[4096,1024,32]{2,0,1} %convert.1327), metadata={op_type="aten__mul" op_name="aten__mul"}
  %constant.4048 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__sum"}
  %reduce.4054 = bf16[32]{0} reduce(bf16[4096,1024,32]{2,0,1} %multiply.4047, bf16[] %constant.4048), dimensions={0,1}, to_apply=%AddComputation.4050, metadata={op_type="aten__sum" op_name="aten__sum"}
  %add.4059 = bf16[32]{0} add(bf16[32]{0} %p70.4058, bf16[32]{0} %reduce.4054), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.4068 = (bf16[32]{0}, bf16[]) reduce-scatter(bf16[32]{0} %add.4059, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.4064, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.4069 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %reduce-scatter.4068), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.4072 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4069, bf16[32]{0} %get-tuple-element.4069), metadata={op_type="aten__mul" op_name="aten__norm.13/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4073 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4079 = bf16[] reduce(bf16[32]{0} %multiply.4072, bf16[] %constant.4073), dimensions={0}, to_apply=%AddComputation.4075, metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4080 = bf16[] sqrt(bf16[] %reduce.4079), metadata={op_type="aten__sqrt" op_name="aten__norm.13/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4082 = bf16[] multiply(bf16[] %sqrt.4080, bf16[] %sqrt.4080), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.883 = bf16[1]{0} reshape(bf16[] %multiply.4082), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %add.4445 = bf16[1]{0} add(bf16[1]{0} %add.4443, bf16[1]{0} %reshape.883), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=128}
  %multiply.4028 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %get-tuple-element.4025, bf16[32000,32]{1,0} %get-tuple-element.4025), metadata={op_type="aten__mul" op_name="aten__norm.14/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.4029 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.4035 = bf16[] reduce(bf16[32000,32]{1,0} %multiply.4028, bf16[] %constant.4029), dimensions={0,1}, to_apply=%AddComputation.4031, metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.4036 = bf16[] sqrt(bf16[] %reduce.4035), metadata={op_type="aten__sqrt" op_name="aten__norm.14/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.4038 = bf16[] multiply(bf16[] %sqrt.4036, bf16[] %sqrt.4036), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.885 = bf16[1]{0} reshape(bf16[] %multiply.4038), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4449 = bf16[1]{0} add(bf16[1]{0} %add.4445, bf16[1]{0} %reshape.885), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p68.3866 = bf16[96,32]{1,0} parameter(68), frontend_attributes={neff_input_names="input68"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.3862 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.3861), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.3863 = bf16[96,4194304]{0,1} transpose(bf16[4194304,96]{1,0} %reshape.3862), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1244 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1243), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.3618 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1244), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.3864 = bf16[96,32]{1,0} dot(bf16[96,4194304]{0,1} %transpose.3863, bf16[4194304,32]{1,0} %reshape.3618), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.3867 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %p68.3866, bf16[96,32]{1,0} %dot.3864), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.3876 = (bf16[96,32]{1,0}, bf16[]) reduce-scatter(bf16[96,32]{1,0} %add.3867, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3872, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.3877 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %reduce-scatter.3876), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.3880 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.3877, bf16[96,32]{1,0} %get-tuple-element.3877), metadata={op_type="aten__mul" op_name="aten__norm.15/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.3881 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.3887 = bf16[] reduce(bf16[96,32]{1,0} %multiply.3880, bf16[] %constant.3881), dimensions={0,1}, to_apply=%AddComputation.3883, metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.3888 = bf16[] sqrt(bf16[] %reduce.3887), metadata={op_type="aten__sqrt" op_name="aten__norm.15/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.3890 = bf16[] multiply(bf16[] %sqrt.3888, bf16[] %sqrt.3888), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.889 = bf16[1]{0} reshape(bf16[] %multiply.3890), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4451 = bf16[1]{0} add(bf16[1]{0} %add.4449, bf16[1]{0} %reshape.889), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p67.3586 = bf16[32,32]{1,0} parameter(67), frontend_attributes={neff_input_names="input67"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.3582 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.3581), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.3583 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.3582), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.3503 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.1), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.3504 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.3503), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.3506 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.3504), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.3584 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.3583, bf16[4194304,32]{1,0} %reshape.3506), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.3587 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p67.3586, bf16[32,32]{1,0} %dot.3584), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.3596 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.3587, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3592, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.3597 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.3596), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.3600 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.3597, bf16[32,32]{1,0} %get-tuple-element.3597), metadata={op_type="aten__mul" op_name="aten__norm.16/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.3601 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.3607 = bf16[] reduce(bf16[32,32]{1,0} %multiply.3600, bf16[] %constant.3601), dimensions={0,1}, to_apply=%AddComputation.3603, metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.3608 = bf16[] sqrt(bf16[] %reduce.3607), metadata={op_type="aten__sqrt" op_name="aten__norm.16/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.3610 = bf16[] multiply(bf16[] %sqrt.3608, bf16[] %sqrt.3608), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.898 = bf16[1]{0} reshape(bf16[] %multiply.3610), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4453 = bf16[1]{0} add(bf16[1]{0} %add.4451, bf16[1]{0} %reshape.898), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p66.3471 = bf16[64,32]{1,0} parameter(66), frontend_attributes={neff_input_names="input66"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.3467 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.3466), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.3468 = bf16[64,4194304]{0,1} transpose(bf16[4194304,64]{1,0} %reshape.3467), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1236 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1235), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.3420 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1236), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.3469 = bf16[64,32]{1,0} dot(bf16[64,4194304]{0,1} %transpose.3468, bf16[4194304,32]{1,0} %reshape.3420), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.3472 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %p66.3471, bf16[64,32]{1,0} %dot.3469), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.3481 = (bf16[64,32]{1,0}, bf16[]) reduce-scatter(bf16[64,32]{1,0} %add.3472, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3477, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.3482 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %reduce-scatter.3481), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.3485 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.3482, bf16[64,32]{1,0} %get-tuple-element.3482), metadata={op_type="aten__mul" op_name="aten__norm.17/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.3486 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.3492 = bf16[] reduce(bf16[64,32]{1,0} %multiply.3485, bf16[] %constant.3486), dimensions={0,1}, to_apply=%AddComputation.3488, metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.3493 = bf16[] sqrt(bf16[] %reduce.3492), metadata={op_type="aten__sqrt" op_name="aten__norm.17/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.3495 = bf16[] multiply(bf16[] %sqrt.3493, bf16[] %sqrt.3493), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.902 = bf16[1]{0} reshape(bf16[] %multiply.3495), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4455 = bf16[1]{0} add(bf16[1]{0} %add.4453, bf16[1]{0} %reshape.902), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p65.3388 = bf16[32,32]{1,0} parameter(65), frontend_attributes={neff_input_names="input65"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.3384 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.3383), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.3385 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.3384), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.3308 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.313), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.3386 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.3385, bf16[4194304,32]{1,0} %reshape.3308), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.3389 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p65.3388, bf16[32,32]{1,0} %dot.3386), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.3398 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.3389, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3394, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.3399 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.3398), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.3402 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.3399, bf16[32,32]{1,0} %get-tuple-element.3399), metadata={op_type="aten__mul" op_name="aten__norm.18/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.3403 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.3409 = bf16[] reduce(bf16[32,32]{1,0} %multiply.3402, bf16[] %constant.3403), dimensions={0,1}, to_apply=%AddComputation.3405, metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.3410 = bf16[] sqrt(bf16[] %reduce.3409), metadata={op_type="aten__sqrt" op_name="aten__norm.18/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.3412 = bf16[] multiply(bf16[] %sqrt.3410, bf16[] %sqrt.3410), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.907 = bf16[1]{0} reshape(bf16[] %multiply.3412), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4457 = bf16[1]{0} add(bf16[1]{0} %add.4455, bf16[1]{0} %reshape.907), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p64.3276 = bf16[96,32]{1,0} parameter(64), frontend_attributes={neff_input_names="input64"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.3272 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.3271), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.3273 = bf16[96,4194304]{0,1} transpose(bf16[4194304,96]{1,0} %reshape.3272), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1228 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1227), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.3028 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1228), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.3274 = bf16[96,32]{1,0} dot(bf16[96,4194304]{0,1} %transpose.3273, bf16[4194304,32]{1,0} %reshape.3028), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.3277 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %p64.3276, bf16[96,32]{1,0} %dot.3274), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.3286 = (bf16[96,32]{1,0}, bf16[]) reduce-scatter(bf16[96,32]{1,0} %add.3277, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3282, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.3287 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %reduce-scatter.3286), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.3290 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.3287, bf16[96,32]{1,0} %get-tuple-element.3287), metadata={op_type="aten__mul" op_name="aten__norm.19/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.3291 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.3297 = bf16[] reduce(bf16[96,32]{1,0} %multiply.3290, bf16[] %constant.3291), dimensions={0,1}, to_apply=%AddComputation.3293, metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.3298 = bf16[] sqrt(bf16[] %reduce.3297), metadata={op_type="aten__sqrt" op_name="aten__norm.19/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.3300 = bf16[] multiply(bf16[] %sqrt.3298, bf16[] %sqrt.3298), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.911 = bf16[1]{0} reshape(bf16[] %multiply.3300), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4459 = bf16[1]{0} add(bf16[1]{0} %add.4457, bf16[1]{0} %reshape.911), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p63.2996 = bf16[32,32]{1,0} parameter(63), frontend_attributes={neff_input_names="input63"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2992 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2991), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2993 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.2992), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.2913 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.3), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2914 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2913), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.2916 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2914), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2994 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.2993, bf16[4194304,32]{1,0} %reshape.2916), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2997 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p63.2996, bf16[32,32]{1,0} %dot.2994), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.3006 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.2997, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.3002, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.3007 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.3006), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.3010 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.3007, bf16[32,32]{1,0} %get-tuple-element.3007), metadata={op_type="aten__mul" op_name="aten__norm.20/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.3011 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.3017 = bf16[] reduce(bf16[32,32]{1,0} %multiply.3010, bf16[] %constant.3011), dimensions={0,1}, to_apply=%AddComputation.3013, metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.3018 = bf16[] sqrt(bf16[] %reduce.3017), metadata={op_type="aten__sqrt" op_name="aten__norm.20/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.3020 = bf16[] multiply(bf16[] %sqrt.3018, bf16[] %sqrt.3018), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.918 = bf16[1]{0} reshape(bf16[] %multiply.3020), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4461 = bf16[1]{0} add(bf16[1]{0} %add.4459, bf16[1]{0} %reshape.918), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p62.2881 = bf16[64,32]{1,0} parameter(62), frontend_attributes={neff_input_names="input62"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2877 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.2876), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2878 = bf16[64,4194304]{0,1} transpose(bf16[4194304,64]{1,0} %reshape.2877), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1220 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1219), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.2830 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1220), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2879 = bf16[64,32]{1,0} dot(bf16[64,4194304]{0,1} %transpose.2878, bf16[4194304,32]{1,0} %reshape.2830), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2882 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %p62.2881, bf16[64,32]{1,0} %dot.2879), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2891 = (bf16[64,32]{1,0}, bf16[]) reduce-scatter(bf16[64,32]{1,0} %add.2882, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2887, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2892 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %reduce-scatter.2891), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2895 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.2892, bf16[64,32]{1,0} %get-tuple-element.2892), metadata={op_type="aten__mul" op_name="aten__norm.21/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2896 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2902 = bf16[] reduce(bf16[64,32]{1,0} %multiply.2895, bf16[] %constant.2896), dimensions={0,1}, to_apply=%AddComputation.2898, metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2903 = bf16[] sqrt(bf16[] %reduce.2902), metadata={op_type="aten__sqrt" op_name="aten__norm.21/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2905 = bf16[] multiply(bf16[] %sqrt.2903, bf16[] %sqrt.2903), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.921 = bf16[1]{0} reshape(bf16[] %multiply.2905), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4463 = bf16[1]{0} add(bf16[1]{0} %add.4461, bf16[1]{0} %reshape.921), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p61.2798 = bf16[32,32]{1,0} parameter(61), frontend_attributes={neff_input_names="input61"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2794 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2793), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2795 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.2794), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.2718 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.611), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2796 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.2795, bf16[4194304,32]{1,0} %reshape.2718), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2799 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p61.2798, bf16[32,32]{1,0} %dot.2796), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2808 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.2799, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2804, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2809 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.2808), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2812 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.2809, bf16[32,32]{1,0} %get-tuple-element.2809), metadata={op_type="aten__mul" op_name="aten__norm.22/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2813 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2819 = bf16[] reduce(bf16[32,32]{1,0} %multiply.2812, bf16[] %constant.2813), dimensions={0,1}, to_apply=%AddComputation.2815, metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2820 = bf16[] sqrt(bf16[] %reduce.2819), metadata={op_type="aten__sqrt" op_name="aten__norm.22/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2822 = bf16[] multiply(bf16[] %sqrt.2820, bf16[] %sqrt.2820), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.924 = bf16[1]{0} reshape(bf16[] %multiply.2822), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4465 = bf16[1]{0} add(bf16[1]{0} %add.4463, bf16[1]{0} %reshape.924), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p60.2686 = bf16[96,32]{1,0} parameter(60), frontend_attributes={neff_input_names="input60"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2682 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.2681), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2683 = bf16[96,4194304]{0,1} transpose(bf16[4194304,96]{1,0} %reshape.2682), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1212 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1211), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.2438 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1212), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2684 = bf16[96,32]{1,0} dot(bf16[96,4194304]{0,1} %transpose.2683, bf16[4194304,32]{1,0} %reshape.2438), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2687 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %p60.2686, bf16[96,32]{1,0} %dot.2684), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2696 = (bf16[96,32]{1,0}, bf16[]) reduce-scatter(bf16[96,32]{1,0} %add.2687, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2692, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2697 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %reduce-scatter.2696), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2700 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.2697, bf16[96,32]{1,0} %get-tuple-element.2697), metadata={op_type="aten__mul" op_name="aten__norm.23/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2701 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2707 = bf16[] reduce(bf16[96,32]{1,0} %multiply.2700, bf16[] %constant.2701), dimensions={0,1}, to_apply=%AddComputation.2703, metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2708 = bf16[] sqrt(bf16[] %reduce.2707), metadata={op_type="aten__sqrt" op_name="aten__norm.23/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2710 = bf16[] multiply(bf16[] %sqrt.2708, bf16[] %sqrt.2708), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.927 = bf16[1]{0} reshape(bf16[] %multiply.2710), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4467 = bf16[1]{0} add(bf16[1]{0} %add.4465, bf16[1]{0} %reshape.927), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p59.2406 = bf16[32,32]{1,0} parameter(59), frontend_attributes={neff_input_names="input59"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2402 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2401), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2403 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.2402), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.2323 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.5), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2324 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.2323), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.2326 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.2324), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2404 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.2403, bf16[4194304,32]{1,0} %reshape.2326), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2407 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p59.2406, bf16[32,32]{1,0} %dot.2404), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2416 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.2407, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2412, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2417 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.2416), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2420 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.2417, bf16[32,32]{1,0} %get-tuple-element.2417), metadata={op_type="aten__mul" op_name="aten__norm.24/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2421 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2427 = bf16[] reduce(bf16[32,32]{1,0} %multiply.2420, bf16[] %constant.2421), dimensions={0,1}, to_apply=%AddComputation.2423, metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2428 = bf16[] sqrt(bf16[] %reduce.2427), metadata={op_type="aten__sqrt" op_name="aten__norm.24/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2430 = bf16[] multiply(bf16[] %sqrt.2428, bf16[] %sqrt.2428), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.934 = bf16[1]{0} reshape(bf16[] %multiply.2430), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4469 = bf16[1]{0} add(bf16[1]{0} %add.4467, bf16[1]{0} %reshape.934), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p58.2291 = bf16[64,32]{1,0} parameter(58), frontend_attributes={neff_input_names="input58"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2287 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.2286), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2288 = bf16[64,4194304]{0,1} transpose(bf16[4194304,64]{1,0} %reshape.2287), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1204 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1203), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.2240 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1204), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2289 = bf16[64,32]{1,0} dot(bf16[64,4194304]{0,1} %transpose.2288, bf16[4194304,32]{1,0} %reshape.2240), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2292 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %p58.2291, bf16[64,32]{1,0} %dot.2289), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2301 = (bf16[64,32]{1,0}, bf16[]) reduce-scatter(bf16[64,32]{1,0} %add.2292, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2297, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2302 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %reduce-scatter.2301), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2305 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.2302, bf16[64,32]{1,0} %get-tuple-element.2302), metadata={op_type="aten__mul" op_name="aten__norm.25/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2306 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2312 = bf16[] reduce(bf16[64,32]{1,0} %multiply.2305, bf16[] %constant.2306), dimensions={0,1}, to_apply=%AddComputation.2308, metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2313 = bf16[] sqrt(bf16[] %reduce.2312), metadata={op_type="aten__sqrt" op_name="aten__norm.25/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2315 = bf16[] multiply(bf16[] %sqrt.2313, bf16[] %sqrt.2313), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.937 = bf16[1]{0} reshape(bf16[] %multiply.2315), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4471 = bf16[1]{0} add(bf16[1]{0} %add.4469, bf16[1]{0} %reshape.937), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p57.2208 = bf16[32,32]{1,0} parameter(57), frontend_attributes={neff_input_names="input57"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2204 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.2203), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2205 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.2204), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.2128 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.909), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2206 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.2205, bf16[4194304,32]{1,0} %reshape.2128), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2209 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p57.2208, bf16[32,32]{1,0} %dot.2206), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2218 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.2209, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2214, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2219 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.2218), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2222 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.2219, bf16[32,32]{1,0} %get-tuple-element.2219), metadata={op_type="aten__mul" op_name="aten__norm.26/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2223 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2229 = bf16[] reduce(bf16[32,32]{1,0} %multiply.2222, bf16[] %constant.2223), dimensions={0,1}, to_apply=%AddComputation.2225, metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2230 = bf16[] sqrt(bf16[] %reduce.2229), metadata={op_type="aten__sqrt" op_name="aten__norm.26/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2232 = bf16[] multiply(bf16[] %sqrt.2230, bf16[] %sqrt.2230), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.941 = bf16[1]{0} reshape(bf16[] %multiply.2232), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4473 = bf16[1]{0} add(bf16[1]{0} %add.4471, bf16[1]{0} %reshape.941), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p56.2096 = bf16[96,32]{1,0} parameter(56), frontend_attributes={neff_input_names="input56"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.2092 = bf16[4194304,96]{1,0} reshape(bf16[4096,1024,96]{2,1,0} %concatenate.2091), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.2093 = bf16[96,4194304]{0,1} transpose(bf16[4194304,96]{1,0} %reshape.2092), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1196 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1195), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.1848 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1196), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.2094 = bf16[96,32]{1,0} dot(bf16[96,4194304]{0,1} %transpose.2093, bf16[4194304,32]{1,0} %reshape.1848), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.2097 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %p56.2096, bf16[96,32]{1,0} %dot.2094), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.2106 = (bf16[96,32]{1,0}, bf16[]) reduce-scatter(bf16[96,32]{1,0} %add.2097, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.2102, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.2107 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %reduce-scatter.2106), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.2110 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.2107, bf16[96,32]{1,0} %get-tuple-element.2107), metadata={op_type="aten__mul" op_name="aten__norm.27/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.2111 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.2117 = bf16[] reduce(bf16[96,32]{1,0} %multiply.2110, bf16[] %constant.2111), dimensions={0,1}, to_apply=%AddComputation.2113, metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.2118 = bf16[] sqrt(bf16[] %reduce.2117), metadata={op_type="aten__sqrt" op_name="aten__norm.27/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.2120 = bf16[] multiply(bf16[] %sqrt.2118, bf16[] %sqrt.2118), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.944 = bf16[1]{0} reshape(bf16[] %multiply.2120), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4475 = bf16[1]{0} add(bf16[1]{0} %add.4473, bf16[1]{0} %reshape.944), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p55.1816 = bf16[32,32]{1,0} parameter(55), frontend_attributes={neff_input_names="input55"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.1812 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %add.1811), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.1813 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.1812), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.1733 = bf16[1024,32,4096,1]{3,2,1,0} reshape(bf16[32768,4096]{1,0} %convert.7), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.1734 = bf16[4096,1024,32,1]{3,0,2,1} transpose(bf16[1024,32,4096,1]{3,2,1,0} %reshape.1733), dimensions={2,0,1,3}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.1736 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32,1]{3,0,2,1} %transpose.1734), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.1814 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.1813, bf16[4194304,32]{1,0} %reshape.1736), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.1817 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p55.1816, bf16[32,32]{1,0} %dot.1814), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.1826 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.1817, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.1822, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.1827 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.1826), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.1830 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.1827, bf16[32,32]{1,0} %get-tuple-element.1827), metadata={op_type="aten__mul" op_name="aten__norm.28/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.1831 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.1837 = bf16[] reduce(bf16[32,32]{1,0} %multiply.1830, bf16[] %constant.1831), dimensions={0,1}, to_apply=%AddComputation.1833, metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.1838 = bf16[] sqrt(bf16[] %reduce.1837), metadata={op_type="aten__sqrt" op_name="aten__norm.28/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.1840 = bf16[] multiply(bf16[] %sqrt.1838, bf16[] %sqrt.1838), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.950 = bf16[1]{0} reshape(bf16[] %multiply.1840), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4477 = bf16[1]{0} add(bf16[1]{0} %add.4475, bf16[1]{0} %reshape.950), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p54.1701 = bf16[64,32]{1,0} parameter(54), frontend_attributes={neff_input_names="input54"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.1697 = bf16[4194304,64]{1,0} reshape(bf16[4096,1024,64]{2,1,0} %concatenate.1696), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.1698 = bf16[64,4194304]{0,1} transpose(bf16[4194304,64]{1,0} %reshape.1697), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %get-tuple-element.1188 = bf16[4096,1024,32]{2,1,0} get-tuple-element((bf16[4096,1024,32]{2,1,0}, bf16[]) %all-gather.1187), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %reshape.1650 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %get-tuple-element.1188), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.1699 = bf16[64,32]{1,0} dot(bf16[64,4194304]{0,1} %transpose.1698, bf16[4194304,32]{1,0} %reshape.1650), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.1702 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %p54.1701, bf16[64,32]{1,0} %dot.1699), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.1711 = (bf16[64,32]{1,0}, bf16[]) reduce-scatter(bf16[64,32]{1,0} %add.1702, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.1707, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.1712 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %reduce-scatter.1711), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.1715 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.1712, bf16[64,32]{1,0} %get-tuple-element.1712), metadata={op_type="aten__mul" op_name="aten__norm.29/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.1716 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.1722 = bf16[] reduce(bf16[64,32]{1,0} %multiply.1715, bf16[] %constant.1716), dimensions={0,1}, to_apply=%AddComputation.1718, metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.1723 = bf16[] sqrt(bf16[] %reduce.1722), metadata={op_type="aten__sqrt" op_name="aten__norm.29/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.1725 = bf16[] multiply(bf16[] %sqrt.1723, bf16[] %sqrt.1723), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.953 = bf16[1]{0} reshape(bf16[] %multiply.1725), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4479 = bf16[1]{0} add(bf16[1]{0} %add.4477, bf16[1]{0} %reshape.953), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p53.1618 = bf16[32,32]{1,0} parameter(53), frontend_attributes={neff_input_names="input53"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.1614 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,0,1} %convert.1613), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.1615 = bf16[32,4194304]{0,1} transpose(bf16[4194304,32]{1,0} %reshape.1614), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.1553 = bf16[4194304,32]{1,0} reshape(bf16[4096,1024,32]{2,1,0} %multiply.1294), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.1616 = bf16[32,32]{1,0} dot(bf16[32,4194304]{0,1} %transpose.1615, bf16[4194304,32]{1,0} %reshape.1553), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.1619 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %p53.1618, bf16[32,32]{1,0} %dot.1616), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.1628 = (bf16[32,32]{1,0}, bf16[]) reduce-scatter(bf16[32,32]{1,0} %add.1619, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.1624, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.1629 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %reduce-scatter.1628), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.1632 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.1629, bf16[32,32]{1,0} %get-tuple-element.1629), metadata={op_type="aten__mul" op_name="aten__norm.30/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.1633 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.1639 = bf16[] reduce(bf16[32,32]{1,0} %multiply.1632, bf16[] %constant.1633), dimensions={0,1}, to_apply=%AddComputation.1635, metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.1640 = bf16[] sqrt(bf16[] %reduce.1639), metadata={op_type="aten__sqrt" op_name="aten__norm.30/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.1642 = bf16[] multiply(bf16[] %sqrt.1640, bf16[] %sqrt.1640), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.956 = bf16[1]{0} reshape(bf16[] %multiply.1642), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4481 = bf16[1]{0} add(bf16[1]{0} %add.4479, bf16[1]{0} %reshape.956), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %p49.1521 = bf16[32000,32]{1,0} parameter(49), frontend_attributes={neff_input_names="input49"}, metadata={op_type="xla__device_data" op_name="xla__device_data"}
  %reshape.1517 = bf16[4194304,32000]{1,0} reshape(bf16[1024,4096,32000]{2,1,0} %convert.1516), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.1518 = bf16[32000,4194304]{0,1} transpose(bf16[4194304,32000]{1,0} %reshape.1517), dimensions={1,0}, metadata={op_type="aten__permute" op_name="aten__permute" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %transpose.1331 = bf16[1024,4096,32]{2,0,1} transpose(bf16[4096,1024,32]{2,1,0} %multiply.1330), dimensions={1,0,2}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %reshape.1332 = bf16[4194304,32]{1,0} reshape(bf16[1024,4096,32]{2,0,1} %transpose.1331), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %dot.1519 = bf16[32000,32]{1,0} dot(bf16[32000,4194304]{0,1} %transpose.1518, bf16[4194304,32]{1,0} %reshape.1332), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/layers.py" source_line=326}
  %add.1522 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %p49.1521, bf16[32000,32]{1,0} %dot.1519), metadata={op_type="aten__add" op_name="aten__add"}
  %reduce-scatter.1531 = (bf16[32000,32]{1,0}, bf16[]) reduce-scatter(bf16[32000,32]{1,0} %add.1522, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, to_apply=%AddComputation.1527, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %get-tuple-element.1532 = bf16[32000,32]{1,0} get-tuple-element((bf16[32000,32]{1,0}, bf16[]) %reduce-scatter.1531), index=0, metadata={op_type="xla__reduce_scatter" op_name="xla__reduce_scatter" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=825}
  %multiply.1535 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %get-tuple-element.1532, bf16[32000,32]{1,0} %get-tuple-element.1532), metadata={op_type="aten__mul" op_name="aten__norm.31/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.1536 = bf16[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.1542 = bf16[] reduce(bf16[32000,32]{1,0} %multiply.1535, bf16[] %constant.1536), dimensions={0,1}, to_apply=%AddComputation.1538, metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.1543 = bf16[] sqrt(bf16[] %reduce.1542), metadata={op_type="aten__sqrt" op_name="aten__norm.31/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %multiply.1545 = bf16[] multiply(bf16[] %sqrt.1543, bf16[] %sqrt.1543), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=40}
  %reshape.961 = bf16[1]{0} reshape(bf16[] %multiply.1545), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %add.4483 = bf16[1]{0} add(bf16[1]{0} %add.4481, bf16[1]{0} %reshape.961), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=133}
  %constant.170 = bf16[1]{0} constant({0.5})
  %power.4484 = bf16[1]{0} power(bf16[1]{0} %add.4483, bf16[1]{0} %constant.170), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=136}
  %p40.1258 = bf16[] parameter(40), frontend_attributes={neff_input_names="input40"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=187}
  %reshape.965 = bf16[1]{0} reshape(bf16[] %p40.1258), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=187}
  %add.4486 = bf16[1]{0} add(bf16[1]{0} %power.4484, bf16[1]{0} %reshape.965), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=187}
  %divide.4489 = bf16[1]{0} divide(bf16[1]{0} %constant.168, bf16[1]{0} %add.4486), metadata={op_type="aten__reciprocal" op_name="aten__reciprocal" source_file="/home/ubuntu/kahfi/pytorch/torch/_tensor.py" source_line=913}
  %constant.173 = bf16[1]{0} constant({1})
  %compare.4496 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.173), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.174 = bf16[1]{0} constant({1})
  %select.4498 = bf16[1]{0} select(pred[1]{0} %compare.4496, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.174), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4500 = bf16[] reshape(bf16[1]{0} %select.4498), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4502 = bf16[32000,32]{1,0} broadcast(bf16[] %reshape.4500), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4503 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %get-tuple-element.4025, bf16[32000,32]{1,0} %broadcast.4502), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %p82.4518 = bf16[] parameter(82), frontend_attributes={neff_input_names="input82"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4522 = bf16[32000,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4523 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.4503, bf16[32000,32]{1,0} %broadcast.4522), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4528 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %multiply.4527, bf16[32000,32]{1,0} %multiply.4523), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p81.4505 = bf16[32000,32]{1,0} parameter(81), frontend_attributes={neff_input_names="input81"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %p80.4504 = bf16[] parameter(80), frontend_attributes={neff_input_names="input80"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4506 = bf16[32000,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4507 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %p81.4505, bf16[32000,32]{1,0} %broadcast.4506), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4509 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.4503, bf16[32000,32]{1,0} %multiply.4503), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %p39.1252 = f32[] parameter(39), frontend_attributes={neff_input_names="input39"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4508 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4510 = bf16[32000,32]{1,0} broadcast(bf16[] %convert.4508), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4511 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.4509, bf16[32000,32]{1,0} %broadcast.4510), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4512 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %multiply.4507, bf16[32000,32]{1,0} %multiply.4511), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4513 = bf16[32000,32]{1,0} sqrt(bf16[32000,32]{1,0} %add.4512), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %p38.1251 = bf16[] parameter(38), frontend_attributes={neff_input_names="input38"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4514 = bf16[32000,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4515 = bf16[32000,32]{1,0} divide(bf16[32000,32]{1,0} %sqrt.4513, bf16[32000,32]{1,0} %broadcast.4514), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %p37.1249 = bf16[] parameter(37), frontend_attributes={neff_input_names="input37"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4516 = bf16[32000,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4517 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %divide.4515, bf16[32000,32]{1,0} %broadcast.4516), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4544 = bf16[32000,32]{1,0} divide(bf16[32000,32]{1,0} %add.4528, bf16[32000,32]{1,0} %add.4517), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %p36.1247 = f32[] parameter(36), frontend_attributes={neff_input_names="input36"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4543 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4545 = bf16[32000,32]{1,0} broadcast(bf16[] %convert.4543), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4546 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %divide.4544, bf16[32000,32]{1,0} %broadcast.4545), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4547 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %subtract.4542, bf16[32000,32]{1,0} %multiply.4546), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %all-gather.4552 = (bf16[32000,32]{1,0}, bf16[]) all-gather(bf16[32000,32]{1,0} %add.4547, bf16[] %get-tuple-element.4551), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4553 = bf16[32000,32]{1,0} get-tuple-element((bf16[32000,32]{1,0}, bf16[]) %all-gather.4552), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p90.4601 = bf16[96,32]{1,0} parameter(90), frontend_attributes={neff_input_names="input90"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4602 = bf16[96,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4603 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p90.4601, bf16[96,32]{1,0} %broadcast.4602), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4604 = bf16[96,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4606 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4603, bf16[96,32]{1,0} %broadcast.4604), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4607 = bf16[96,32]{1,0} subtract(bf16[96,32]{1,0} %p90.4601, bf16[96,32]{1,0} %multiply.4606), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p89.4592 = bf16[96,32]{1,0} parameter(89), frontend_attributes={neff_input_names="input89"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4593 = bf16[96,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4594 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p89.4592, bf16[96,32]{1,0} %broadcast.4593), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.176 = bf16[1]{0} constant({1})
  %compare.4566 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.176), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.177 = bf16[1]{0} constant({1})
  %select.4568 = bf16[1]{0} select(pred[1]{0} %compare.4566, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.177), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4570 = bf16[] reshape(bf16[1]{0} %select.4568), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4572 = bf16[96,32]{1,0} broadcast(bf16[] %reshape.4570), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4573 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.3877, bf16[96,32]{1,0} %broadcast.4572), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4590 = bf16[96,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4591 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4573, bf16[96,32]{1,0} %broadcast.4590), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4595 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.4594, bf16[96,32]{1,0} %multiply.4591), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p88.4574 = bf16[96,32]{1,0} parameter(88), frontend_attributes={neff_input_names="input88"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4575 = bf16[96,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4576 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p88.4574, bf16[96,32]{1,0} %broadcast.4575), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4578 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4573, bf16[96,32]{1,0} %multiply.4573), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4577 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4579 = bf16[96,32]{1,0} broadcast(bf16[] %convert.4577), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4580 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4578, bf16[96,32]{1,0} %broadcast.4579), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4581 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.4576, bf16[96,32]{1,0} %multiply.4580), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4582 = bf16[96,32]{1,0} sqrt(bf16[96,32]{1,0} %add.4581), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4583 = bf16[96,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4584 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %sqrt.4582, bf16[96,32]{1,0} %broadcast.4583), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4585 = bf16[96,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4586 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %divide.4584, bf16[96,32]{1,0} %broadcast.4585), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4609 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %add.4595, bf16[96,32]{1,0} %add.4586), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4608 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4610 = bf16[96,32]{1,0} broadcast(bf16[] %convert.4608), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4611 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %divide.4609, bf16[96,32]{1,0} %broadcast.4610), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4612 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %subtract.4607, bf16[96,32]{1,0} %multiply.4611), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.4616 = bf16[] get-tuple-element((bf16[32000,32]{1,0}, bf16[]) %all-gather.4552), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.4617 = (bf16[96,32]{1,0}, bf16[]) all-gather(bf16[96,32]{1,0} %add.4612, bf16[] %get-tuple-element.4616), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4618 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.4617), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p93.4666 = bf16[32,32]{1,0} parameter(93), frontend_attributes={neff_input_names="input93"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4667 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4668 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p93.4666, bf16[32,32]{1,0} %broadcast.4667), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4669 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4671 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4668, bf16[32,32]{1,0} %broadcast.4669), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4672 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p93.4666, bf16[32,32]{1,0} %multiply.4671), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p92.4657 = bf16[32,32]{1,0} parameter(92), frontend_attributes={neff_input_names="input92"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4658 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4659 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p92.4657, bf16[32,32]{1,0} %broadcast.4658), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.179 = bf16[1]{0} constant({1})
  %compare.4631 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.179), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.180 = bf16[1]{0} constant({1})
  %select.4633 = bf16[1]{0} select(pred[1]{0} %compare.4631, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.180), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4635 = bf16[] reshape(bf16[1]{0} %select.4633), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4637 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.4635), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4638 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.3597, bf16[32,32]{1,0} %broadcast.4637), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4655 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4656 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4638, bf16[32,32]{1,0} %broadcast.4655), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4660 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.4659, bf16[32,32]{1,0} %multiply.4656), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p91.4639 = bf16[32,32]{1,0} parameter(91), frontend_attributes={neff_input_names="input91"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4640 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4641 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p91.4639, bf16[32,32]{1,0} %broadcast.4640), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4643 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4638, bf16[32,32]{1,0} %multiply.4638), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4642 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4644 = bf16[32,32]{1,0} broadcast(bf16[] %convert.4642), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4645 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4643, bf16[32,32]{1,0} %broadcast.4644), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4646 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.4641, bf16[32,32]{1,0} %multiply.4645), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4647 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.4646), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4648 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4649 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.4647, bf16[32,32]{1,0} %broadcast.4648), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4650 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4651 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.4649, bf16[32,32]{1,0} %broadcast.4650), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4674 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.4660, bf16[32,32]{1,0} %add.4651), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4673 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4675 = bf16[32,32]{1,0} broadcast(bf16[] %convert.4673), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4676 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.4674, bf16[32,32]{1,0} %broadcast.4675), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4677 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.4672, bf16[32,32]{1,0} %multiply.4676), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.4681 = bf16[] get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.4617), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.4682 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.4677, bf16[] %get-tuple-element.4681), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4683 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.4682), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p96.4731 = bf16[64,32]{1,0} parameter(96), frontend_attributes={neff_input_names="input96"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4732 = bf16[64,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4733 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p96.4731, bf16[64,32]{1,0} %broadcast.4732), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4734 = bf16[64,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4736 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.4733, bf16[64,32]{1,0} %broadcast.4734), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4737 = bf16[64,32]{1,0} subtract(bf16[64,32]{1,0} %p96.4731, bf16[64,32]{1,0} %multiply.4736), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p95.4722 = bf16[64,32]{1,0} parameter(95), frontend_attributes={neff_input_names="input95"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4723 = bf16[64,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4724 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p95.4722, bf16[64,32]{1,0} %broadcast.4723), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.183 = bf16[1]{0} constant({1})
  %compare.4696 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.183), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.184 = bf16[1]{0} constant({1})
  %select.4698 = bf16[1]{0} select(pred[1]{0} %compare.4696, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.184), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4700 = bf16[] reshape(bf16[1]{0} %select.4698), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4702 = bf16[64,32]{1,0} broadcast(bf16[] %reshape.4700), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4703 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.3482, bf16[64,32]{1,0} %broadcast.4702), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4720 = bf16[64,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4721 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.4703, bf16[64,32]{1,0} %broadcast.4720), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4725 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.4724, bf16[64,32]{1,0} %multiply.4721), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p94.4704 = bf16[64,32]{1,0} parameter(94), frontend_attributes={neff_input_names="input94"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4705 = bf16[64,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4706 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p94.4704, bf16[64,32]{1,0} %broadcast.4705), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4708 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.4703, bf16[64,32]{1,0} %multiply.4703), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4707 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4709 = bf16[64,32]{1,0} broadcast(bf16[] %convert.4707), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4710 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.4708, bf16[64,32]{1,0} %broadcast.4709), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4711 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.4706, bf16[64,32]{1,0} %multiply.4710), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4712 = bf16[64,32]{1,0} sqrt(bf16[64,32]{1,0} %add.4711), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4713 = bf16[64,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4714 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %sqrt.4712, bf16[64,32]{1,0} %broadcast.4713), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4715 = bf16[64,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4716 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %divide.4714, bf16[64,32]{1,0} %broadcast.4715), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4739 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %add.4725, bf16[64,32]{1,0} %add.4716), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4738 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4740 = bf16[64,32]{1,0} broadcast(bf16[] %convert.4738), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4741 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %divide.4739, bf16[64,32]{1,0} %broadcast.4740), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4742 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %subtract.4737, bf16[64,32]{1,0} %multiply.4741), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.4746 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.4682), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.4747 = (bf16[64,32]{1,0}, bf16[]) all-gather(bf16[64,32]{1,0} %add.4742, bf16[] %get-tuple-element.4746), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4748 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.4747), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p99.4796 = bf16[32,32]{1,0} parameter(99), frontend_attributes={neff_input_names="input99"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4797 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4798 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p99.4796, bf16[32,32]{1,0} %broadcast.4797), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4799 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4801 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4798, bf16[32,32]{1,0} %broadcast.4799), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4802 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p99.4796, bf16[32,32]{1,0} %multiply.4801), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p98.4787 = bf16[32,32]{1,0} parameter(98), frontend_attributes={neff_input_names="input98"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4788 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4789 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p98.4787, bf16[32,32]{1,0} %broadcast.4788), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.186 = bf16[1]{0} constant({1})
  %compare.4761 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.186), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.187 = bf16[1]{0} constant({1})
  %select.4763 = bf16[1]{0} select(pred[1]{0} %compare.4761, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.187), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4765 = bf16[] reshape(bf16[1]{0} %select.4763), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4767 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.4765), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4768 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.3399, bf16[32,32]{1,0} %broadcast.4767), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4785 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4786 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4768, bf16[32,32]{1,0} %broadcast.4785), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4790 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.4789, bf16[32,32]{1,0} %multiply.4786), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p97.4769 = bf16[32,32]{1,0} parameter(97), frontend_attributes={neff_input_names="input97"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4770 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4771 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p97.4769, bf16[32,32]{1,0} %broadcast.4770), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4773 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4768, bf16[32,32]{1,0} %multiply.4768), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4772 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4774 = bf16[32,32]{1,0} broadcast(bf16[] %convert.4772), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4775 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.4773, bf16[32,32]{1,0} %broadcast.4774), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4776 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.4771, bf16[32,32]{1,0} %multiply.4775), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4777 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.4776), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4778 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4779 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.4777, bf16[32,32]{1,0} %broadcast.4778), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4780 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4781 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.4779, bf16[32,32]{1,0} %broadcast.4780), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4804 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.4790, bf16[32,32]{1,0} %add.4781), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4803 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4805 = bf16[32,32]{1,0} broadcast(bf16[] %convert.4803), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4806 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.4804, bf16[32,32]{1,0} %broadcast.4805), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4807 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.4802, bf16[32,32]{1,0} %multiply.4806), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.4811 = bf16[] get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.4747), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.4812 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.4807, bf16[] %get-tuple-element.4811), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4813 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.4812), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p102.4860 = bf16[32]{0} parameter(102), frontend_attributes={neff_input_names="input102"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4861 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4862 = bf16[32]{0} multiply(bf16[32]{0} %p102.4860, bf16[32]{0} %broadcast.4861), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4863 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4865 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4862, bf16[32]{0} %broadcast.4863), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4866 = bf16[32]{0} subtract(bf16[32]{0} %p102.4860, bf16[32]{0} %multiply.4865), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p101.4851 = bf16[32]{0} parameter(101), frontend_attributes={neff_input_names="input101"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4852 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4853 = bf16[32]{0} multiply(bf16[32]{0} %p101.4851, bf16[32]{0} %broadcast.4852), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.189 = bf16[1]{0} constant({1})
  %compare.4826 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.189), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.190 = bf16[1]{0} constant({1})
  %select.4828 = bf16[1]{0} select(pred[1]{0} %compare.4826, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.190), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4830 = bf16[] reshape(bf16[1]{0} %select.4828), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4831 = bf16[32]{0} broadcast(bf16[] %reshape.4830), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4832 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4413, bf16[32]{0} %broadcast.4831), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4849 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4850 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4832, bf16[32]{0} %broadcast.4849), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4854 = bf16[32]{0} add(bf16[32]{0} %multiply.4853, bf16[32]{0} %multiply.4850), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p100.4833 = bf16[32]{0} parameter(100), frontend_attributes={neff_input_names="input100"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4834 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4835 = bf16[32]{0} multiply(bf16[32]{0} %p100.4833, bf16[32]{0} %broadcast.4834), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4837 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4832, bf16[32]{0} %multiply.4832), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4836 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4838 = bf16[32]{0} broadcast(bf16[] %convert.4836), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4839 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4837, bf16[32]{0} %broadcast.4838), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4840 = bf16[32]{0} add(bf16[32]{0} %multiply.4835, bf16[32]{0} %multiply.4839), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4841 = bf16[32]{0} sqrt(bf16[32]{0} %add.4840), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4842 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4843 = bf16[32]{0} divide(bf16[32]{0} %sqrt.4841, bf16[32]{0} %broadcast.4842), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4844 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4845 = bf16[32]{0} add(bf16[32]{0} %divide.4843, bf16[32]{0} %broadcast.4844), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4868 = bf16[32]{0} divide(bf16[32]{0} %add.4854, bf16[32]{0} %add.4845), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4867 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4869 = bf16[32]{0} broadcast(bf16[] %convert.4867), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4870 = bf16[32]{0} multiply(bf16[32]{0} %divide.4868, bf16[32]{0} %broadcast.4869), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4871 = bf16[32]{0} add(bf16[32]{0} %subtract.4866, bf16[32]{0} %multiply.4870), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.4875 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.4812), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.4876 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.4871, bf16[] %get-tuple-element.4875), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4877 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.4876), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p105.4924 = bf16[32]{0} parameter(105), frontend_attributes={neff_input_names="input105"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4925 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4926 = bf16[32]{0} multiply(bf16[32]{0} %p105.4924, bf16[32]{0} %broadcast.4925), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4927 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4929 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4926, bf16[32]{0} %broadcast.4927), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4930 = bf16[32]{0} subtract(bf16[32]{0} %p105.4924, bf16[32]{0} %multiply.4929), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p104.4915 = bf16[32]{0} parameter(104), frontend_attributes={neff_input_names="input104"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4916 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4917 = bf16[32]{0} multiply(bf16[32]{0} %p104.4915, bf16[32]{0} %broadcast.4916), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.192 = bf16[1]{0} constant({1})
  %compare.4890 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.192), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.193 = bf16[1]{0} constant({1})
  %select.4892 = bf16[1]{0} select(pred[1]{0} %compare.4890, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.193), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4894 = bf16[] reshape(bf16[1]{0} %select.4892), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4895 = bf16[32]{0} broadcast(bf16[] %reshape.4894), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4896 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4370, bf16[32]{0} %broadcast.4895), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4913 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4914 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4896, bf16[32]{0} %broadcast.4913), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4918 = bf16[32]{0} add(bf16[32]{0} %multiply.4917, bf16[32]{0} %multiply.4914), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p103.4897 = bf16[32]{0} parameter(103), frontend_attributes={neff_input_names="input103"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4898 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4899 = bf16[32]{0} multiply(bf16[32]{0} %p103.4897, bf16[32]{0} %broadcast.4898), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4901 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4896, bf16[32]{0} %multiply.4896), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4900 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4902 = bf16[32]{0} broadcast(bf16[] %convert.4900), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4903 = bf16[32]{0} multiply(bf16[32]{0} %multiply.4901, bf16[32]{0} %broadcast.4902), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4904 = bf16[32]{0} add(bf16[32]{0} %multiply.4899, bf16[32]{0} %multiply.4903), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4905 = bf16[32]{0} sqrt(bf16[32]{0} %add.4904), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4906 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4907 = bf16[32]{0} divide(bf16[32]{0} %sqrt.4905, bf16[32]{0} %broadcast.4906), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4908 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4909 = bf16[32]{0} add(bf16[32]{0} %divide.4907, bf16[32]{0} %broadcast.4908), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4932 = bf16[32]{0} divide(bf16[32]{0} %add.4918, bf16[32]{0} %add.4909), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4931 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4933 = bf16[32]{0} broadcast(bf16[] %convert.4931), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4934 = bf16[32]{0} multiply(bf16[32]{0} %divide.4932, bf16[32]{0} %broadcast.4933), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.4935 = bf16[32]{0} add(bf16[32]{0} %subtract.4930, bf16[32]{0} %multiply.4934), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.4939 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.4876), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.4940 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.4935, bf16[] %get-tuple-element.4939), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.4941 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.4940), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p108.4989 = bf16[96,32]{1,0} parameter(108), frontend_attributes={neff_input_names="input108"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4990 = bf16[96,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4991 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p108.4989, bf16[96,32]{1,0} %broadcast.4990), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.4992 = bf16[96,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.4994 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4991, bf16[96,32]{1,0} %broadcast.4992), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.4995 = bf16[96,32]{1,0} subtract(bf16[96,32]{1,0} %p108.4989, bf16[96,32]{1,0} %multiply.4994), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p107.4980 = bf16[96,32]{1,0} parameter(107), frontend_attributes={neff_input_names="input107"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.4981 = bf16[96,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4982 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p107.4980, bf16[96,32]{1,0} %broadcast.4981), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.195 = bf16[1]{0} constant({1})
  %compare.4954 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.195), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.196 = bf16[1]{0} constant({1})
  %select.4956 = bf16[1]{0} select(pred[1]{0} %compare.4954, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.196), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.4958 = bf16[] reshape(bf16[1]{0} %select.4956), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4960 = bf16[96,32]{1,0} broadcast(bf16[] %reshape.4958), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.4961 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.3287, bf16[96,32]{1,0} %broadcast.4960), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.4978 = bf16[96,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.4979 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4961, bf16[96,32]{1,0} %broadcast.4978), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.4983 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.4982, bf16[96,32]{1,0} %multiply.4979), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p106.4962 = bf16[96,32]{1,0} parameter(106), frontend_attributes={neff_input_names="input106"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4963 = bf16[96,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4964 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p106.4962, bf16[96,32]{1,0} %broadcast.4963), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4966 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4961, bf16[96,32]{1,0} %multiply.4961), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.4965 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.4967 = bf16[96,32]{1,0} broadcast(bf16[] %convert.4965), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.4968 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.4966, bf16[96,32]{1,0} %broadcast.4967), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.4969 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.4964, bf16[96,32]{1,0} %multiply.4968), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.4970 = bf16[96,32]{1,0} sqrt(bf16[96,32]{1,0} %add.4969), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4971 = bf16[96,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4972 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %sqrt.4970, bf16[96,32]{1,0} %broadcast.4971), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.4973 = bf16[96,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.4974 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %divide.4972, bf16[96,32]{1,0} %broadcast.4973), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.4997 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %add.4983, bf16[96,32]{1,0} %add.4974), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.4996 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.4998 = bf16[96,32]{1,0} broadcast(bf16[] %convert.4996), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.4999 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %divide.4997, bf16[96,32]{1,0} %broadcast.4998), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5000 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %subtract.4995, bf16[96,32]{1,0} %multiply.4999), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5004 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.4940), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5005 = (bf16[96,32]{1,0}, bf16[]) all-gather(bf16[96,32]{1,0} %add.5000, bf16[] %get-tuple-element.5004), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5006 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.5005), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p111.5054 = bf16[32,32]{1,0} parameter(111), frontend_attributes={neff_input_names="input111"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5055 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5056 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p111.5054, bf16[32,32]{1,0} %broadcast.5055), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5057 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5059 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5056, bf16[32,32]{1,0} %broadcast.5057), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5060 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p111.5054, bf16[32,32]{1,0} %multiply.5059), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p110.5045 = bf16[32,32]{1,0} parameter(110), frontend_attributes={neff_input_names="input110"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5046 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5047 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p110.5045, bf16[32,32]{1,0} %broadcast.5046), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.198 = bf16[1]{0} constant({1})
  %compare.5019 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.198), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.199 = bf16[1]{0} constant({1})
  %select.5021 = bf16[1]{0} select(pred[1]{0} %compare.5019, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.199), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5023 = bf16[] reshape(bf16[1]{0} %select.5021), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5025 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.5023), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5026 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.3007, bf16[32,32]{1,0} %broadcast.5025), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5043 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5044 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5026, bf16[32,32]{1,0} %broadcast.5043), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5048 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5047, bf16[32,32]{1,0} %multiply.5044), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p109.5027 = bf16[32,32]{1,0} parameter(109), frontend_attributes={neff_input_names="input109"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5028 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5029 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p109.5027, bf16[32,32]{1,0} %broadcast.5028), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5031 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5026, bf16[32,32]{1,0} %multiply.5026), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5030 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5032 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5030), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5033 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5031, bf16[32,32]{1,0} %broadcast.5032), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5034 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5029, bf16[32,32]{1,0} %multiply.5033), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5035 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.5034), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5036 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5037 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.5035, bf16[32,32]{1,0} %broadcast.5036), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5038 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5039 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.5037, bf16[32,32]{1,0} %broadcast.5038), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5062 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.5048, bf16[32,32]{1,0} %add.5039), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5061 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5063 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5061), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5064 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.5062, bf16[32,32]{1,0} %broadcast.5063), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5065 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.5060, bf16[32,32]{1,0} %multiply.5064), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5069 = bf16[] get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.5005), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5070 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.5065, bf16[] %get-tuple-element.5069), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5071 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5070), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p114.5119 = bf16[64,32]{1,0} parameter(114), frontend_attributes={neff_input_names="input114"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5120 = bf16[64,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5121 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p114.5119, bf16[64,32]{1,0} %broadcast.5120), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5122 = bf16[64,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5124 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5121, bf16[64,32]{1,0} %broadcast.5122), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5125 = bf16[64,32]{1,0} subtract(bf16[64,32]{1,0} %p114.5119, bf16[64,32]{1,0} %multiply.5124), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p113.5110 = bf16[64,32]{1,0} parameter(113), frontend_attributes={neff_input_names="input113"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5111 = bf16[64,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5112 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p113.5110, bf16[64,32]{1,0} %broadcast.5111), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.201 = bf16[1]{0} constant({1})
  %compare.5084 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.201), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.202 = bf16[1]{0} constant({1})
  %select.5086 = bf16[1]{0} select(pred[1]{0} %compare.5084, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.202), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5088 = bf16[] reshape(bf16[1]{0} %select.5086), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5090 = bf16[64,32]{1,0} broadcast(bf16[] %reshape.5088), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5091 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.2892, bf16[64,32]{1,0} %broadcast.5090), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5108 = bf16[64,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5109 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5091, bf16[64,32]{1,0} %broadcast.5108), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5113 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.5112, bf16[64,32]{1,0} %multiply.5109), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p112.5092 = bf16[64,32]{1,0} parameter(112), frontend_attributes={neff_input_names="input112"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5093 = bf16[64,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5094 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p112.5092, bf16[64,32]{1,0} %broadcast.5093), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5096 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5091, bf16[64,32]{1,0} %multiply.5091), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5095 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5097 = bf16[64,32]{1,0} broadcast(bf16[] %convert.5095), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5098 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5096, bf16[64,32]{1,0} %broadcast.5097), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5099 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.5094, bf16[64,32]{1,0} %multiply.5098), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5100 = bf16[64,32]{1,0} sqrt(bf16[64,32]{1,0} %add.5099), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5101 = bf16[64,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5102 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %sqrt.5100, bf16[64,32]{1,0} %broadcast.5101), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5103 = bf16[64,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5104 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %divide.5102, bf16[64,32]{1,0} %broadcast.5103), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5127 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %add.5113, bf16[64,32]{1,0} %add.5104), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5126 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5128 = bf16[64,32]{1,0} broadcast(bf16[] %convert.5126), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5129 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %divide.5127, bf16[64,32]{1,0} %broadcast.5128), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5130 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %subtract.5125, bf16[64,32]{1,0} %multiply.5129), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5134 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5070), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5135 = (bf16[64,32]{1,0}, bf16[]) all-gather(bf16[64,32]{1,0} %add.5130, bf16[] %get-tuple-element.5134), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5136 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.5135), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p117.5184 = bf16[32,32]{1,0} parameter(117), frontend_attributes={neff_input_names="input117"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5185 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5186 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p117.5184, bf16[32,32]{1,0} %broadcast.5185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5187 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5189 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5186, bf16[32,32]{1,0} %broadcast.5187), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5190 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p117.5184, bf16[32,32]{1,0} %multiply.5189), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p116.5175 = bf16[32,32]{1,0} parameter(116), frontend_attributes={neff_input_names="input116"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5176 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5177 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p116.5175, bf16[32,32]{1,0} %broadcast.5176), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.204 = bf16[1]{0} constant({1})
  %compare.5149 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.204), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.205 = bf16[1]{0} constant({1})
  %select.5151 = bf16[1]{0} select(pred[1]{0} %compare.5149, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.205), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5153 = bf16[] reshape(bf16[1]{0} %select.5151), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5155 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.5153), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5156 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.2809, bf16[32,32]{1,0} %broadcast.5155), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5173 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5174 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5156, bf16[32,32]{1,0} %broadcast.5173), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5178 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5177, bf16[32,32]{1,0} %multiply.5174), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p115.5157 = bf16[32,32]{1,0} parameter(115), frontend_attributes={neff_input_names="input115"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5158 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5159 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p115.5157, bf16[32,32]{1,0} %broadcast.5158), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5161 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5156, bf16[32,32]{1,0} %multiply.5156), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5160 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5162 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5160), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5163 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5161, bf16[32,32]{1,0} %broadcast.5162), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5164 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5159, bf16[32,32]{1,0} %multiply.5163), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5165 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.5164), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5166 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5167 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.5165, bf16[32,32]{1,0} %broadcast.5166), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5168 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5169 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.5167, bf16[32,32]{1,0} %broadcast.5168), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5192 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.5178, bf16[32,32]{1,0} %add.5169), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5191 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5193 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5191), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5194 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.5192, bf16[32,32]{1,0} %broadcast.5193), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5195 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.5190, bf16[32,32]{1,0} %multiply.5194), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5199 = bf16[] get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.5135), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5200 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.5195, bf16[] %get-tuple-element.5199), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5201 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5200), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p120.5248 = bf16[32]{0} parameter(120), frontend_attributes={neff_input_names="input120"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5249 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5250 = bf16[32]{0} multiply(bf16[32]{0} %p120.5248, bf16[32]{0} %broadcast.5249), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5251 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5253 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5250, bf16[32]{0} %broadcast.5251), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5254 = bf16[32]{0} subtract(bf16[32]{0} %p120.5248, bf16[32]{0} %multiply.5253), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p119.5239 = bf16[32]{0} parameter(119), frontend_attributes={neff_input_names="input119"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5240 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5241 = bf16[32]{0} multiply(bf16[32]{0} %p119.5239, bf16[32]{0} %broadcast.5240), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.207 = bf16[1]{0} constant({1})
  %compare.5214 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.207), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.208 = bf16[1]{0} constant({1})
  %select.5216 = bf16[1]{0} select(pred[1]{0} %compare.5214, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.208), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5218 = bf16[] reshape(bf16[1]{0} %select.5216), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5219 = bf16[32]{0} broadcast(bf16[] %reshape.5218), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5220 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4327, bf16[32]{0} %broadcast.5219), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5237 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5238 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5220, bf16[32]{0} %broadcast.5237), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5242 = bf16[32]{0} add(bf16[32]{0} %multiply.5241, bf16[32]{0} %multiply.5238), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p118.5221 = bf16[32]{0} parameter(118), frontend_attributes={neff_input_names="input118"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5222 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5223 = bf16[32]{0} multiply(bf16[32]{0} %p118.5221, bf16[32]{0} %broadcast.5222), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5225 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5220, bf16[32]{0} %multiply.5220), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5224 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5226 = bf16[32]{0} broadcast(bf16[] %convert.5224), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5227 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5225, bf16[32]{0} %broadcast.5226), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5228 = bf16[32]{0} add(bf16[32]{0} %multiply.5223, bf16[32]{0} %multiply.5227), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5229 = bf16[32]{0} sqrt(bf16[32]{0} %add.5228), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5230 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5231 = bf16[32]{0} divide(bf16[32]{0} %sqrt.5229, bf16[32]{0} %broadcast.5230), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5232 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5233 = bf16[32]{0} add(bf16[32]{0} %divide.5231, bf16[32]{0} %broadcast.5232), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5256 = bf16[32]{0} divide(bf16[32]{0} %add.5242, bf16[32]{0} %add.5233), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5255 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5257 = bf16[32]{0} broadcast(bf16[] %convert.5255), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5258 = bf16[32]{0} multiply(bf16[32]{0} %divide.5256, bf16[32]{0} %broadcast.5257), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5259 = bf16[32]{0} add(bf16[32]{0} %subtract.5254, bf16[32]{0} %multiply.5258), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5263 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5200), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5264 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.5259, bf16[] %get-tuple-element.5263), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5265 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5264), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p123.5312 = bf16[32]{0} parameter(123), frontend_attributes={neff_input_names="input123"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5313 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5314 = bf16[32]{0} multiply(bf16[32]{0} %p123.5312, bf16[32]{0} %broadcast.5313), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5315 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5317 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5314, bf16[32]{0} %broadcast.5315), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5318 = bf16[32]{0} subtract(bf16[32]{0} %p123.5312, bf16[32]{0} %multiply.5317), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p122.5303 = bf16[32]{0} parameter(122), frontend_attributes={neff_input_names="input122"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5304 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5305 = bf16[32]{0} multiply(bf16[32]{0} %p122.5303, bf16[32]{0} %broadcast.5304), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.210 = bf16[1]{0} constant({1})
  %compare.5278 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.210), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.211 = bf16[1]{0} constant({1})
  %select.5280 = bf16[1]{0} select(pred[1]{0} %compare.5278, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.211), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5282 = bf16[] reshape(bf16[1]{0} %select.5280), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5283 = bf16[32]{0} broadcast(bf16[] %reshape.5282), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5284 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4284, bf16[32]{0} %broadcast.5283), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5301 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5302 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5284, bf16[32]{0} %broadcast.5301), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5306 = bf16[32]{0} add(bf16[32]{0} %multiply.5305, bf16[32]{0} %multiply.5302), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p121.5285 = bf16[32]{0} parameter(121), frontend_attributes={neff_input_names="input121"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5286 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5287 = bf16[32]{0} multiply(bf16[32]{0} %p121.5285, bf16[32]{0} %broadcast.5286), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5289 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5284, bf16[32]{0} %multiply.5284), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5288 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5290 = bf16[32]{0} broadcast(bf16[] %convert.5288), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5291 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5289, bf16[32]{0} %broadcast.5290), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5292 = bf16[32]{0} add(bf16[32]{0} %multiply.5287, bf16[32]{0} %multiply.5291), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5293 = bf16[32]{0} sqrt(bf16[32]{0} %add.5292), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5294 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5295 = bf16[32]{0} divide(bf16[32]{0} %sqrt.5293, bf16[32]{0} %broadcast.5294), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5296 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5297 = bf16[32]{0} add(bf16[32]{0} %divide.5295, bf16[32]{0} %broadcast.5296), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5320 = bf16[32]{0} divide(bf16[32]{0} %add.5306, bf16[32]{0} %add.5297), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5319 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5321 = bf16[32]{0} broadcast(bf16[] %convert.5319), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5322 = bf16[32]{0} multiply(bf16[32]{0} %divide.5320, bf16[32]{0} %broadcast.5321), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5323 = bf16[32]{0} add(bf16[32]{0} %subtract.5318, bf16[32]{0} %multiply.5322), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5327 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5264), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5328 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.5323, bf16[] %get-tuple-element.5327), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5329 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5328), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p126.5377 = bf16[96,32]{1,0} parameter(126), frontend_attributes={neff_input_names="input126"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5378 = bf16[96,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5379 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p126.5377, bf16[96,32]{1,0} %broadcast.5378), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5380 = bf16[96,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5382 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5379, bf16[96,32]{1,0} %broadcast.5380), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5383 = bf16[96,32]{1,0} subtract(bf16[96,32]{1,0} %p126.5377, bf16[96,32]{1,0} %multiply.5382), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p125.5368 = bf16[96,32]{1,0} parameter(125), frontend_attributes={neff_input_names="input125"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5369 = bf16[96,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5370 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p125.5368, bf16[96,32]{1,0} %broadcast.5369), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.213 = bf16[1]{0} constant({1})
  %compare.5342 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.213), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.214 = bf16[1]{0} constant({1})
  %select.5344 = bf16[1]{0} select(pred[1]{0} %compare.5342, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.214), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5346 = bf16[] reshape(bf16[1]{0} %select.5344), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5348 = bf16[96,32]{1,0} broadcast(bf16[] %reshape.5346), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5349 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.2697, bf16[96,32]{1,0} %broadcast.5348), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5366 = bf16[96,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5367 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5349, bf16[96,32]{1,0} %broadcast.5366), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5371 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.5370, bf16[96,32]{1,0} %multiply.5367), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p124.5350 = bf16[96,32]{1,0} parameter(124), frontend_attributes={neff_input_names="input124"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5351 = bf16[96,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5352 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p124.5350, bf16[96,32]{1,0} %broadcast.5351), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5354 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5349, bf16[96,32]{1,0} %multiply.5349), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5353 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5355 = bf16[96,32]{1,0} broadcast(bf16[] %convert.5353), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5356 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5354, bf16[96,32]{1,0} %broadcast.5355), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5357 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.5352, bf16[96,32]{1,0} %multiply.5356), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5358 = bf16[96,32]{1,0} sqrt(bf16[96,32]{1,0} %add.5357), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5359 = bf16[96,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5360 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %sqrt.5358, bf16[96,32]{1,0} %broadcast.5359), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5361 = bf16[96,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5362 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %divide.5360, bf16[96,32]{1,0} %broadcast.5361), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5385 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %add.5371, bf16[96,32]{1,0} %add.5362), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5384 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5386 = bf16[96,32]{1,0} broadcast(bf16[] %convert.5384), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5387 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %divide.5385, bf16[96,32]{1,0} %broadcast.5386), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5388 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %subtract.5383, bf16[96,32]{1,0} %multiply.5387), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5392 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5328), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5393 = (bf16[96,32]{1,0}, bf16[]) all-gather(bf16[96,32]{1,0} %add.5388, bf16[] %get-tuple-element.5392), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5394 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.5393), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p129.5442 = bf16[32,32]{1,0} parameter(129), frontend_attributes={neff_input_names="input129"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5443 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5444 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p129.5442, bf16[32,32]{1,0} %broadcast.5443), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5445 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5447 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5444, bf16[32,32]{1,0} %broadcast.5445), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5448 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p129.5442, bf16[32,32]{1,0} %multiply.5447), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p128.5433 = bf16[32,32]{1,0} parameter(128), frontend_attributes={neff_input_names="input128"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5434 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5435 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p128.5433, bf16[32,32]{1,0} %broadcast.5434), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.216 = bf16[1]{0} constant({1})
  %compare.5407 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.216), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.217 = bf16[1]{0} constant({1})
  %select.5409 = bf16[1]{0} select(pred[1]{0} %compare.5407, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.217), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5411 = bf16[] reshape(bf16[1]{0} %select.5409), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5413 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.5411), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5414 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.2417, bf16[32,32]{1,0} %broadcast.5413), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5431 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5432 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5414, bf16[32,32]{1,0} %broadcast.5431), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5436 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5435, bf16[32,32]{1,0} %multiply.5432), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p127.5415 = bf16[32,32]{1,0} parameter(127), frontend_attributes={neff_input_names="input127"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5416 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5417 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p127.5415, bf16[32,32]{1,0} %broadcast.5416), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5419 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5414, bf16[32,32]{1,0} %multiply.5414), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5418 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5420 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5418), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5421 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5419, bf16[32,32]{1,0} %broadcast.5420), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5422 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5417, bf16[32,32]{1,0} %multiply.5421), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5423 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.5422), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5424 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5425 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.5423, bf16[32,32]{1,0} %broadcast.5424), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5426 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5427 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.5425, bf16[32,32]{1,0} %broadcast.5426), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5450 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.5436, bf16[32,32]{1,0} %add.5427), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5449 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5451 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5449), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5452 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.5450, bf16[32,32]{1,0} %broadcast.5451), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5453 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.5448, bf16[32,32]{1,0} %multiply.5452), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5457 = bf16[] get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.5393), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5458 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.5453, bf16[] %get-tuple-element.5457), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5459 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5458), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p132.5507 = bf16[64,32]{1,0} parameter(132), frontend_attributes={neff_input_names="input132"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5508 = bf16[64,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5509 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p132.5507, bf16[64,32]{1,0} %broadcast.5508), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5510 = bf16[64,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5512 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5509, bf16[64,32]{1,0} %broadcast.5510), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5513 = bf16[64,32]{1,0} subtract(bf16[64,32]{1,0} %p132.5507, bf16[64,32]{1,0} %multiply.5512), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p131.5498 = bf16[64,32]{1,0} parameter(131), frontend_attributes={neff_input_names="input131"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5499 = bf16[64,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5500 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p131.5498, bf16[64,32]{1,0} %broadcast.5499), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.219 = bf16[1]{0} constant({1})
  %compare.5472 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.219), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.220 = bf16[1]{0} constant({1})
  %select.5474 = bf16[1]{0} select(pred[1]{0} %compare.5472, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.220), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5476 = bf16[] reshape(bf16[1]{0} %select.5474), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5478 = bf16[64,32]{1,0} broadcast(bf16[] %reshape.5476), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5479 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.2302, bf16[64,32]{1,0} %broadcast.5478), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5496 = bf16[64,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5497 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5479, bf16[64,32]{1,0} %broadcast.5496), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5501 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.5500, bf16[64,32]{1,0} %multiply.5497), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p130.5480 = bf16[64,32]{1,0} parameter(130), frontend_attributes={neff_input_names="input130"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5481 = bf16[64,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5482 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p130.5480, bf16[64,32]{1,0} %broadcast.5481), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5484 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5479, bf16[64,32]{1,0} %multiply.5479), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5483 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5485 = bf16[64,32]{1,0} broadcast(bf16[] %convert.5483), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5486 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5484, bf16[64,32]{1,0} %broadcast.5485), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5487 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.5482, bf16[64,32]{1,0} %multiply.5486), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5488 = bf16[64,32]{1,0} sqrt(bf16[64,32]{1,0} %add.5487), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5489 = bf16[64,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5490 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %sqrt.5488, bf16[64,32]{1,0} %broadcast.5489), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5491 = bf16[64,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5492 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %divide.5490, bf16[64,32]{1,0} %broadcast.5491), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5515 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %add.5501, bf16[64,32]{1,0} %add.5492), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5514 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5516 = bf16[64,32]{1,0} broadcast(bf16[] %convert.5514), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5517 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %divide.5515, bf16[64,32]{1,0} %broadcast.5516), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5518 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %subtract.5513, bf16[64,32]{1,0} %multiply.5517), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5522 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5458), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5523 = (bf16[64,32]{1,0}, bf16[]) all-gather(bf16[64,32]{1,0} %add.5518, bf16[] %get-tuple-element.5522), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5524 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.5523), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p135.5572 = bf16[32,32]{1,0} parameter(135), frontend_attributes={neff_input_names="input135"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5573 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5574 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p135.5572, bf16[32,32]{1,0} %broadcast.5573), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5575 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5577 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5574, bf16[32,32]{1,0} %broadcast.5575), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5578 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p135.5572, bf16[32,32]{1,0} %multiply.5577), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p134.5563 = bf16[32,32]{1,0} parameter(134), frontend_attributes={neff_input_names="input134"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5564 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5565 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p134.5563, bf16[32,32]{1,0} %broadcast.5564), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.222 = bf16[1]{0} constant({1})
  %compare.5537 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.222), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.223 = bf16[1]{0} constant({1})
  %select.5539 = bf16[1]{0} select(pred[1]{0} %compare.5537, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.223), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5541 = bf16[] reshape(bf16[1]{0} %select.5539), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5543 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.5541), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5544 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.2219, bf16[32,32]{1,0} %broadcast.5543), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5561 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5562 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5544, bf16[32,32]{1,0} %broadcast.5561), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5566 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5565, bf16[32,32]{1,0} %multiply.5562), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p133.5545 = bf16[32,32]{1,0} parameter(133), frontend_attributes={neff_input_names="input133"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5546 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5547 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p133.5545, bf16[32,32]{1,0} %broadcast.5546), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5549 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5544, bf16[32,32]{1,0} %multiply.5544), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5548 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5550 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5548), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5551 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5549, bf16[32,32]{1,0} %broadcast.5550), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5552 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5547, bf16[32,32]{1,0} %multiply.5551), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5553 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.5552), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5554 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5555 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.5553, bf16[32,32]{1,0} %broadcast.5554), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5556 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5557 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.5555, bf16[32,32]{1,0} %broadcast.5556), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5580 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.5566, bf16[32,32]{1,0} %add.5557), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5579 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5581 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5579), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5582 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.5580, bf16[32,32]{1,0} %broadcast.5581), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5583 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.5578, bf16[32,32]{1,0} %multiply.5582), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5587 = bf16[] get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.5523), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5588 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.5583, bf16[] %get-tuple-element.5587), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5589 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5588), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p138.5636 = bf16[32]{0} parameter(138), frontend_attributes={neff_input_names="input138"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5637 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5638 = bf16[32]{0} multiply(bf16[32]{0} %p138.5636, bf16[32]{0} %broadcast.5637), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5639 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5641 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5638, bf16[32]{0} %broadcast.5639), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5642 = bf16[32]{0} subtract(bf16[32]{0} %p138.5636, bf16[32]{0} %multiply.5641), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p137.5627 = bf16[32]{0} parameter(137), frontend_attributes={neff_input_names="input137"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5628 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5629 = bf16[32]{0} multiply(bf16[32]{0} %p137.5627, bf16[32]{0} %broadcast.5628), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.226 = bf16[1]{0} constant({1})
  %compare.5602 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.226), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.228 = bf16[1]{0} constant({1})
  %select.5604 = bf16[1]{0} select(pred[1]{0} %compare.5602, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.228), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5606 = bf16[] reshape(bf16[1]{0} %select.5604), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5607 = bf16[32]{0} broadcast(bf16[] %reshape.5606), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5608 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4241, bf16[32]{0} %broadcast.5607), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5625 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5626 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5608, bf16[32]{0} %broadcast.5625), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5630 = bf16[32]{0} add(bf16[32]{0} %multiply.5629, bf16[32]{0} %multiply.5626), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p136.5609 = bf16[32]{0} parameter(136), frontend_attributes={neff_input_names="input136"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5610 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5611 = bf16[32]{0} multiply(bf16[32]{0} %p136.5609, bf16[32]{0} %broadcast.5610), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5613 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5608, bf16[32]{0} %multiply.5608), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5612 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5614 = bf16[32]{0} broadcast(bf16[] %convert.5612), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5615 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5613, bf16[32]{0} %broadcast.5614), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5616 = bf16[32]{0} add(bf16[32]{0} %multiply.5611, bf16[32]{0} %multiply.5615), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5617 = bf16[32]{0} sqrt(bf16[32]{0} %add.5616), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5618 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5619 = bf16[32]{0} divide(bf16[32]{0} %sqrt.5617, bf16[32]{0} %broadcast.5618), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5620 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5621 = bf16[32]{0} add(bf16[32]{0} %divide.5619, bf16[32]{0} %broadcast.5620), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5644 = bf16[32]{0} divide(bf16[32]{0} %add.5630, bf16[32]{0} %add.5621), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5643 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5645 = bf16[32]{0} broadcast(bf16[] %convert.5643), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5646 = bf16[32]{0} multiply(bf16[32]{0} %divide.5644, bf16[32]{0} %broadcast.5645), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5647 = bf16[32]{0} add(bf16[32]{0} %subtract.5642, bf16[32]{0} %multiply.5646), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5651 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5588), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5652 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.5647, bf16[] %get-tuple-element.5651), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5653 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5652), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p141.5700 = bf16[32]{0} parameter(141), frontend_attributes={neff_input_names="input141"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5701 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5702 = bf16[32]{0} multiply(bf16[32]{0} %p141.5700, bf16[32]{0} %broadcast.5701), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5703 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5705 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5702, bf16[32]{0} %broadcast.5703), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5706 = bf16[32]{0} subtract(bf16[32]{0} %p141.5700, bf16[32]{0} %multiply.5705), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p140.5691 = bf16[32]{0} parameter(140), frontend_attributes={neff_input_names="input140"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5692 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5693 = bf16[32]{0} multiply(bf16[32]{0} %p140.5691, bf16[32]{0} %broadcast.5692), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.230 = bf16[1]{0} constant({1})
  %compare.5666 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.230), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.231 = bf16[1]{0} constant({1})
  %select.5668 = bf16[1]{0} select(pred[1]{0} %compare.5666, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.231), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5670 = bf16[] reshape(bf16[1]{0} %select.5668), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5671 = bf16[32]{0} broadcast(bf16[] %reshape.5670), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5672 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4198, bf16[32]{0} %broadcast.5671), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5689 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5690 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5672, bf16[32]{0} %broadcast.5689), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5694 = bf16[32]{0} add(bf16[32]{0} %multiply.5693, bf16[32]{0} %multiply.5690), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p139.5673 = bf16[32]{0} parameter(139), frontend_attributes={neff_input_names="input139"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5674 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5675 = bf16[32]{0} multiply(bf16[32]{0} %p139.5673, bf16[32]{0} %broadcast.5674), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5677 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5672, bf16[32]{0} %multiply.5672), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5676 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5678 = bf16[32]{0} broadcast(bf16[] %convert.5676), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5679 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5677, bf16[32]{0} %broadcast.5678), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5680 = bf16[32]{0} add(bf16[32]{0} %multiply.5675, bf16[32]{0} %multiply.5679), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5681 = bf16[32]{0} sqrt(bf16[32]{0} %add.5680), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5682 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5683 = bf16[32]{0} divide(bf16[32]{0} %sqrt.5681, bf16[32]{0} %broadcast.5682), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5684 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5685 = bf16[32]{0} add(bf16[32]{0} %divide.5683, bf16[32]{0} %broadcast.5684), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5708 = bf16[32]{0} divide(bf16[32]{0} %add.5694, bf16[32]{0} %add.5685), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5707 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5709 = bf16[32]{0} broadcast(bf16[] %convert.5707), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5710 = bf16[32]{0} multiply(bf16[32]{0} %divide.5708, bf16[32]{0} %broadcast.5709), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5711 = bf16[32]{0} add(bf16[32]{0} %subtract.5706, bf16[32]{0} %multiply.5710), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5715 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5652), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5716 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.5711, bf16[] %get-tuple-element.5715), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5717 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5716), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p144.5765 = bf16[96,32]{1,0} parameter(144), frontend_attributes={neff_input_names="input144"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5766 = bf16[96,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5767 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p144.5765, bf16[96,32]{1,0} %broadcast.5766), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5768 = bf16[96,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5770 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5767, bf16[96,32]{1,0} %broadcast.5768), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5771 = bf16[96,32]{1,0} subtract(bf16[96,32]{1,0} %p144.5765, bf16[96,32]{1,0} %multiply.5770), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p143.5756 = bf16[96,32]{1,0} parameter(143), frontend_attributes={neff_input_names="input143"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5757 = bf16[96,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5758 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p143.5756, bf16[96,32]{1,0} %broadcast.5757), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.233 = bf16[1]{0} constant({1})
  %compare.5730 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.233), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.234 = bf16[1]{0} constant({1})
  %select.5732 = bf16[1]{0} select(pred[1]{0} %compare.5730, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.234), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5734 = bf16[] reshape(bf16[1]{0} %select.5732), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5736 = bf16[96,32]{1,0} broadcast(bf16[] %reshape.5734), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5737 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %get-tuple-element.2107, bf16[96,32]{1,0} %broadcast.5736), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5754 = bf16[96,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5755 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5737, bf16[96,32]{1,0} %broadcast.5754), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5759 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.5758, bf16[96,32]{1,0} %multiply.5755), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p142.5738 = bf16[96,32]{1,0} parameter(142), frontend_attributes={neff_input_names="input142"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5739 = bf16[96,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5740 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %p142.5738, bf16[96,32]{1,0} %broadcast.5739), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5742 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5737, bf16[96,32]{1,0} %multiply.5737), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5741 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5743 = bf16[96,32]{1,0} broadcast(bf16[] %convert.5741), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5744 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %multiply.5742, bf16[96,32]{1,0} %broadcast.5743), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5745 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %multiply.5740, bf16[96,32]{1,0} %multiply.5744), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5746 = bf16[96,32]{1,0} sqrt(bf16[96,32]{1,0} %add.5745), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5747 = bf16[96,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5748 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %sqrt.5746, bf16[96,32]{1,0} %broadcast.5747), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5749 = bf16[96,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5750 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %divide.5748, bf16[96,32]{1,0} %broadcast.5749), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5773 = bf16[96,32]{1,0} divide(bf16[96,32]{1,0} %add.5759, bf16[96,32]{1,0} %add.5750), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5772 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5774 = bf16[96,32]{1,0} broadcast(bf16[] %convert.5772), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5775 = bf16[96,32]{1,0} multiply(bf16[96,32]{1,0} %divide.5773, bf16[96,32]{1,0} %broadcast.5774), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5776 = bf16[96,32]{1,0} add(bf16[96,32]{1,0} %subtract.5771, bf16[96,32]{1,0} %multiply.5775), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5780 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.5716), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5781 = (bf16[96,32]{1,0}, bf16[]) all-gather(bf16[96,32]{1,0} %add.5776, bf16[] %get-tuple-element.5780), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5782 = bf16[96,32]{1,0} get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.5781), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p147.5830 = bf16[32,32]{1,0} parameter(147), frontend_attributes={neff_input_names="input147"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5831 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5832 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p147.5830, bf16[32,32]{1,0} %broadcast.5831), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5833 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5835 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5832, bf16[32,32]{1,0} %broadcast.5833), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5836 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p147.5830, bf16[32,32]{1,0} %multiply.5835), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p146.5821 = bf16[32,32]{1,0} parameter(146), frontend_attributes={neff_input_names="input146"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5822 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5823 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p146.5821, bf16[32,32]{1,0} %broadcast.5822), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.236 = bf16[1]{0} constant({1})
  %compare.5795 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.236), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.237 = bf16[1]{0} constant({1})
  %select.5797 = bf16[1]{0} select(pred[1]{0} %compare.5795, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.237), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5799 = bf16[] reshape(bf16[1]{0} %select.5797), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5801 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.5799), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5802 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.1827, bf16[32,32]{1,0} %broadcast.5801), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5819 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5820 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5802, bf16[32,32]{1,0} %broadcast.5819), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5824 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5823, bf16[32,32]{1,0} %multiply.5820), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p145.5803 = bf16[32,32]{1,0} parameter(145), frontend_attributes={neff_input_names="input145"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5804 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5805 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p145.5803, bf16[32,32]{1,0} %broadcast.5804), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5807 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5802, bf16[32,32]{1,0} %multiply.5802), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5806 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5808 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5806), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5809 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5807, bf16[32,32]{1,0} %broadcast.5808), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5810 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5805, bf16[32,32]{1,0} %multiply.5809), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5811 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.5810), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5812 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5813 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.5811, bf16[32,32]{1,0} %broadcast.5812), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5814 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5815 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.5813, bf16[32,32]{1,0} %broadcast.5814), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5838 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.5824, bf16[32,32]{1,0} %add.5815), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5837 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5839 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5837), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5840 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.5838, bf16[32,32]{1,0} %broadcast.5839), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5841 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.5836, bf16[32,32]{1,0} %multiply.5840), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5845 = bf16[] get-tuple-element((bf16[96,32]{1,0}, bf16[]) %all-gather.5781), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5846 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.5841, bf16[] %get-tuple-element.5845), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5847 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5846), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p150.5895 = bf16[64,32]{1,0} parameter(150), frontend_attributes={neff_input_names="input150"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5896 = bf16[64,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5897 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p150.5895, bf16[64,32]{1,0} %broadcast.5896), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5898 = bf16[64,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5900 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5897, bf16[64,32]{1,0} %broadcast.5898), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5901 = bf16[64,32]{1,0} subtract(bf16[64,32]{1,0} %p150.5895, bf16[64,32]{1,0} %multiply.5900), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p149.5886 = bf16[64,32]{1,0} parameter(149), frontend_attributes={neff_input_names="input149"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5887 = bf16[64,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5888 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p149.5886, bf16[64,32]{1,0} %broadcast.5887), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.239 = bf16[1]{0} constant({1})
  %compare.5860 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.239), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.240 = bf16[1]{0} constant({1})
  %select.5862 = bf16[1]{0} select(pred[1]{0} %compare.5860, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.240), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5864 = bf16[] reshape(bf16[1]{0} %select.5862), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5866 = bf16[64,32]{1,0} broadcast(bf16[] %reshape.5864), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5867 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %get-tuple-element.1712, bf16[64,32]{1,0} %broadcast.5866), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5884 = bf16[64,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5885 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5867, bf16[64,32]{1,0} %broadcast.5884), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5889 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.5888, bf16[64,32]{1,0} %multiply.5885), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p148.5868 = bf16[64,32]{1,0} parameter(148), frontend_attributes={neff_input_names="input148"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5869 = bf16[64,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5870 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %p148.5868, bf16[64,32]{1,0} %broadcast.5869), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5872 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5867, bf16[64,32]{1,0} %multiply.5867), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5871 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5873 = bf16[64,32]{1,0} broadcast(bf16[] %convert.5871), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5874 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %multiply.5872, bf16[64,32]{1,0} %broadcast.5873), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5875 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %multiply.5870, bf16[64,32]{1,0} %multiply.5874), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5876 = bf16[64,32]{1,0} sqrt(bf16[64,32]{1,0} %add.5875), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5877 = bf16[64,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5878 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %sqrt.5876, bf16[64,32]{1,0} %broadcast.5877), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5879 = bf16[64,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5880 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %divide.5878, bf16[64,32]{1,0} %broadcast.5879), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5903 = bf16[64,32]{1,0} divide(bf16[64,32]{1,0} %add.5889, bf16[64,32]{1,0} %add.5880), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5902 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5904 = bf16[64,32]{1,0} broadcast(bf16[] %convert.5902), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5905 = bf16[64,32]{1,0} multiply(bf16[64,32]{1,0} %divide.5903, bf16[64,32]{1,0} %broadcast.5904), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5906 = bf16[64,32]{1,0} add(bf16[64,32]{1,0} %subtract.5901, bf16[64,32]{1,0} %multiply.5905), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5910 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5846), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5911 = (bf16[64,32]{1,0}, bf16[]) all-gather(bf16[64,32]{1,0} %add.5906, bf16[] %get-tuple-element.5910), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5912 = bf16[64,32]{1,0} get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.5911), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p153.5960 = bf16[32,32]{1,0} parameter(153), frontend_attributes={neff_input_names="input153"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5961 = bf16[32,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5962 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p153.5960, bf16[32,32]{1,0} %broadcast.5961), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.5963 = bf16[32,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.5965 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5962, bf16[32,32]{1,0} %broadcast.5963), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.5966 = bf16[32,32]{1,0} subtract(bf16[32,32]{1,0} %p153.5960, bf16[32,32]{1,0} %multiply.5965), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p152.5951 = bf16[32,32]{1,0} parameter(152), frontend_attributes={neff_input_names="input152"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.5952 = bf16[32,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5953 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p152.5951, bf16[32,32]{1,0} %broadcast.5952), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.242 = bf16[1]{0} constant({1})
  %compare.5925 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.242), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.243 = bf16[1]{0} constant({1})
  %select.5927 = bf16[1]{0} select(pred[1]{0} %compare.5925, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.243), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5929 = bf16[] reshape(bf16[1]{0} %select.5927), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5931 = bf16[32,32]{1,0} broadcast(bf16[] %reshape.5929), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5932 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %get-tuple-element.1629, bf16[32,32]{1,0} %broadcast.5931), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5949 = bf16[32,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.5950 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5932, bf16[32,32]{1,0} %broadcast.5949), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.5954 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5953, bf16[32,32]{1,0} %multiply.5950), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p151.5933 = bf16[32,32]{1,0} parameter(151), frontend_attributes={neff_input_names="input151"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5934 = bf16[32,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5935 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %p151.5933, bf16[32,32]{1,0} %broadcast.5934), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5937 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5932, bf16[32,32]{1,0} %multiply.5932), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.5936 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5938 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5936), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5939 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %multiply.5937, bf16[32,32]{1,0} %broadcast.5938), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.5940 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %multiply.5935, bf16[32,32]{1,0} %multiply.5939), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.5941 = bf16[32,32]{1,0} sqrt(bf16[32,32]{1,0} %add.5940), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5942 = bf16[32,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5943 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %sqrt.5941, bf16[32,32]{1,0} %broadcast.5942), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.5944 = bf16[32,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.5945 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %divide.5943, bf16[32,32]{1,0} %broadcast.5944), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.5968 = bf16[32,32]{1,0} divide(bf16[32,32]{1,0} %add.5954, bf16[32,32]{1,0} %add.5945), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.5967 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.5969 = bf16[32,32]{1,0} broadcast(bf16[] %convert.5967), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.5970 = bf16[32,32]{1,0} multiply(bf16[32,32]{1,0} %divide.5968, bf16[32,32]{1,0} %broadcast.5969), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.5971 = bf16[32,32]{1,0} add(bf16[32,32]{1,0} %subtract.5966, bf16[32,32]{1,0} %multiply.5970), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.5975 = bf16[] get-tuple-element((bf16[64,32]{1,0}, bf16[]) %all-gather.5911), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.5976 = (bf16[32,32]{1,0}, bf16[]) all-gather(bf16[32,32]{1,0} %add.5971, bf16[] %get-tuple-element.5975), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.5977 = bf16[32,32]{1,0} get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5976), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p156.6024 = bf16[32]{0} parameter(156), frontend_attributes={neff_input_names="input156"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6025 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6026 = bf16[32]{0} multiply(bf16[32]{0} %p156.6024, bf16[32]{0} %broadcast.6025), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6027 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6029 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6026, bf16[32]{0} %broadcast.6027), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.6030 = bf16[32]{0} subtract(bf16[32]{0} %p156.6024, bf16[32]{0} %multiply.6029), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p155.6015 = bf16[32]{0} parameter(155), frontend_attributes={neff_input_names="input155"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.6016 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6017 = bf16[32]{0} multiply(bf16[32]{0} %p155.6015, bf16[32]{0} %broadcast.6016), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.245 = bf16[1]{0} constant({1})
  %compare.5990 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.245), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.246 = bf16[1]{0} constant({1})
  %select.5992 = bf16[1]{0} select(pred[1]{0} %compare.5990, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.246), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.5994 = bf16[] reshape(bf16[1]{0} %select.5992), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.5995 = bf16[32]{0} broadcast(bf16[] %reshape.5994), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.5996 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4155, bf16[32]{0} %broadcast.5995), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6013 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6014 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5996, bf16[32]{0} %broadcast.6013), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.6018 = bf16[32]{0} add(bf16[32]{0} %multiply.6017, bf16[32]{0} %multiply.6014), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p154.5997 = bf16[32]{0} parameter(154), frontend_attributes={neff_input_names="input154"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.5998 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.5999 = bf16[32]{0} multiply(bf16[32]{0} %p154.5997, bf16[32]{0} %broadcast.5998), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6001 = bf16[32]{0} multiply(bf16[32]{0} %multiply.5996, bf16[32]{0} %multiply.5996), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.6000 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6002 = bf16[32]{0} broadcast(bf16[] %convert.6000), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6003 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6001, bf16[32]{0} %broadcast.6002), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.6004 = bf16[32]{0} add(bf16[32]{0} %multiply.5999, bf16[32]{0} %multiply.6003), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.6005 = bf16[32]{0} sqrt(bf16[32]{0} %add.6004), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6006 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6007 = bf16[32]{0} divide(bf16[32]{0} %sqrt.6005, bf16[32]{0} %broadcast.6006), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6008 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.6009 = bf16[32]{0} add(bf16[32]{0} %divide.6007, bf16[32]{0} %broadcast.6008), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6032 = bf16[32]{0} divide(bf16[32]{0} %add.6018, bf16[32]{0} %add.6009), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.6031 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.6033 = bf16[32]{0} broadcast(bf16[] %convert.6031), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.6034 = bf16[32]{0} multiply(bf16[32]{0} %divide.6032, bf16[32]{0} %broadcast.6033), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.6035 = bf16[32]{0} add(bf16[32]{0} %subtract.6030, bf16[32]{0} %multiply.6034), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.6039 = bf16[] get-tuple-element((bf16[32,32]{1,0}, bf16[]) %all-gather.5976), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.6040 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.6035, bf16[] %get-tuple-element.6039), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.6041 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.6040), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p159.6088 = bf16[32]{0} parameter(159), frontend_attributes={neff_input_names="input159"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6089 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6090 = bf16[32]{0} multiply(bf16[32]{0} %p159.6088, bf16[32]{0} %broadcast.6089), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6091 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6093 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6090, bf16[32]{0} %broadcast.6091), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.6094 = bf16[32]{0} subtract(bf16[32]{0} %p159.6088, bf16[32]{0} %multiply.6093), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p158.6079 = bf16[32]{0} parameter(158), frontend_attributes={neff_input_names="input158"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.6080 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6081 = bf16[32]{0} multiply(bf16[32]{0} %p158.6079, bf16[32]{0} %broadcast.6080), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.248 = bf16[1]{0} constant({1})
  %compare.6054 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.248), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.249 = bf16[1]{0} constant({1})
  %select.6056 = bf16[1]{0} select(pred[1]{0} %compare.6054, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.249), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.6058 = bf16[] reshape(bf16[1]{0} %select.6056), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6059 = bf16[32]{0} broadcast(bf16[] %reshape.6058), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.6060 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4112, bf16[32]{0} %broadcast.6059), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6077 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6078 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6060, bf16[32]{0} %broadcast.6077), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.6082 = bf16[32]{0} add(bf16[32]{0} %multiply.6081, bf16[32]{0} %multiply.6078), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p157.6061 = bf16[32]{0} parameter(157), frontend_attributes={neff_input_names="input157"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6062 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6063 = bf16[32]{0} multiply(bf16[32]{0} %p157.6061, bf16[32]{0} %broadcast.6062), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6065 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6060, bf16[32]{0} %multiply.6060), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.6064 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6066 = bf16[32]{0} broadcast(bf16[] %convert.6064), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6067 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6065, bf16[32]{0} %broadcast.6066), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.6068 = bf16[32]{0} add(bf16[32]{0} %multiply.6063, bf16[32]{0} %multiply.6067), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.6069 = bf16[32]{0} sqrt(bf16[32]{0} %add.6068), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6070 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6071 = bf16[32]{0} divide(bf16[32]{0} %sqrt.6069, bf16[32]{0} %broadcast.6070), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6072 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.6073 = bf16[32]{0} add(bf16[32]{0} %divide.6071, bf16[32]{0} %broadcast.6072), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6096 = bf16[32]{0} divide(bf16[32]{0} %add.6082, bf16[32]{0} %add.6073), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.6095 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.6097 = bf16[32]{0} broadcast(bf16[] %convert.6095), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.6098 = bf16[32]{0} multiply(bf16[32]{0} %divide.6096, bf16[32]{0} %broadcast.6097), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.6099 = bf16[32]{0} add(bf16[32]{0} %subtract.6094, bf16[32]{0} %multiply.6098), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.6103 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.6040), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.6104 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.6099, bf16[] %get-tuple-element.6103), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.6105 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.6104), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p162.6152 = bf16[32]{0} parameter(162), frontend_attributes={neff_input_names="input162"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6153 = bf16[32]{0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6154 = bf16[32]{0} multiply(bf16[32]{0} %p162.6152, bf16[32]{0} %broadcast.6153), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6155 = bf16[32]{0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6157 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6154, bf16[32]{0} %broadcast.6155), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.6158 = bf16[32]{0} subtract(bf16[32]{0} %p162.6152, bf16[32]{0} %multiply.6157), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p161.6143 = bf16[32]{0} parameter(161), frontend_attributes={neff_input_names="input161"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.6144 = bf16[32]{0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6145 = bf16[32]{0} multiply(bf16[32]{0} %p161.6143, bf16[32]{0} %broadcast.6144), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.251 = bf16[1]{0} constant({1})
  %compare.6118 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.251), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.254 = bf16[1]{0} constant({1})
  %select.6120 = bf16[1]{0} select(pred[1]{0} %compare.6118, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.254), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.6122 = bf16[] reshape(bf16[1]{0} %select.6120), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6123 = bf16[32]{0} broadcast(bf16[] %reshape.6122), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.6124 = bf16[32]{0} multiply(bf16[32]{0} %get-tuple-element.4069, bf16[32]{0} %broadcast.6123), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6141 = bf16[32]{0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6142 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6124, bf16[32]{0} %broadcast.6141), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.6146 = bf16[32]{0} add(bf16[32]{0} %multiply.6145, bf16[32]{0} %multiply.6142), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p160.6125 = bf16[32]{0} parameter(160), frontend_attributes={neff_input_names="input160"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6126 = bf16[32]{0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6127 = bf16[32]{0} multiply(bf16[32]{0} %p160.6125, bf16[32]{0} %broadcast.6126), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6129 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6124, bf16[32]{0} %multiply.6124), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.6128 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6130 = bf16[32]{0} broadcast(bf16[] %convert.6128), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6131 = bf16[32]{0} multiply(bf16[32]{0} %multiply.6129, bf16[32]{0} %broadcast.6130), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.6132 = bf16[32]{0} add(bf16[32]{0} %multiply.6127, bf16[32]{0} %multiply.6131), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.6133 = bf16[32]{0} sqrt(bf16[32]{0} %add.6132), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6134 = bf16[32]{0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6135 = bf16[32]{0} divide(bf16[32]{0} %sqrt.6133, bf16[32]{0} %broadcast.6134), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6136 = bf16[32]{0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.6137 = bf16[32]{0} add(bf16[32]{0} %divide.6135, bf16[32]{0} %broadcast.6136), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6160 = bf16[32]{0} divide(bf16[32]{0} %add.6146, bf16[32]{0} %add.6137), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.6159 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.6161 = bf16[32]{0} broadcast(bf16[] %convert.6159), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.6162 = bf16[32]{0} multiply(bf16[32]{0} %divide.6160, bf16[32]{0} %broadcast.6161), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.6163 = bf16[32]{0} add(bf16[32]{0} %subtract.6158, bf16[32]{0} %multiply.6162), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.6167 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.6104), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.6168 = (bf16[32]{0}, bf16[]) all-gather(bf16[32]{0} %add.6163, bf16[] %get-tuple-element.6167), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.6169 = bf16[32]{0} get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.6168), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %p165.6217 = bf16[32000,32]{1,0} parameter(165), frontend_attributes={neff_input_names="input165"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6218 = bf16[32000,32]{1,0} broadcast(bf16[] %p86.4535), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6219 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %p165.6217, bf16[32000,32]{1,0} %broadcast.6218), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %broadcast.6220 = bf16[32000,32]{1,0} broadcast(bf16[] %p85.4534), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %multiply.6222 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.6219, bf16[32000,32]{1,0} %broadcast.6220), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %subtract.6223 = bf16[32000,32]{1,0} subtract(bf16[32000,32]{1,0} %p165.6217, bf16[32000,32]{1,0} %multiply.6222), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=58}
  %p164.6208 = bf16[32000,32]{1,0} parameter(164), frontend_attributes={neff_input_names="input164"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %broadcast.6209 = bf16[32000,32]{1,0} broadcast(bf16[] %p83.4524), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6210 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %p164.6208, bf16[32000,32]{1,0} %broadcast.6209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %constant.256 = bf16[1]{0} constant({1})
  %compare.6182 = pred[1]{0} compare(bf16[1]{0} %divide.4489, bf16[1]{0} %constant.256), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %constant.257 = bf16[1]{0} constant({1})
  %select.6184 = bf16[1]{0} select(pred[1]{0} %compare.6182, bf16[1]{0} %divide.4489, bf16[1]{0} %constant.257), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %reshape.6186 = bf16[] reshape(bf16[1]{0} %select.6184), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6188 = bf16[32000,32]{1,0} broadcast(bf16[] %reshape.6186), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %multiply.6189 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %get-tuple-element.1532, bf16[32000,32]{1,0} %broadcast.6188), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/grads.py" source_line=189}
  %broadcast.6206 = bf16[32000,32]{1,0} broadcast(bf16[] %p82.4518), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %multiply.6207 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.6189, bf16[32000,32]{1,0} %broadcast.6206), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %add.6211 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %multiply.6210, bf16[32000,32]{1,0} %multiply.6207), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=61}
  %p163.6190 = bf16[32000,32]{1,0} parameter(163), frontend_attributes={neff_input_names="input163"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6191 = bf16[32000,32]{1,0} broadcast(bf16[] %p80.4504), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6192 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %p163.6190, bf16[32000,32]{1,0} %broadcast.6191), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6194 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.6189, bf16[32000,32]{1,0} %multiply.6189), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %convert.6193 = bf16[] convert(f32[] %p39.1252), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %broadcast.6195 = bf16[32000,32]{1,0} broadcast(bf16[] %convert.6193), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %multiply.6196 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %multiply.6194, bf16[32000,32]{1,0} %broadcast.6195), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %add.6197 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %multiply.6192, bf16[32000,32]{1,0} %multiply.6196), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=62}
  %sqrt.6198 = bf16[32000,32]{1,0} sqrt(bf16[32000,32]{1,0} %add.6197), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6199 = bf16[32000,32]{1,0} broadcast(bf16[] %p38.1251), dimensions={}, metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6200 = bf16[32000,32]{1,0} divide(bf16[32000,32]{1,0} %sqrt.6198, bf16[32000,32]{1,0} %broadcast.6199), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %broadcast.6201 = bf16[32000,32]{1,0} broadcast(bf16[] %p37.1249), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %add.6202 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %divide.6200, bf16[32000,32]{1,0} %broadcast.6201), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=117}
  %divide.6225 = bf16[32000,32]{1,0} divide(bf16[32000,32]{1,0} %add.6211, bf16[32000,32]{1,0} %add.6202), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %convert.6224 = bf16[] convert(f32[] %p36.1247), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %broadcast.6226 = bf16[32000,32]{1,0} broadcast(bf16[] %convert.6224), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %multiply.6227 = bf16[32000,32]{1,0} multiply(bf16[32000,32]{1,0} %divide.6225, bf16[32000,32]{1,0} %broadcast.6226), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %add.6228 = bf16[32000,32]{1,0} add(bf16[32000,32]{1,0} %subtract.6223, bf16[32000,32]{1,0} %multiply.6227), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/torch_neuronx/optim/adamw.py" source_line=119}
  %get-tuple-element.6232 = bf16[] get-tuple-element((bf16[32]{0}, bf16[]) %all-gather.6168), index=1, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %all-gather.6233 = (bf16[32000,32]{1,0}, bf16[]) all-gather(bf16[32000,32]{1,0} %add.6228, bf16[] %get-tuple-element.6232), replica_groups={{0}}, dimensions={0}, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %get-tuple-element.6234 = bf16[32000,32]{1,0} get-tuple-element((bf16[32000,32]{1,0}, bf16[]) %all-gather.6233), index=0, metadata={op_type="xla__all_gather" op_name="xla__all_gather" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=585}
  %constant.6238 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6242 = bf16[32000,32]{1,0} broadcast(bf16[] %constant.6238), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6243 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6247 = bf16[32]{0} broadcast(bf16[] %constant.6243), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6248 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6252 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6248), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6253 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6257 = bf16[64,32]{1,0} broadcast(bf16[] %constant.6253), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6258 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6262 = bf16[32]{0} broadcast(bf16[] %constant.6258), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6263 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6267 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6263), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6268 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6272 = bf16[96,32]{1,0} broadcast(bf16[] %constant.6268), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6273 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6277 = bf16[32]{0} broadcast(bf16[] %constant.6273), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6278 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6282 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6278), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6283 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6287 = bf16[64,32]{1,0} broadcast(bf16[] %constant.6283), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6288 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6292 = bf16[32]{0} broadcast(bf16[] %constant.6288), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6293 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6297 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6293), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6298 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6302 = bf16[96,32]{1,0} broadcast(bf16[] %constant.6298), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6303 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6307 = bf16[32]{0} broadcast(bf16[] %constant.6303), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6308 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6312 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6308), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6313 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6317 = bf16[64,32]{1,0} broadcast(bf16[] %constant.6313), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6318 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6322 = bf16[32]{0} broadcast(bf16[] %constant.6318), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6323 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6327 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6323), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6328 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6332 = bf16[96,32]{1,0} broadcast(bf16[] %constant.6328), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6333 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6337 = bf16[32]{0} broadcast(bf16[] %constant.6333), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6338 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6342 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6338), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6343 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6347 = bf16[64,32]{1,0} broadcast(bf16[] %constant.6343), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6348 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6352 = bf16[32]{0} broadcast(bf16[] %constant.6348), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6353 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6357 = bf16[32,32]{1,0} broadcast(bf16[] %constant.6353), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6358 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6362 = bf16[96,32]{1,0} broadcast(bf16[] %constant.6358), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6363 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6367 = bf16[32]{0} broadcast(bf16[] %constant.6363), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %constant.6368 = bf16[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %broadcast.6372 = bf16[32000,32]{1,0} broadcast(bf16[] %constant.6368), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/pytorch/torch/optim/optimizer.py" source_line=815}
  %p166.6373 = s64[1024,4096]{1,0} parameter(166), frontend_attributes={neff_input_names="input166"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="run_llama_nxd.py" source_line=272}
  %log.6409 = f32[4193280]{0} log(f32[4193280]{0} %reduce.1459), metadata={op_type="aten__log" op_name="aten__log" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=74}
  %slice.6403 = f32[1024,4095,32000]{2,1,0} slice(f32[1024,4096,32000]{2,1,0} %select.1448), slice={[0:1024], [0:4095], [0:32000]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %reshape.6404 = f32[4193280,32000]{1,0} reshape(f32[1024,4095,32000]{2,1,0} %slice.6403), metadata={op_type="aten__view" op_name="aten__view" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %iota.30 = s64[4193280,1]{1,0} iota(), iota_dimension=0, metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=46}
  %constant.6385 = s64[] constant(0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %broadcast.6386 = s64[4193280]{0} broadcast(s64[] %constant.6385), dimensions={}, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %compare.6387 = pred[4193280]{0} compare(s64[4193280]{0} %select, s64[4193280]{0} %broadcast.6386), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %broadcast.6383 = s64[4193280]{0} broadcast(s64[] %p45.1348), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %add.6384 = s64[4193280]{0} add(s64[4193280]{0} %select, s64[4193280]{0} %broadcast.6383), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %select.6388 = s64[4193280]{0} select(pred[4193280]{0} %compare.6387, s64[4193280]{0} %add.6384, s64[4193280]{0} %select), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %reshape.6400 = s64[4193280,1]{1,0} reshape(s64[4193280]{0} %select.6388), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %concatenate.6401 = s64[4193280,2]{1,0} concatenate(s64[4193280,1]{1,0} %iota.30, s64[4193280,1]{1,0} %reshape.6400), dimensions={1}, metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %gather.6405 = f32[4193280]{0} gather(f32[4193280,32000]{1,0} %reshape.6404, s64[4193280,2]{1,0} %concatenate.6401), offset_dims={}, collapsed_slice_dims={0,1}, start_index_map={0,1}, index_vector_dim=1, slice_sizes={1,1}, metadata={op_type="aten__index" op_name="aten__index" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=47}
  %constant.265 = f32[] constant(0)
  %broadcast.677 = f32[4193280]{0} broadcast(f32[] %constant.265), dimensions={}
  %select.17 = f32[4193280]{0} select(pred[4193280]{0} %convert.1360, f32[4193280]{0} %gather.6405, f32[4193280]{0} %broadcast.677), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=53}
  %subtract.6410 = f32[4193280]{0} subtract(f32[4193280]{0} %log.6409, f32[4193280]{0} %select.17), metadata={op_type="aten__sub" op_name="aten__sub" source_file="/home/ubuntu/kahfi/xla-explore-env/lib/python3.8/site-packages/neuronx_distributed/parallel_layers/loss_functions.py" source_line=74}
  %constant.6411 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=702}
  %reduce.6417 = f32[] reduce(f32[4193280]{0} %subtract.6410, f32[] %constant.6411), dimensions={0}, to_apply=%AddComputation.6413, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=702}
  %constant.264 = f32[] constant(2.38476815e-07)
  %multiply.6425 = f32[] multiply(f32[] %reduce.6417, f32[] %constant.264), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/home/ubuntu/kahfi/neuronx_distributed_examples/tp_pp_llama2_7b_hf_pretrain/modeling_llama_nxd.py" source_line=702}
  ROOT %tuple.6427 = (bf16[32000,32]{1,0}, bf16[1,1,4096,2]{3,2,1,0}, bf16[1,1,4096,2]{3,2,1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, /*index=5*/bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, bf16[1,1,4096,2]{3,2,1,0}, /*index=10*/bf16[1,1,4096,2]{3,2,1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, /*index=15*/bf16[32]{0}, bf16[32]{0}, bf16[1,1,4096,2]{3,2,1,0}, bf16[1,1,4096,2]{3,2,1,0}, bf16[96,32]{1,0}, /*index=20*/bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, /*index=25*/bf16[1,1,4096,2]{3,2,1,0}, bf16[1,1,4096,2]{3,2,1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, /*index=30*/bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, bf16[32000,32]{1,0}, /*index=35*/bf16[32000,32]{1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, /*index=40*/bf16[32]{0}, bf16[32]{0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, /*index=45*/bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, /*index=50*/bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, bf16[96,32]{1,0}, /*index=55*/bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, /*index=60*/bf16[32]{0}, bf16[32000,32]{1,0}, bf16[32000,32]{1,0}, bf16[32]{0}, bf16[32,32]{1,0}, /*index=65*/bf16[64,32]{1,0}, bf16[32]{0}, bf16[32,32]{1,0}, bf16[96,32]{1,0}, bf16[32]{0}, /*index=70*/bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32]{0}, bf16[32,32]{1,0}, bf16[96,32]{1,0}, /*index=75*/bf16[32]{0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32]{0}, bf16[32,32]{1,0}, /*index=80*/bf16[96,32]{1,0}, bf16[32]{0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[32]{0}, /*index=85*/bf16[32,32]{1,0}, bf16[96,32]{1,0}, bf16[32]{0}, bf16[32000,32]{1,0}, bf16[32000,32]{1,0}, /*index=90*/bf16[32000,32]{1,0}, bf16[96,32]{1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, bf16[32,32]{1,0}, /*index=95*/bf16[64,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32,32]{1,0}, bf16[32]{0}, /*index=100*/bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, bf16[96,32]{1,0}, bf16[96,32]{1,0}, /*index=105*/bf16[32,32]{1,0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, /*index=110*/bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, /*index=115*/bf16[96,32]{1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, bf16[32,32]{1,0}, bf16[64,32]{1,0}, /*index=120*/bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32,32]{1,0}, bf16[32]{0}, bf16[32]{0}, /*index=125*/bf16[32]{0}, bf16[32]{0}, bf16[96,32]{1,0}, bf16[96,32]{1,0}, bf16[32,32]{1,0}, /*index=130*/bf16[32,32]{1,0}, bf16[64,32]{1,0}, bf16[64,32]{1,0}, bf16[32,32]{1,0}, bf16[32,32]{1,0}, /*index=135*/bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, bf16[32]{0}, /*index=140*/bf16[32]{0}, bf16[32000,32]{1,0}, bf16[32000,32]{1,0}, s64[1024,4096]{1,0}, s64[1024,4096]{1,0}, /*index=145*/s64[1024,4096]{1,0}, f32[], bf16[1]{0}) tuple(bf16[32000,32]{1,0} %get-tuple-element.4553, bf16[1,1,4096,2]{3,2,1,0} %p10.159, bf16[1,1,4096,2]{3,2,1,0} %p9.121, bf16[96,32]{1,0} %get-tuple-element.4618, bf16[32,32]{1,0} %get-tuple-element.4683, /*index=5*/bf16[64,32]{1,0} %get-tuple-element.4748, bf16[32,32]{1,0} %get-tuple-element.4813, bf16[32]{0} %get-tuple-element.4877, bf16[32]{0} %get-tuple-element.4941, bf16[1,1,4096,2]{3,2,1,0} %p18.457, /*index=10*/bf16[1,1,4096,2]{3,2,1,0} %p17.419, bf16[96,32]{1,0} %get-tuple-element.5006, bf16[32,32]{1,0} %get-tuple-element.5071, bf16[64,32]{1,0} %get-tuple-element.5136, bf16[32,32]{1,0} %get-tuple-element.5201, /*index=15*/bf16[32]{0} %get-tuple-element.5265, bf16[32]{0} %get-tuple-element.5329, bf16[1,1,4096,2]{3,2,1,0} %p26.755, bf16[1,1,4096,2]{3,2,1,0} %p25.717, bf16[96,32]{1,0} %get-tuple-element.5394, /*index=20*/bf16[32,32]{1,0} %get-tuple-element.5459, bf16[64,32]{1,0} %get-tuple-element.5524, bf16[32,32]{1,0} %get-tuple-element.5589, bf16[32]{0} %get-tuple-element.5653, bf16[32]{0} %get-tuple-element.5717, /*index=25*/bf16[1,1,4096,2]{3,2,1,0} %p34.1053, bf16[1,1,4096,2]{3,2,1,0} %p33.1015, bf16[96,32]{1,0} %get-tuple-element.5782, bf16[32,32]{1,0} %get-tuple-element.5847, bf16[64,32]{1,0} %get-tuple-element.5912, /*index=30*/bf16[32,32]{1,0} %get-tuple-element.5977, bf16[32]{0} %get-tuple-element.6041, bf16[32]{0} %get-tuple-element.6105, bf16[32]{0} %get-tuple-element.6169, bf16[32000,32]{1,0} %get-tuple-element.6234, /*index=35*/bf16[32000,32]{1,0} %add.4547, bf16[96,32]{1,0} %add.4612, bf16[32,32]{1,0} %add.4677, bf16[64,32]{1,0} %add.4742, bf16[32,32]{1,0} %add.4807, /*index=40*/bf16[32]{0} %add.4871, bf16[32]{0} %add.4935, bf16[96,32]{1,0} %add.5000, bf16[32,32]{1,0} %add.5065, bf16[64,32]{1,0} %add.5130, /*index=45*/bf16[32,32]{1,0} %add.5195, bf16[32]{0} %add.5259, bf16[32]{0} %add.5323, bf16[96,32]{1,0} %add.5388, bf16[32,32]{1,0} %add.5453, /*index=50*/bf16[64,32]{1,0} %add.5518, bf16[32,32]{1,0} %add.5583, bf16[32]{0} %add.5647, bf16[32]{0} %add.5711, bf16[96,32]{1,0} %add.5776, /*index=55*/bf16[32,32]{1,0} %add.5841, bf16[64,32]{1,0} %add.5906, bf16[32,32]{1,0} %add.5971, bf16[32]{0} %add.6035, bf16[32]{0} %add.6099, /*index=60*/bf16[32]{0} %add.6163, bf16[32000,32]{1,0} %add.6228, bf16[32000,32]{1,0} %broadcast.6242, bf16[32]{0} %broadcast.6247, bf16[32,32]{1,0} %broadcast.6252, /*index=65*/bf16[64,32]{1,0} %broadcast.6257, bf16[32]{0} %broadcast.6262, bf16[32,32]{1,0} %broadcast.6267, bf16[96,32]{1,0} %broadcast.6272, bf16[32]{0} %broadcast.6277, /*index=70*/bf16[32,32]{1,0} %broadcast.6282, bf16[64,32]{1,0} %broadcast.6287, bf16[32]{0} %broadcast.6292, bf16[32,32]{1,0} %broadcast.6297, bf16[96,32]{1,0} %broadcast.6302, /*index=75*/bf16[32]{0} %broadcast.6307, bf16[32,32]{1,0} %broadcast.6312, bf16[64,32]{1,0} %broadcast.6317, bf16[32]{0} %broadcast.6322, bf16[32,32]{1,0} %broadcast.6327, /*index=80*/bf16[96,32]{1,0} %broadcast.6332, bf16[32]{0} %broadcast.6337, bf16[32,32]{1,0} %broadcast.6342, bf16[64,32]{1,0} %broadcast.6347, bf16[32]{0} %broadcast.6352, /*index=85*/bf16[32,32]{1,0} %broadcast.6357, bf16[96,32]{1,0} %broadcast.6362, bf16[32]{0} %broadcast.6367, bf16[32000,32]{1,0} %broadcast.6372, bf16[32000,32]{1,0} %add.4528, /*index=90*/bf16[32000,32]{1,0} %add.4512, bf16[96,32]{1,0} %add.4595, bf16[96,32]{1,0} %add.4581, bf16[32,32]{1,0} %add.4660, bf16[32,32]{1,0} %add.4646, /*index=95*/bf16[64,32]{1,0} %add.4725, bf16[64,32]{1,0} %add.4711, bf16[32,32]{1,0} %add.4790, bf16[32,32]{1,0} %add.4776, bf16[32]{0} %add.4854, /*index=100*/bf16[32]{0} %add.4840, bf16[32]{0} %add.4918, bf16[32]{0} %add.4904, bf16[96,32]{1,0} %add.4983, bf16[96,32]{1,0} %add.4969, /*index=105*/bf16[32,32]{1,0} %add.5048, bf16[32,32]{1,0} %add.5034, bf16[64,32]{1,0} %add.5113, bf16[64,32]{1,0} %add.5099, bf16[32,32]{1,0} %add.5178, /*index=110*/bf16[32,32]{1,0} %add.5164, bf16[32]{0} %add.5242, bf16[32]{0} %add.5228, bf16[32]{0} %add.5306, bf16[32]{0} %add.5292, /*index=115*/bf16[96,32]{1,0} %add.5371, bf16[96,32]{1,0} %add.5357, bf16[32,32]{1,0} %add.5436, bf16[32,32]{1,0} %add.5422, bf16[64,32]{1,0} %add.5501, /*index=120*/bf16[64,32]{1,0} %add.5487, bf16[32,32]{1,0} %add.5566, bf16[32,32]{1,0} %add.5552, bf16[32]{0} %add.5630, bf16[32]{0} %add.5616, /*index=125*/bf16[32]{0} %add.5694, bf16[32]{0} %add.5680, bf16[96,32]{1,0} %add.5759, bf16[96,32]{1,0} %add.5745, bf16[32,32]{1,0} %add.5824, /*index=130*/bf16[32,32]{1,0} %add.5810, bf16[64,32]{1,0} %add.5889, bf16[64,32]{1,0} %add.5875, bf16[32,32]{1,0} %add.5954, bf16[32,32]{1,0} %add.5940, /*index=135*/bf16[32]{0} %add.6018, bf16[32]{0} %add.6004, bf16[32]{0} %add.6082, bf16[32]{0} %add.6068, bf16[32]{0} %add.6146, /*index=140*/bf16[32]{0} %add.6132, bf16[32000,32]{1,0} %add.6211, bf16[32000,32]{1,0} %add.6197, s64[1024,4096]{1,0} %p2.10, s64[1024,4096]{1,0} %p166.6373, /*index=145*/s64[1024,4096]{1,0} %p46.1349, f32[] %multiply.6425, bf16[1]{0} %power.4484), frontend_attributes={neff_output_names="output0,output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12,output13,output14,output15,output16,output17,output18,output19,output20,output21,output22,output23,output24,output25,output26,output27,output28,output29,output30,output31,output32,output33,output34,output35,output36,output37,output38,output39,output40,output41,output42,output43,output44,output45,output46,output47,output48,output49,output50,output51,output52,output53,output54,output55,output56,output57,output58,output59,output60,output61,output62,output63,output64,output65,output66,output67,output68,output69,output70,output71,output72,output73,output74,output75,output76,output77,output78,output79,output80,output81,output82,output83,output84,output85,output86,output87,output88,output89,output90,output91,output92,output93,output94,output95,output96,output97,output98,output99,output100,output101,output102,output103,output104,output105,output106,output107,output108,output109,output110,output111,output112,output113,output114,output115,output116,output117,output118,output119,output120,output121,output122,output123,output124,output125,output126,output127,output128,output129,output130,output131,output132,output133,output134,output135,output136,output137,output138,output139,output140,output141,output142,output143,output144,output145,output146,output147"}
}


`

export default text;
