const text = `
HloModule SyncTensorsGraph.1450, input_output_alias={ {0}: (42, {}, must-alias), {1}: (45, {}, must-alias), {2}: (48, {}, must-alias), {3}: (51, {}, must-alias), {4}: (54, {}, must-alias), {5}: (57, {}, must-alias), {6}: (60, {}, must-alias), {7}: (63, {}, must-alias), {8}: (66, {}, must-alias), {9}: (69, {}, must-alias), {10}: (72, {}, must-alias), {11}: (75, {}, must-alias), {12}: (78, {}, must-alias), {13}: (81, {}, must-alias), {14}: (84, {}, must-alias), {15}: (87, {}, must-alias), {16}: (90, {}, must-alias), {17}: (93, {}, must-alias), {18}: (96, {}, must-alias), {19}: (99, {}, must-alias), {20}: (102, {}, must-alias), {21}: (105, {}, must-alias), {22}: (108, {}, must-alias), {23}: (111, {}, must-alias), {24}: (114, {}, must-alias), {25}: (117, {}, must-alias), {26}: (120, {}, must-alias), {27}: (123, {}, must-alias), {28}: (126, {}, must-alias), {29}: (129, {}, must-alias), {30}: (6, {}, must-alias), {31}: (41, {}, must-alias), {32}: (38, {}, must-alias), {33}: (44, {}, must-alias), {34}: (43, {}, must-alias), {35}: (47, {}, must-alias), {36}: (46, {}, must-alias), {37}: (56, {}, must-alias), {38}: (55, {}, must-alias), {39}: (62, {}, must-alias), {40}: (61, {}, must-alias), {41}: (68, {}, must-alias), {42}: (67, {}, must-alias), {43}: (74, {}, must-alias), {44}: (73, {}, must-alias), {45}: (86, {}, must-alias), {46}: (85, {}, must-alias), {47}: (92, {}, must-alias), {48}: (91, {}, must-alias), {49}: (104, {}, must-alias), {50}: (103, {}, must-alias), {51}: (110, {}, must-alias), {52}: (109, {}, must-alias), {53}: (125, {}, must-alias), {54}: (124, {}, must-alias), {55}: (50, {}, must-alias), {56}: (49, {}, must-alias), {57}: (53, {}, must-alias), {58}: (52, {}, must-alias), {59}: (59, {}, must-alias), {60}: (58, {}, must-alias), {61}: (65, {}, must-alias), {62}: (64, {}, must-alias), {63}: (71, {}, must-alias), {64}: (70, {}, must-alias), {65}: (77, {}, must-alias), {66}: (76, {}, must-alias), {67}: (80, {}, must-alias), {68}: (79, {}, must-alias), {69}: (83, {}, must-alias), {70}: (82, {}, must-alias), {71}: (89, {}, must-alias), {72}: (88, {}, must-alias), {73}: (95, {}, must-alias), {74}: (94, {}, must-alias), {75}: (98, {}, must-alias), {76}: (97, {}, must-alias), {77}: (101, {}, must-alias), {78}: (100, {}, must-alias), {79}: (107, {}, must-alias), {80}: (106, {}, must-alias), {81}: (122, {}, must-alias), {82}: (121, {}, must-alias), {83}: (113, {}, must-alias), {84}: (112, {}, must-alias), {85}: (116, {}, must-alias), {86}: (115, {}, must-alias), {87}: (119, {}, must-alias), {88}: (118, {}, must-alias), {89}: (128, {}, must-alias), {90}: (127, {}, must-alias) }

%AddComputation.22 (x.23: f32[], y.24: f32[]) -> f32[] {
  %x.23 = f32[] parameter(0)
  %y.24 = f32[] parameter(1)
  ROOT %add.25 = f32[] add(f32[] %x.23, f32[] %y.24)
}

%AddComputation.91 (x.92: f32[], y.93: f32[]) -> f32[] {
  %x.92 = f32[] parameter(0)
  %y.93 = f32[] parameter(1)
  ROOT %add.94 = f32[] add(f32[] %x.92, f32[] %y.93)
}

%AddComputation.220 (x.221: f32[], y.222: f32[]) -> f32[] {
  %x.221 = f32[] parameter(0)
  %y.222 = f32[] parameter(1)
  ROOT %add.223 = f32[] add(f32[] %x.221, f32[] %y.222)
}

%AddComputation.229 (x.230: f32[], y.231: f32[]) -> f32[] {
  %x.230 = f32[] parameter(0)
  %y.231 = f32[] parameter(1)
  ROOT %add.232 = f32[] add(f32[] %x.230, f32[] %y.231)
}

%AddComputation.238 (x.239: f32[], y.240: f32[]) -> f32[] {
  %x.239 = f32[] parameter(0)
  %y.240 = f32[] parameter(1)
  ROOT %add.241 = f32[] add(f32[] %x.239, f32[] %y.240)
}

%AddComputation.247 (x.248: f32[], y.249: f32[]) -> f32[] {
  %x.248 = f32[] parameter(0)
  %y.249 = f32[] parameter(1)
  ROOT %add.250 = f32[] add(f32[] %x.248, f32[] %y.249)
}

%AddComputation.256 (x.257: f32[], y.258: f32[]) -> f32[] {
  %x.257 = f32[] parameter(0)
  %y.258 = f32[] parameter(1)
  ROOT %add.259 = f32[] add(f32[] %x.257, f32[] %y.258)
}

%AddComputation.265 (x.266: f32[], y.267: f32[]) -> f32[] {
  %x.266 = f32[] parameter(0)
  %y.267 = f32[] parameter(1)
  ROOT %add.268 = f32[] add(f32[] %x.266, f32[] %y.267)
}

%AddComputation.274 (x.275: f32[], y.276: f32[]) -> f32[] {
  %x.275 = f32[] parameter(0)
  %y.276 = f32[] parameter(1)
  ROOT %add.277 = f32[] add(f32[] %x.275, f32[] %y.276)
}

%AddComputation.283 (x.284: f32[], y.285: f32[]) -> f32[] {
  %x.284 = f32[] parameter(0)
  %y.285 = f32[] parameter(1)
  ROOT %add.286 = f32[] add(f32[] %x.284, f32[] %y.285)
}

%AddComputation.292 (x.293: f32[], y.294: f32[]) -> f32[] {
  %x.293 = f32[] parameter(0)
  %y.294 = f32[] parameter(1)
  ROOT %add.295 = f32[] add(f32[] %x.293, f32[] %y.294)
}

%AddComputation.301 (x.302: f32[], y.303: f32[]) -> f32[] {
  %x.302 = f32[] parameter(0)
  %y.303 = f32[] parameter(1)
  ROOT %add.304 = f32[] add(f32[] %x.302, f32[] %y.303)
}

%AddComputation.310 (x.311: f32[], y.312: f32[]) -> f32[] {
  %x.311 = f32[] parameter(0)
  %y.312 = f32[] parameter(1)
  ROOT %add.313 = f32[] add(f32[] %x.311, f32[] %y.312)
}

%AddComputation.319 (x.320: f32[], y.321: f32[]) -> f32[] {
  %x.320 = f32[] parameter(0)
  %y.321 = f32[] parameter(1)
  ROOT %add.322 = f32[] add(f32[] %x.320, f32[] %y.321)
}

%AddComputation.328 (x.329: f32[], y.330: f32[]) -> f32[] {
  %x.329 = f32[] parameter(0)
  %y.330 = f32[] parameter(1)
  ROOT %add.331 = f32[] add(f32[] %x.329, f32[] %y.330)
}

%AddComputation.337 (x.338: f32[], y.339: f32[]) -> f32[] {
  %x.338 = f32[] parameter(0)
  %y.339 = f32[] parameter(1)
  ROOT %add.340 = f32[] add(f32[] %x.338, f32[] %y.339)
}

%AddComputation.346 (x.347: f32[], y.348: f32[]) -> f32[] {
  %x.347 = f32[] parameter(0)
  %y.348 = f32[] parameter(1)
  ROOT %add.349 = f32[] add(f32[] %x.347, f32[] %y.348)
}

%AddComputation.355 (x.356: f32[], y.357: f32[]) -> f32[] {
  %x.356 = f32[] parameter(0)
  %y.357 = f32[] parameter(1)
  ROOT %add.358 = f32[] add(f32[] %x.356, f32[] %y.357)
}

%AddComputation.364 (x.365: f32[], y.366: f32[]) -> f32[] {
  %x.365 = f32[] parameter(0)
  %y.366 = f32[] parameter(1)
  ROOT %add.367 = f32[] add(f32[] %x.365, f32[] %y.366)
}

%AddComputation.373 (x.374: f32[], y.375: f32[]) -> f32[] {
  %x.374 = f32[] parameter(0)
  %y.375 = f32[] parameter(1)
  ROOT %add.376 = f32[] add(f32[] %x.374, f32[] %y.375)
}

%AddComputation.382 (x.383: f32[], y.384: f32[]) -> f32[] {
  %x.383 = f32[] parameter(0)
  %y.384 = f32[] parameter(1)
  ROOT %add.385 = f32[] add(f32[] %x.383, f32[] %y.384)
}

%AddComputation.391 (x.392: f32[], y.393: f32[]) -> f32[] {
  %x.392 = f32[] parameter(0)
  %y.393 = f32[] parameter(1)
  ROOT %add.394 = f32[] add(f32[] %x.392, f32[] %y.393)
}

%AddComputation.400 (x.401: f32[], y.402: f32[]) -> f32[] {
  %x.401 = f32[] parameter(0)
  %y.402 = f32[] parameter(1)
  ROOT %add.403 = f32[] add(f32[] %x.401, f32[] %y.402)
}

%AddComputation.409 (x.410: f32[], y.411: f32[]) -> f32[] {
  %x.410 = f32[] parameter(0)
  %y.411 = f32[] parameter(1)
  ROOT %add.412 = f32[] add(f32[] %x.410, f32[] %y.411)
}

%AddComputation.418 (x.419: f32[], y.420: f32[]) -> f32[] {
  %x.419 = f32[] parameter(0)
  %y.420 = f32[] parameter(1)
  ROOT %add.421 = f32[] add(f32[] %x.419, f32[] %y.420)
}

%AddComputation.427 (x.428: f32[], y.429: f32[]) -> f32[] {
  %x.428 = f32[] parameter(0)
  %y.429 = f32[] parameter(1)
  ROOT %add.430 = f32[] add(f32[] %x.428, f32[] %y.429)
}

%AddComputation.436 (x.437: f32[], y.438: f32[]) -> f32[] {
  %x.437 = f32[] parameter(0)
  %y.438 = f32[] parameter(1)
  ROOT %add.439 = f32[] add(f32[] %x.437, f32[] %y.438)
}

%AddComputation.445 (x.446: f32[], y.447: f32[]) -> f32[] {
  %x.446 = f32[] parameter(0)
  %y.447 = f32[] parameter(1)
  ROOT %add.448 = f32[] add(f32[] %x.446, f32[] %y.447)
}

%AddComputation.454 (x.455: f32[], y.456: f32[]) -> f32[] {
  %x.455 = f32[] parameter(0)
  %y.456 = f32[] parameter(1)
  ROOT %add.457 = f32[] add(f32[] %x.455, f32[] %y.456)
}

%AddComputation.463 (x.464: f32[], y.465: f32[]) -> f32[] {
  %x.464 = f32[] parameter(0)
  %y.465 = f32[] parameter(1)
  ROOT %add.466 = f32[] add(f32[] %x.464, f32[] %y.465)
}

%AddComputation.472 (x.473: f32[], y.474: f32[]) -> f32[] {
  %x.473 = f32[] parameter(0)
  %y.474 = f32[] parameter(1)
  ROOT %add.475 = f32[] add(f32[] %x.473, f32[] %y.474)
}

%AddComputation.481 (x.482: f32[], y.483: f32[]) -> f32[] {
  %x.482 = f32[] parameter(0)
  %y.483 = f32[] parameter(1)
  ROOT %add.484 = f32[] add(f32[] %x.482, f32[] %y.483)
}

%AddComputation.521 (x.522: f32[], y.523: f32[]) -> f32[] {
  %x.522 = f32[] parameter(0)
  %y.523 = f32[] parameter(1)
  ROOT %add.524 = f32[] add(f32[] %x.522, f32[] %y.523)
}

ENTRY %SyncTensorsGraph.1450 (p0.1: f32[], p1.6: f32[], p2.8: f32[], p3.10: f32[], p4.14: f32[], p5.15: f32[], p6.16: f32[1], p7.29: f32[30522,16], p8.30: f32[512,16], p9.31: f32[2,16], p10.32: f32[16,16], p11.33: f32[16,16], p12.34: f32[16,16], p13.35: f32[16,16], p14.36: f32[4096,16], p15.37: f32[16,4096], p16.38: f32[16,16], p17.39: f32[16,16], p18.40: f32[2,16], p19.41: f32[16], p20.42: f32[16], p21.43: f32[16], p22.44: f32[16], p23.45: f32[16], p24.46: f32[16], p25.47: f32[16], p26.48: f32[16], p27.49: f32[4096], p28.50: f32[16], p29.51: f32[16], p30.52: f32[16], p31.53: f32[16], p32.54: f32[30522], p33.55: f32[16], p34.56: f32[16], p35.57: f32[16], p36.58: f32[2], p37.536: f32[], p38.537: f32[30522,16], p39.547: f32[], p40.553: f32[], p41.554: f32[30522,16], p42.558: f32[30522,16], p43.573: f32[512,16], p44.588: f32[512,16], p45.592: f32[512,16], p46.607: f32[2,16], p47.622: f32[2,16], p48.626: f32[2,16], p49.637: f32[16], p50.652: f32[16], p51.656: f32[16], p52.665: f32[16], p53.680: f32[16], p54.684: f32[16], p55.697: f32[16,16], p56.712: f32[16,16], p57.716: f32[16,16], p58.727: f32[16], p59.742: f32[16], p60.746: f32[16], p61.759: f32[16,16], p62.774: f32[16,16], p63.778: f32[16,16], p64.789: f32[16], p65.804: f32[16], p66.808: f32[16], p67.821: f32[16,16], p68.836: f32[16,16], p69.840: f32[16,16], p70.851: f32[16], p71.866: f32[16], p72.870: f32[16], p73.883: f32[16,16], p74.898: f32[16,16], p75.902: f32[16,16], p76.913: f32[16], p77.928: f32[16], p78.932: f32[16], p79.941: f32[16], p80.956: f32[16], p81.960: f32[16], p82.969: f32[16], p83.984: f32[16], p84.988: f32[16], p85.1001: f32[4096,16], p86.1016: f32[4096,16], p87.1020: f32[4096,16], p88.1031: f32[4096], p89.1046: f32[4096], p90.1050: f32[4096], p91.1063: f32[16,4096], p92.1078: f32[16,4096], p93.1082: f32[16,4096], p94.1093: f32[16], p95.1108: f32[16], p96.1112: f32[16], p97.1121: f32[16], p98.1136: f32[16], p99.1140: f32[16], p100.1149: f32[16], p101.1164: f32[16], p102.1168: f32[16], p103.1181: f32[16,16], p104.1196: f32[16,16], p105.1200: f32[16,16], p106.1211: f32[16], p107.1226: f32[16], p108.1230: f32[16], p109.1243: f32[16,16], p110.1258: f32[16,16], p111.1262: f32[16,16], p112.1273: f32[16], p113.1288: f32[16], p114.1292: f32[16], p115.1301: f32[16], p116.1316: f32[16], p117.1320: f32[16], p118.1329: f32[16], p119.1344: f32[16], p120.1348: f32[16], p121.1357: f32[30522], p122.1372: f32[30522], p123.1376: f32[30522], p124.1389: f32[2,16], p125.1404: f32[2,16], p126.1408: f32[2,16], p127.1419: f32[2], p128.1434: f32[2], p129.1438: f32[2]) -> (f32[30522,16], f32[512,16], f32[2,16], f32[16], f32[16], /*index=5*/f32[16,16], f32[16], f32[16,16], f32[16], f32[16,16], /*index=10*/f32[16], f32[16,16], f32[16], f32[16], f32[16], /*index=15*/f32[4096,16], f32[4096], f32[16,4096], f32[16], f32[16], /*index=20*/f32[16], f32[16,16], f32[16], f32[16,16], f32[16], /*index=25*/f32[16], f32[16], f32[30522], f32[2,16], f32[2], /*index=30*/f32[1], f32[30522,16], f32[30522,16], f32[512,16], f32[512,16], /*index=35*/f32[2,16], f32[2,16], f32[16,16], f32[16,16], f32[16,16], /*index=40*/f32[16,16], f32[16,16], f32[16,16], f32[16,16], f32[16,16], /*index=45*/f32[4096,16], f32[4096,16], f32[16,4096], f32[16,4096], f32[16,16], /*index=50*/f32[16,16], f32[16,16], f32[16,16], f32[2,16], f32[2,16], /*index=55*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=60*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=65*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=70*/f32[16], f32[4096], f32[4096], f32[16], f32[16], /*index=75*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=80*/f32[16], f32[30522], f32[30522], f32[16], f32[16], /*index=85*/f32[16], f32[16], f32[16], f32[16], f32[2], /*index=90*/f32[2], f32[1], f32[1], f32[1]) {
  %p42.558 = f32[30522,16]{1,0} parameter(42), frontend_attributes={neff_input_names="input42"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p41.554 = f32[30522,16]{1,0} parameter(41), frontend_attributes={neff_input_names="input41"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p40.553 = f32[] parameter(40), frontend_attributes={neff_input_names="input40"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.555 = f32[30522,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.556 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %p41.554, f32[30522,16]{1,0} %broadcast.555), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p36.58 = f32[2]{0} parameter(36), frontend_attributes={neff_input_names="input36"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p35.57 = f32[16]{0} parameter(35), frontend_attributes={neff_input_names="input35"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p34.56 = f32[16]{0} parameter(34), frontend_attributes={neff_input_names="input34"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p33.55 = f32[16]{0} parameter(33), frontend_attributes={neff_input_names="input33"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p32.54 = f32[30522]{0} parameter(32), frontend_attributes={neff_input_names="input32"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p31.53 = f32[16]{0} parameter(31), frontend_attributes={neff_input_names="input31"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p30.52 = f32[16]{0} parameter(30), frontend_attributes={neff_input_names="input30"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p29.51 = f32[16]{0} parameter(29), frontend_attributes={neff_input_names="input29"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p28.50 = f32[16]{0} parameter(28), frontend_attributes={neff_input_names="input28"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p27.49 = f32[4096]{0} parameter(27), frontend_attributes={neff_input_names="input27"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p26.48 = f32[16]{0} parameter(26), frontend_attributes={neff_input_names="input26"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p25.47 = f32[16]{0} parameter(25), frontend_attributes={neff_input_names="input25"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p24.46 = f32[16]{0} parameter(24), frontend_attributes={neff_input_names="input24"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p23.45 = f32[16]{0} parameter(23), frontend_attributes={neff_input_names="input23"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p22.44 = f32[16]{0} parameter(22), frontend_attributes={neff_input_names="input22"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p21.43 = f32[16]{0} parameter(21), frontend_attributes={neff_input_names="input21"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p20.42 = f32[16]{0} parameter(20), frontend_attributes={neff_input_names="input20"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p19.41 = f32[16]{0} parameter(19), frontend_attributes={neff_input_names="input19"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p18.40 = f32[2,16]{1,0} parameter(18), frontend_attributes={neff_input_names="input18"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p17.39 = f32[16,16]{1,0} parameter(17), frontend_attributes={neff_input_names="input17"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p16.38 = f32[16,16]{1,0} parameter(16), frontend_attributes={neff_input_names="input16"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p15.37 = f32[16,4096]{1,0} parameter(15), frontend_attributes={neff_input_names="input15"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p14.36 = f32[4096,16]{1,0} parameter(14), frontend_attributes={neff_input_names="input14"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p13.35 = f32[16,16]{1,0} parameter(13), frontend_attributes={neff_input_names="input13"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p12.34 = f32[16,16]{1,0} parameter(12), frontend_attributes={neff_input_names="input12"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p11.33 = f32[16,16]{1,0} parameter(11), frontend_attributes={neff_input_names="input11"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p10.32 = f32[16,16]{1,0} parameter(10), frontend_attributes={neff_input_names="input10"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p9.31 = f32[2,16]{1,0} parameter(9), frontend_attributes={neff_input_names="input9"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p8.30 = f32[512,16]{1,0} parameter(8), frontend_attributes={neff_input_names="input8"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p7.29 = f32[30522,16]{1,0} parameter(7), frontend_attributes={neff_input_names="input7"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p6.16 = f32[1]{0} parameter(6), frontend_attributes={neff_input_names="input6"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %p5.15 = f32[] parameter(5), frontend_attributes={neff_input_names="input5"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %reshape = f32[1]{0} reshape(f32[] %p5.15), metadata={op_type="aten__div" op_name="aten__div" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %divide.18 = f32[1]{0} divide(f32[1]{0} %p6.16, f32[1]{0} %reshape), metadata={op_type="aten__div" op_name="aten__div" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %p4.14 = f32[] parameter(4), frontend_attributes={neff_input_names="input4"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %all-reduce.26 = (f32[1]{0}, f32[]) all-reduce(f32[1]{0} %divide.18, f32[] %p4.14), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.22, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %get-tuple-element.90 = f32[] get-tuple-element((f32[1]{0}, f32[]) %all-reduce.26), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %all-reduce.95 = (f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) all-reduce(f32[2]{0} %p36.58, f32[16]{0} %p35.57, f32[16]{0} %p34.56, f32[16]{0} %p33.55, f32[30522]{0} %p32.54, /*index=5*/f32[16]{0} %p31.53, f32[16]{0} %p30.52, f32[16]{0} %p29.51, f32[16]{0} %p28.50, f32[4096]{0} %p27.49, /*index=10*/f32[16]{0} %p26.48, f32[16]{0} %p25.47, f32[16]{0} %p24.46, f32[16]{0} %p23.45, f32[16]{0} %p22.44, /*index=15*/f32[16]{0} %p21.43, f32[16]{0} %p20.42, f32[16]{0} %p19.41, f32[2,16]{1,0} %p18.40, f32[16,16]{1,0} %p17.39, /*index=20*/f32[16,16]{1,0} %p16.38, f32[16,4096]{1,0} %p15.37, f32[4096,16]{1,0} %p14.36, f32[16,16]{1,0} %p13.35, f32[16,16]{1,0} %p12.34, /*index=25*/f32[16,16]{1,0} %p11.33, f32[16,16]{1,0} %p10.32, f32[2,16]{1,0} %p9.31, f32[512,16]{1,0} %p8.30, f32[30522,16]{1,0} %p7.29, /*index=30*/f32[] %get-tuple-element.90), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.91, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %get-tuple-element.212 = f32[30522,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=29, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.528 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=49}
  %constant.213 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.214 = f32[30522,16]{1,0} broadcast(f32[] %constant.213), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.215 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %get-tuple-element.212, f32[30522,16]{1,0} %broadcast.214), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.478 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.215, f32[30522,16]{1,0} %multiply.215), metadata={op_type="aten__mul" op_name="aten__norm.1/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.479 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.1/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.485 = f32[] reduce(f32[30522,16]{1,0} %multiply.478, f32[] %constant.479), dimensions={0,1}, to_apply=%AddComputation.481, metadata={op_type="aten__sum" op_name="aten__norm.1/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.486 = f32[] sqrt(f32[] %reduce.485), metadata={op_type="aten__sqrt" op_name="aten__norm.1/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.487 = f32[1]{0} reshape(f32[] %sqrt.486), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.208 = f32[512,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=28, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.209 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.210 = f32[512,16]{1,0} broadcast(f32[] %constant.209), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.211 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %get-tuple-element.208, f32[512,16]{1,0} %broadcast.210), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.469 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.211, f32[512,16]{1,0} %multiply.211), metadata={op_type="aten__mul" op_name="aten__norm.2/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.470 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.2/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.476 = f32[] reduce(f32[512,16]{1,0} %multiply.469, f32[] %constant.470), dimensions={0,1}, to_apply=%AddComputation.472, metadata={op_type="aten__sum" op_name="aten__norm.2/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.477 = f32[] sqrt(f32[] %reduce.476), metadata={op_type="aten__sqrt" op_name="aten__norm.2/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.488 = f32[1]{0} reshape(f32[] %sqrt.477), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.204 = f32[2,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=27, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.205 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.206 = f32[2,16]{1,0} broadcast(f32[] %constant.205), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.207 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.204, f32[2,16]{1,0} %broadcast.206), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.460 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.207, f32[2,16]{1,0} %multiply.207), metadata={op_type="aten__mul" op_name="aten__norm.3/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.461 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.3/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.467 = f32[] reduce(f32[2,16]{1,0} %multiply.460, f32[] %constant.461), dimensions={0,1}, to_apply=%AddComputation.463, metadata={op_type="aten__sum" op_name="aten__norm.3/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.468 = f32[] sqrt(f32[] %reduce.467), metadata={op_type="aten__sqrt" op_name="aten__norm.3/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.489 = f32[1]{0} reshape(f32[] %sqrt.468), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.164 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=17, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.165 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.166 = f32[16]{0} broadcast(f32[] %constant.165), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.167 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.164, f32[16]{0} %broadcast.166), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.451 = f32[16]{0} multiply(f32[16]{0} %multiply.167, f32[16]{0} %multiply.167), metadata={op_type="aten__mul" op_name="aten__norm.4/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.452 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.4/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.458 = f32[] reduce(f32[16]{0} %multiply.451, f32[] %constant.452), dimensions={0}, to_apply=%AddComputation.454, metadata={op_type="aten__sum" op_name="aten__norm.4/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.459 = f32[] sqrt(f32[] %reduce.458), metadata={op_type="aten__sqrt" op_name="aten__norm.4/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.490 = f32[1]{0} reshape(f32[] %sqrt.459), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.160 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=16, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.161 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.162 = f32[16]{0} broadcast(f32[] %constant.161), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.163 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.160, f32[16]{0} %broadcast.162), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.442 = f32[16]{0} multiply(f32[16]{0} %multiply.163, f32[16]{0} %multiply.163), metadata={op_type="aten__mul" op_name="aten__norm.5/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.443 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.449 = f32[] reduce(f32[16]{0} %multiply.442, f32[] %constant.443), dimensions={0}, to_apply=%AddComputation.445, metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.450 = f32[] sqrt(f32[] %reduce.449), metadata={op_type="aten__sqrt" op_name="aten__norm.5/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.491 = f32[1]{0} reshape(f32[] %sqrt.450), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.200 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=26, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.201 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.202 = f32[16,16]{1,0} broadcast(f32[] %constant.201), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.203 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.200, f32[16,16]{1,0} %broadcast.202), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.433 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.203, f32[16,16]{1,0} %multiply.203), metadata={op_type="aten__mul" op_name="aten__norm.6/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.434 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.440 = f32[] reduce(f32[16,16]{1,0} %multiply.433, f32[] %constant.434), dimensions={0,1}, to_apply=%AddComputation.436, metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.441 = f32[] sqrt(f32[] %reduce.440), metadata={op_type="aten__sqrt" op_name="aten__norm.6/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.492 = f32[1]{0} reshape(f32[] %sqrt.441), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.156 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=15, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.157 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.158 = f32[16]{0} broadcast(f32[] %constant.157), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.159 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.156, f32[16]{0} %broadcast.158), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.424 = f32[16]{0} multiply(f32[16]{0} %multiply.159, f32[16]{0} %multiply.159), metadata={op_type="aten__mul" op_name="aten__norm.7/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.425 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.431 = f32[] reduce(f32[16]{0} %multiply.424, f32[] %constant.425), dimensions={0}, to_apply=%AddComputation.427, metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.432 = f32[] sqrt(f32[] %reduce.431), metadata={op_type="aten__sqrt" op_name="aten__norm.7/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.493 = f32[1]{0} reshape(f32[] %sqrt.432), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.196 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=25, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.197 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.198 = f32[16,16]{1,0} broadcast(f32[] %constant.197), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.199 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.196, f32[16,16]{1,0} %broadcast.198), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.415 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.199, f32[16,16]{1,0} %multiply.199), metadata={op_type="aten__mul" op_name="aten__norm.8/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.416 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.422 = f32[] reduce(f32[16,16]{1,0} %multiply.415, f32[] %constant.416), dimensions={0,1}, to_apply=%AddComputation.418, metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.423 = f32[] sqrt(f32[] %reduce.422), metadata={op_type="aten__sqrt" op_name="aten__norm.8/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.494 = f32[1]{0} reshape(f32[] %sqrt.423), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.152 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=14, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.153 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.154 = f32[16]{0} broadcast(f32[] %constant.153), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.155 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.152, f32[16]{0} %broadcast.154), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.406 = f32[16]{0} multiply(f32[16]{0} %multiply.155, f32[16]{0} %multiply.155), metadata={op_type="aten__mul" op_name="aten__norm.9/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.407 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.413 = f32[] reduce(f32[16]{0} %multiply.406, f32[] %constant.407), dimensions={0}, to_apply=%AddComputation.409, metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.414 = f32[] sqrt(f32[] %reduce.413), metadata={op_type="aten__sqrt" op_name="aten__norm.9/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.495 = f32[1]{0} reshape(f32[] %sqrt.414), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.192 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.193 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.194 = f32[16,16]{1,0} broadcast(f32[] %constant.193), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.195 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.192, f32[16,16]{1,0} %broadcast.194), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.397 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.195, f32[16,16]{1,0} %multiply.195), metadata={op_type="aten__mul" op_name="aten__norm.10/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.398 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.404 = f32[] reduce(f32[16,16]{1,0} %multiply.397, f32[] %constant.398), dimensions={0,1}, to_apply=%AddComputation.400, metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.405 = f32[] sqrt(f32[] %reduce.404), metadata={op_type="aten__sqrt" op_name="aten__norm.10/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.496 = f32[1]{0} reshape(f32[] %sqrt.405), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.148 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=13, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.149 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.150 = f32[16]{0} broadcast(f32[] %constant.149), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.151 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.148, f32[16]{0} %broadcast.150), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.388 = f32[16]{0} multiply(f32[16]{0} %multiply.151, f32[16]{0} %multiply.151), metadata={op_type="aten__mul" op_name="aten__norm.11/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.389 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.395 = f32[] reduce(f32[16]{0} %multiply.388, f32[] %constant.389), dimensions={0}, to_apply=%AddComputation.391, metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.396 = f32[] sqrt(f32[] %reduce.395), metadata={op_type="aten__sqrt" op_name="aten__norm.11/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.497 = f32[1]{0} reshape(f32[] %sqrt.396), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.188 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=23, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.189 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.190 = f32[16,16]{1,0} broadcast(f32[] %constant.189), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.191 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.188, f32[16,16]{1,0} %broadcast.190), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.379 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.191, f32[16,16]{1,0} %multiply.191), metadata={op_type="aten__mul" op_name="aten__norm.12/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.380 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.386 = f32[] reduce(f32[16,16]{1,0} %multiply.379, f32[] %constant.380), dimensions={0,1}, to_apply=%AddComputation.382, metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.387 = f32[] sqrt(f32[] %reduce.386), metadata={op_type="aten__sqrt" op_name="aten__norm.12/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.498 = f32[1]{0} reshape(f32[] %sqrt.387), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.144 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=12, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.145 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.146 = f32[16]{0} broadcast(f32[] %constant.145), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.147 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.144, f32[16]{0} %broadcast.146), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.370 = f32[16]{0} multiply(f32[16]{0} %multiply.147, f32[16]{0} %multiply.147), metadata={op_type="aten__mul" op_name="aten__norm.13/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.371 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.377 = f32[] reduce(f32[16]{0} %multiply.370, f32[] %constant.371), dimensions={0}, to_apply=%AddComputation.373, metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.378 = f32[] sqrt(f32[] %reduce.377), metadata={op_type="aten__sqrt" op_name="aten__norm.13/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.499 = f32[1]{0} reshape(f32[] %sqrt.378), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.140 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=11, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.141 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.142 = f32[16]{0} broadcast(f32[] %constant.141), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.143 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.140, f32[16]{0} %broadcast.142), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.361 = f32[16]{0} multiply(f32[16]{0} %multiply.143, f32[16]{0} %multiply.143), metadata={op_type="aten__mul" op_name="aten__norm.14/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.362 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.368 = f32[] reduce(f32[16]{0} %multiply.361, f32[] %constant.362), dimensions={0}, to_apply=%AddComputation.364, metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.369 = f32[] sqrt(f32[] %reduce.368), metadata={op_type="aten__sqrt" op_name="aten__norm.14/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.500 = f32[1]{0} reshape(f32[] %sqrt.369), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.136 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=10, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.137 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.138 = f32[16]{0} broadcast(f32[] %constant.137), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.139 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.136, f32[16]{0} %broadcast.138), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.352 = f32[16]{0} multiply(f32[16]{0} %multiply.139, f32[16]{0} %multiply.139), metadata={op_type="aten__mul" op_name="aten__norm.15/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.353 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.359 = f32[] reduce(f32[16]{0} %multiply.352, f32[] %constant.353), dimensions={0}, to_apply=%AddComputation.355, metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.360 = f32[] sqrt(f32[] %reduce.359), metadata={op_type="aten__sqrt" op_name="aten__norm.15/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.501 = f32[1]{0} reshape(f32[] %sqrt.360), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.184 = f32[4096,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=22, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.185 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.186 = f32[4096,16]{1,0} broadcast(f32[] %constant.185), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.187 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %get-tuple-element.184, f32[4096,16]{1,0} %broadcast.186), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.343 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.187, f32[4096,16]{1,0} %multiply.187), metadata={op_type="aten__mul" op_name="aten__norm.16/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.344 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.350 = f32[] reduce(f32[4096,16]{1,0} %multiply.343, f32[] %constant.344), dimensions={0,1}, to_apply=%AddComputation.346, metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.351 = f32[] sqrt(f32[] %reduce.350), metadata={op_type="aten__sqrt" op_name="aten__norm.16/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.502 = f32[1]{0} reshape(f32[] %sqrt.351), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.132 = f32[4096]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.133 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.134 = f32[4096]{0} broadcast(f32[] %constant.133), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.135 = f32[4096]{0} multiply(f32[4096]{0} %get-tuple-element.132, f32[4096]{0} %broadcast.134), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.334 = f32[4096]{0} multiply(f32[4096]{0} %multiply.135, f32[4096]{0} %multiply.135), metadata={op_type="aten__mul" op_name="aten__norm.17/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.335 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.341 = f32[] reduce(f32[4096]{0} %multiply.334, f32[] %constant.335), dimensions={0}, to_apply=%AddComputation.337, metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.342 = f32[] sqrt(f32[] %reduce.341), metadata={op_type="aten__sqrt" op_name="aten__norm.17/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.503 = f32[1]{0} reshape(f32[] %sqrt.342), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.180 = f32[16,4096]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=21, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.181 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.182 = f32[16,4096]{1,0} broadcast(f32[] %constant.181), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.183 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %get-tuple-element.180, f32[16,4096]{1,0} %broadcast.182), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.325 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.183, f32[16,4096]{1,0} %multiply.183), metadata={op_type="aten__mul" op_name="aten__norm.18/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.326 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.332 = f32[] reduce(f32[16,4096]{1,0} %multiply.325, f32[] %constant.326), dimensions={0,1}, to_apply=%AddComputation.328, metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.333 = f32[] sqrt(f32[] %reduce.332), metadata={op_type="aten__sqrt" op_name="aten__norm.18/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.504 = f32[1]{0} reshape(f32[] %sqrt.333), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.128 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=8, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.129 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.130 = f32[16]{0} broadcast(f32[] %constant.129), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.131 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.128, f32[16]{0} %broadcast.130), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.316 = f32[16]{0} multiply(f32[16]{0} %multiply.131, f32[16]{0} %multiply.131), metadata={op_type="aten__mul" op_name="aten__norm.19/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.317 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.323 = f32[] reduce(f32[16]{0} %multiply.316, f32[] %constant.317), dimensions={0}, to_apply=%AddComputation.319, metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.324 = f32[] sqrt(f32[] %reduce.323), metadata={op_type="aten__sqrt" op_name="aten__norm.19/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.505 = f32[1]{0} reshape(f32[] %sqrt.324), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.124 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=7, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.125 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.126 = f32[16]{0} broadcast(f32[] %constant.125), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.127 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.124, f32[16]{0} %broadcast.126), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.307 = f32[16]{0} multiply(f32[16]{0} %multiply.127, f32[16]{0} %multiply.127), metadata={op_type="aten__mul" op_name="aten__norm.20/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.308 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.314 = f32[] reduce(f32[16]{0} %multiply.307, f32[] %constant.308), dimensions={0}, to_apply=%AddComputation.310, metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.315 = f32[] sqrt(f32[] %reduce.314), metadata={op_type="aten__sqrt" op_name="aten__norm.20/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.506 = f32[1]{0} reshape(f32[] %sqrt.315), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.120 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=6, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.121 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.122 = f32[16]{0} broadcast(f32[] %constant.121), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.123 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.120, f32[16]{0} %broadcast.122), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.298 = f32[16]{0} multiply(f32[16]{0} %multiply.123, f32[16]{0} %multiply.123), metadata={op_type="aten__mul" op_name="aten__norm.21/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.299 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.305 = f32[] reduce(f32[16]{0} %multiply.298, f32[] %constant.299), dimensions={0}, to_apply=%AddComputation.301, metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.306 = f32[] sqrt(f32[] %reduce.305), metadata={op_type="aten__sqrt" op_name="aten__norm.21/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.507 = f32[1]{0} reshape(f32[] %sqrt.306), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.176 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=20, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.177 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.178 = f32[16,16]{1,0} broadcast(f32[] %constant.177), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.179 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.176, f32[16,16]{1,0} %broadcast.178), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.289 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.179, f32[16,16]{1,0} %multiply.179), metadata={op_type="aten__mul" op_name="aten__norm.22/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.290 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.296 = f32[] reduce(f32[16,16]{1,0} %multiply.289, f32[] %constant.290), dimensions={0,1}, to_apply=%AddComputation.292, metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.297 = f32[] sqrt(f32[] %reduce.296), metadata={op_type="aten__sqrt" op_name="aten__norm.22/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.508 = f32[1]{0} reshape(f32[] %sqrt.297), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.116 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=5, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.117 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.118 = f32[16]{0} broadcast(f32[] %constant.117), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.119 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.116, f32[16]{0} %broadcast.118), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.280 = f32[16]{0} multiply(f32[16]{0} %multiply.119, f32[16]{0} %multiply.119), metadata={op_type="aten__mul" op_name="aten__norm.23/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.281 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.287 = f32[] reduce(f32[16]{0} %multiply.280, f32[] %constant.281), dimensions={0}, to_apply=%AddComputation.283, metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.288 = f32[] sqrt(f32[] %reduce.287), metadata={op_type="aten__sqrt" op_name="aten__norm.23/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.509 = f32[1]{0} reshape(f32[] %sqrt.288), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.112 = f32[30522]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=4, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.113 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.114 = f32[30522]{0} broadcast(f32[] %constant.113), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.115 = f32[30522]{0} multiply(f32[30522]{0} %get-tuple-element.112, f32[30522]{0} %broadcast.114), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.271 = f32[30522]{0} multiply(f32[30522]{0} %multiply.115, f32[30522]{0} %multiply.115), metadata={op_type="aten__mul" op_name="aten__norm.24/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.272 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.278 = f32[] reduce(f32[30522]{0} %multiply.271, f32[] %constant.272), dimensions={0}, to_apply=%AddComputation.274, metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.279 = f32[] sqrt(f32[] %reduce.278), metadata={op_type="aten__sqrt" op_name="aten__norm.24/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.510 = f32[1]{0} reshape(f32[] %sqrt.279), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.172 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=19, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.173 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.174 = f32[16,16]{1,0} broadcast(f32[] %constant.173), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.175 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.172, f32[16,16]{1,0} %broadcast.174), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.262 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.175, f32[16,16]{1,0} %multiply.175), metadata={op_type="aten__mul" op_name="aten__norm.25/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.263 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.269 = f32[] reduce(f32[16,16]{1,0} %multiply.262, f32[] %constant.263), dimensions={0,1}, to_apply=%AddComputation.265, metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.270 = f32[] sqrt(f32[] %reduce.269), metadata={op_type="aten__sqrt" op_name="aten__norm.25/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.511 = f32[1]{0} reshape(f32[] %sqrt.270), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.108 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=3, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.109 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.110 = f32[16]{0} broadcast(f32[] %constant.109), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.111 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.108, f32[16]{0} %broadcast.110), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.253 = f32[16]{0} multiply(f32[16]{0} %multiply.111, f32[16]{0} %multiply.111), metadata={op_type="aten__mul" op_name="aten__norm.26/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.254 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.260 = f32[] reduce(f32[16]{0} %multiply.253, f32[] %constant.254), dimensions={0}, to_apply=%AddComputation.256, metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.261 = f32[] sqrt(f32[] %reduce.260), metadata={op_type="aten__sqrt" op_name="aten__norm.26/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.512 = f32[1]{0} reshape(f32[] %sqrt.261), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.104 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=2, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.105 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.106 = f32[16]{0} broadcast(f32[] %constant.105), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.107 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.104, f32[16]{0} %broadcast.106), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.244 = f32[16]{0} multiply(f32[16]{0} %multiply.107, f32[16]{0} %multiply.107), metadata={op_type="aten__mul" op_name="aten__norm.27/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.245 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.251 = f32[] reduce(f32[16]{0} %multiply.244, f32[] %constant.245), dimensions={0}, to_apply=%AddComputation.247, metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.252 = f32[] sqrt(f32[] %reduce.251), metadata={op_type="aten__sqrt" op_name="aten__norm.27/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.513 = f32[1]{0} reshape(f32[] %sqrt.252), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.100 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.101 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.102 = f32[16]{0} broadcast(f32[] %constant.101), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.103 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.100, f32[16]{0} %broadcast.102), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.235 = f32[16]{0} multiply(f32[16]{0} %multiply.103, f32[16]{0} %multiply.103), metadata={op_type="aten__mul" op_name="aten__norm.28/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.236 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.242 = f32[] reduce(f32[16]{0} %multiply.235, f32[] %constant.236), dimensions={0}, to_apply=%AddComputation.238, metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.243 = f32[] sqrt(f32[] %reduce.242), metadata={op_type="aten__sqrt" op_name="aten__norm.28/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.514 = f32[1]{0} reshape(f32[] %sqrt.243), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.168 = f32[2,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=18, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.169 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.170 = f32[2,16]{1,0} broadcast(f32[] %constant.169), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.171 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.168, f32[2,16]{1,0} %broadcast.170), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.226 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.171, f32[2,16]{1,0} %multiply.171), metadata={op_type="aten__mul" op_name="aten__norm.29/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.227 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.233 = f32[] reduce(f32[2,16]{1,0} %multiply.226, f32[] %constant.227), dimensions={0,1}, to_apply=%AddComputation.229, metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.234 = f32[] sqrt(f32[] %reduce.233), metadata={op_type="aten__sqrt" op_name="aten__norm.29/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.515 = f32[1]{0} reshape(f32[] %sqrt.234), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.96 = f32[2]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.97 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.98 = f32[2]{0} broadcast(f32[] %constant.97), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.99 = f32[2]{0} multiply(f32[2]{0} %get-tuple-element.96, f32[2]{0} %broadcast.98), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.217 = f32[2]{0} multiply(f32[2]{0} %multiply.99, f32[2]{0} %multiply.99), metadata={op_type="aten__mul" op_name="aten__norm.30/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.218 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.224 = f32[] reduce(f32[2]{0} %multiply.217, f32[] %constant.218), dimensions={0}, to_apply=%AddComputation.220, metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.225 = f32[] sqrt(f32[] %reduce.224), metadata={op_type="aten__sqrt" op_name="aten__norm.30/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.516 = f32[1]{0} reshape(f32[] %sqrt.225), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %concatenate.517 = f32[30]{0} concatenate(f32[1]{0} %reshape.487, f32[1]{0} %reshape.488, f32[1]{0} %reshape.489, f32[1]{0} %reshape.490, f32[1]{0} %reshape.491, /*index=5*/f32[1]{0} %reshape.492, f32[1]{0} %reshape.493, f32[1]{0} %reshape.494, f32[1]{0} %reshape.495, f32[1]{0} %reshape.496, /*index=10*/f32[1]{0} %reshape.497, f32[1]{0} %reshape.498, f32[1]{0} %reshape.499, f32[1]{0} %reshape.500, f32[1]{0} %reshape.501, /*index=15*/f32[1]{0} %reshape.502, f32[1]{0} %reshape.503, f32[1]{0} %reshape.504, f32[1]{0} %reshape.505, f32[1]{0} %reshape.506, /*index=20*/f32[1]{0} %reshape.507, f32[1]{0} %reshape.508, f32[1]{0} %reshape.509, f32[1]{0} %reshape.510, f32[1]{0} %reshape.511, /*index=25*/f32[1]{0} %reshape.512, f32[1]{0} %reshape.513, f32[1]{0} %reshape.514, f32[1]{0} %reshape.515, f32[1]{0} %reshape.516), dimensions={0}, metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %multiply.518 = f32[30]{0} multiply(f32[30]{0} %concatenate.517, f32[30]{0} %concatenate.517), metadata={op_type="aten__mul" op_name="aten__norm.31/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.519 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.525 = f32[] reduce(f32[30]{0} %multiply.518, f32[] %constant.519), dimensions={0}, to_apply=%AddComputation.521, metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.526 = f32[] sqrt(f32[] %reduce.525), metadata={op_type="aten__sqrt" op_name="aten__norm.31/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %p2.8 = f32[] parameter(2), frontend_attributes={neff_input_names="input2"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.527 = f32[] add(f32[] %sqrt.526, f32[] %p2.8), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=49}
  %divide.529 = f32[] divide(f32[] %constant.528, f32[] %add.527), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=49}
  %constant = f32[] constant(1)
  %compare.532 = pred[] compare(f32[] %divide.529, f32[] %constant), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=50}
  %constant.11 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=51}
  %select.533 = f32[] select(pred[] %compare.532, f32[] %divide.529, f32[] %constant.11), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=50}
  %multiply = f32[] multiply(f32[] %select.533, f32[] %constant.213), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast = f32[30522,16]{1,0} broadcast(f32[] %multiply), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.535 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %get-tuple-element.212, f32[30522,16]{1,0} %broadcast), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %p39.547 = f32[] parameter(39), frontend_attributes={neff_input_names="input39"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.551 = f32[30522,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.552 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.535, f32[30522,16]{1,0} %broadcast.551), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.557 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %multiply.556, f32[30522,16]{1,0} %multiply.552), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p38.537 = f32[30522,16]{1,0} parameter(38), frontend_attributes={neff_input_names="input38"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %p37.536 = f32[] parameter(37), frontend_attributes={neff_input_names="input37"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.538 = f32[30522,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.539 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %p38.537, f32[30522,16]{1,0} %broadcast.538), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.540 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.535, f32[30522,16]{1,0} %multiply.535), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %p3.10 = f32[] parameter(3), frontend_attributes={neff_input_names="input3"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.541 = f32[30522,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.542 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.540, f32[30522,16]{1,0} %broadcast.541), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.543 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %multiply.539, f32[30522,16]{1,0} %multiply.542), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.544 = f32[30522,16]{1,0} sqrt(f32[30522,16]{1,0} %add.543), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.545 = f32[30522,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.546 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %sqrt.544, f32[30522,16]{1,0} %broadcast.545), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.559 = f32[30522,16]{1,0} divide(f32[30522,16]{1,0} %add.557, f32[30522,16]{1,0} %add.546), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p1.6 = f32[] parameter(1), frontend_attributes={neff_input_names="input1"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.560 = f32[30522,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.561 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %divide.559, f32[30522,16]{1,0} %broadcast.560), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.562 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %p42.558, f32[30522,16]{1,0} %multiply.561), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p0.1 = f32[] parameter(0), frontend_attributes={neff_input_names="input0"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.5 = f32[30522,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.563 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %add.562, f32[30522,16]{1,0} %broadcast.5), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.564 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %add.562, f32[30522,16]{1,0} %multiply.563), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p45.592 = f32[512,16]{1,0} parameter(45), frontend_attributes={neff_input_names="input45"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p44.588 = f32[512,16]{1,0} parameter(44), frontend_attributes={neff_input_names="input44"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.589 = f32[512,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.590 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %p44.588, f32[512,16]{1,0} %broadcast.589), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.2 = f32[] multiply(f32[] %select.533, f32[] %constant.209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1 = f32[512,16]{1,0} broadcast(f32[] %multiply.2), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.572 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %get-tuple-element.208, f32[512,16]{1,0} %broadcast.1), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.586 = f32[512,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.587 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.572, f32[512,16]{1,0} %broadcast.586), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.591 = f32[512,16]{1,0} add(f32[512,16]{1,0} %multiply.590, f32[512,16]{1,0} %multiply.587), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p43.573 = f32[512,16]{1,0} parameter(43), frontend_attributes={neff_input_names="input43"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.574 = f32[512,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.575 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %p43.573, f32[512,16]{1,0} %broadcast.574), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.576 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.572, f32[512,16]{1,0} %multiply.572), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.577 = f32[512,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.578 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.576, f32[512,16]{1,0} %broadcast.577), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.579 = f32[512,16]{1,0} add(f32[512,16]{1,0} %multiply.575, f32[512,16]{1,0} %multiply.578), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.580 = f32[512,16]{1,0} sqrt(f32[512,16]{1,0} %add.579), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.581 = f32[512,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.582 = f32[512,16]{1,0} add(f32[512,16]{1,0} %sqrt.580, f32[512,16]{1,0} %broadcast.581), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.593 = f32[512,16]{1,0} divide(f32[512,16]{1,0} %add.591, f32[512,16]{1,0} %add.582), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.594 = f32[512,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.595 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %divide.593, f32[512,16]{1,0} %broadcast.594), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.596 = f32[512,16]{1,0} add(f32[512,16]{1,0} %p45.592, f32[512,16]{1,0} %multiply.595), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.568 = f32[512,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.597 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %add.596, f32[512,16]{1,0} %broadcast.568), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.598 = f32[512,16]{1,0} add(f32[512,16]{1,0} %add.596, f32[512,16]{1,0} %multiply.597), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p48.626 = f32[2,16]{1,0} parameter(48), frontend_attributes={neff_input_names="input48"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p47.622 = f32[2,16]{1,0} parameter(47), frontend_attributes={neff_input_names="input47"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.623 = f32[2,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.624 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %p47.622, f32[2,16]{1,0} %broadcast.623), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.4 = f32[] multiply(f32[] %select.533, f32[] %constant.205), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.2 = f32[2,16]{1,0} broadcast(f32[] %multiply.4), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.606 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.204, f32[2,16]{1,0} %broadcast.2), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.620 = f32[2,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.621 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.606, f32[2,16]{1,0} %broadcast.620), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.625 = f32[2,16]{1,0} add(f32[2,16]{1,0} %multiply.624, f32[2,16]{1,0} %multiply.621), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p46.607 = f32[2,16]{1,0} parameter(46), frontend_attributes={neff_input_names="input46"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.608 = f32[2,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.609 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %p46.607, f32[2,16]{1,0} %broadcast.608), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.610 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.606, f32[2,16]{1,0} %multiply.606), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.611 = f32[2,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.612 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.610, f32[2,16]{1,0} %broadcast.611), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.613 = f32[2,16]{1,0} add(f32[2,16]{1,0} %multiply.609, f32[2,16]{1,0} %multiply.612), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.614 = f32[2,16]{1,0} sqrt(f32[2,16]{1,0} %add.613), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.615 = f32[2,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.616 = f32[2,16]{1,0} add(f32[2,16]{1,0} %sqrt.614, f32[2,16]{1,0} %broadcast.615), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.627 = f32[2,16]{1,0} divide(f32[2,16]{1,0} %add.625, f32[2,16]{1,0} %add.616), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.628 = f32[2,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.629 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %divide.627, f32[2,16]{1,0} %broadcast.628), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.630 = f32[2,16]{1,0} add(f32[2,16]{1,0} %p48.626, f32[2,16]{1,0} %multiply.629), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.602 = f32[2,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.631 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %add.630, f32[2,16]{1,0} %broadcast.602), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.632 = f32[2,16]{1,0} add(f32[2,16]{1,0} %add.630, f32[2,16]{1,0} %multiply.631), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p51.656 = f32[16]{0} parameter(51), frontend_attributes={neff_input_names="input51"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p50.652 = f32[16]{0} parameter(50), frontend_attributes={neff_input_names="input50"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.653 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.654 = f32[16]{0} multiply(f32[16]{0} %p50.652, f32[16]{0} %broadcast.653), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.6 = f32[] multiply(f32[] %select.533, f32[] %constant.165), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.4 = f32[16]{0} broadcast(f32[] %multiply.6), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.636 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.164, f32[16]{0} %broadcast.4), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.650 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.651 = f32[16]{0} multiply(f32[16]{0} %multiply.636, f32[16]{0} %broadcast.650), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.655 = f32[16]{0} add(f32[16]{0} %multiply.654, f32[16]{0} %multiply.651), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p49.637 = f32[16]{0} parameter(49), frontend_attributes={neff_input_names="input49"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.638 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.639 = f32[16]{0} multiply(f32[16]{0} %p49.637, f32[16]{0} %broadcast.638), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.640 = f32[16]{0} multiply(f32[16]{0} %multiply.636, f32[16]{0} %multiply.636), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.641 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.642 = f32[16]{0} multiply(f32[16]{0} %multiply.640, f32[16]{0} %broadcast.641), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.643 = f32[16]{0} add(f32[16]{0} %multiply.639, f32[16]{0} %multiply.642), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.644 = f32[16]{0} sqrt(f32[16]{0} %add.643), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.645 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.646 = f32[16]{0} add(f32[16]{0} %sqrt.644, f32[16]{0} %broadcast.645), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.657 = f32[16]{0} divide(f32[16]{0} %add.655, f32[16]{0} %add.646), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.658 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.659 = f32[16]{0} multiply(f32[16]{0} %divide.657, f32[16]{0} %broadcast.658), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.660 = f32[16]{0} add(f32[16]{0} %p51.656, f32[16]{0} %multiply.659), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p54.684 = f32[16]{0} parameter(54), frontend_attributes={neff_input_names="input54"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p53.680 = f32[16]{0} parameter(53), frontend_attributes={neff_input_names="input53"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.681 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.682 = f32[16]{0} multiply(f32[16]{0} %p53.680, f32[16]{0} %broadcast.681), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.8 = f32[] multiply(f32[] %select.533, f32[] %constant.161), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.6 = f32[16]{0} broadcast(f32[] %multiply.8), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.664 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.160, f32[16]{0} %broadcast.6), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.678 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.679 = f32[16]{0} multiply(f32[16]{0} %multiply.664, f32[16]{0} %broadcast.678), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.683 = f32[16]{0} add(f32[16]{0} %multiply.682, f32[16]{0} %multiply.679), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p52.665 = f32[16]{0} parameter(52), frontend_attributes={neff_input_names="input52"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.666 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.667 = f32[16]{0} multiply(f32[16]{0} %p52.665, f32[16]{0} %broadcast.666), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.668 = f32[16]{0} multiply(f32[16]{0} %multiply.664, f32[16]{0} %multiply.664), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.669 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.670 = f32[16]{0} multiply(f32[16]{0} %multiply.668, f32[16]{0} %broadcast.669), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.671 = f32[16]{0} add(f32[16]{0} %multiply.667, f32[16]{0} %multiply.670), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.672 = f32[16]{0} sqrt(f32[16]{0} %add.671), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.673 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.674 = f32[16]{0} add(f32[16]{0} %sqrt.672, f32[16]{0} %broadcast.673), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.685 = f32[16]{0} divide(f32[16]{0} %add.683, f32[16]{0} %add.674), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.686 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.687 = f32[16]{0} multiply(f32[16]{0} %divide.685, f32[16]{0} %broadcast.686), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.688 = f32[16]{0} add(f32[16]{0} %p54.684, f32[16]{0} %multiply.687), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p57.716 = f32[16,16]{1,0} parameter(57), frontend_attributes={neff_input_names="input57"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p56.712 = f32[16,16]{1,0} parameter(56), frontend_attributes={neff_input_names="input56"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.713 = f32[16,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.714 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p56.712, f32[16,16]{1,0} %broadcast.713), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.11 = f32[] multiply(f32[] %select.533, f32[] %constant.201), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.7 = f32[16,16]{1,0} broadcast(f32[] %multiply.11), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.696 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.200, f32[16,16]{1,0} %broadcast.7), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.710 = f32[16,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.711 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.696, f32[16,16]{1,0} %broadcast.710), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.715 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.714, f32[16,16]{1,0} %multiply.711), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p55.697 = f32[16,16]{1,0} parameter(55), frontend_attributes={neff_input_names="input55"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.698 = f32[16,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.699 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p55.697, f32[16,16]{1,0} %broadcast.698), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.700 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.696, f32[16,16]{1,0} %multiply.696), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.701 = f32[16,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.702 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.700, f32[16,16]{1,0} %broadcast.701), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.703 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.699, f32[16,16]{1,0} %multiply.702), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.704 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.703), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.705 = f32[16,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.706 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.704, f32[16,16]{1,0} %broadcast.705), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.717 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.715, f32[16,16]{1,0} %add.706), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.718 = f32[16,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.719 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.717, f32[16,16]{1,0} %broadcast.718), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.720 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p57.716, f32[16,16]{1,0} %multiply.719), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.692 = f32[16,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.721 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.720, f32[16,16]{1,0} %broadcast.692), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.722 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.720, f32[16,16]{1,0} %multiply.721), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p60.746 = f32[16]{0} parameter(60), frontend_attributes={neff_input_names="input60"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p59.742 = f32[16]{0} parameter(59), frontend_attributes={neff_input_names="input59"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.743 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.744 = f32[16]{0} multiply(f32[16]{0} %p59.742, f32[16]{0} %broadcast.743), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.14 = f32[] multiply(f32[] %select.533, f32[] %constant.157), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.8 = f32[16]{0} broadcast(f32[] %multiply.14), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.726 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.156, f32[16]{0} %broadcast.8), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.740 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.741 = f32[16]{0} multiply(f32[16]{0} %multiply.726, f32[16]{0} %broadcast.740), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.745 = f32[16]{0} add(f32[16]{0} %multiply.744, f32[16]{0} %multiply.741), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p58.727 = f32[16]{0} parameter(58), frontend_attributes={neff_input_names="input58"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.728 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.729 = f32[16]{0} multiply(f32[16]{0} %p58.727, f32[16]{0} %broadcast.728), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.730 = f32[16]{0} multiply(f32[16]{0} %multiply.726, f32[16]{0} %multiply.726), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.731 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.732 = f32[16]{0} multiply(f32[16]{0} %multiply.730, f32[16]{0} %broadcast.731), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.733 = f32[16]{0} add(f32[16]{0} %multiply.729, f32[16]{0} %multiply.732), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.734 = f32[16]{0} sqrt(f32[16]{0} %add.733), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.735 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.736 = f32[16]{0} add(f32[16]{0} %sqrt.734, f32[16]{0} %broadcast.735), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.747 = f32[16]{0} divide(f32[16]{0} %add.745, f32[16]{0} %add.736), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.748 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.749 = f32[16]{0} multiply(f32[16]{0} %divide.747, f32[16]{0} %broadcast.748), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.750 = f32[16]{0} add(f32[16]{0} %p60.746, f32[16]{0} %multiply.749), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p63.778 = f32[16,16]{1,0} parameter(63), frontend_attributes={neff_input_names="input63"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p62.774 = f32[16,16]{1,0} parameter(62), frontend_attributes={neff_input_names="input62"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.775 = f32[16,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.776 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p62.774, f32[16,16]{1,0} %broadcast.775), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.16 = f32[] multiply(f32[] %select.533, f32[] %constant.197), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.9 = f32[16,16]{1,0} broadcast(f32[] %multiply.16), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.758 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.196, f32[16,16]{1,0} %broadcast.9), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.772 = f32[16,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.773 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.758, f32[16,16]{1,0} %broadcast.772), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.777 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.776, f32[16,16]{1,0} %multiply.773), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p61.759 = f32[16,16]{1,0} parameter(61), frontend_attributes={neff_input_names="input61"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.760 = f32[16,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.761 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p61.759, f32[16,16]{1,0} %broadcast.760), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.762 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.758, f32[16,16]{1,0} %multiply.758), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.763 = f32[16,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.764 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.762, f32[16,16]{1,0} %broadcast.763), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.765 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.761, f32[16,16]{1,0} %multiply.764), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.766 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.765), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.767 = f32[16,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.768 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.766, f32[16,16]{1,0} %broadcast.767), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.779 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.777, f32[16,16]{1,0} %add.768), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.780 = f32[16,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.781 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.779, f32[16,16]{1,0} %broadcast.780), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.782 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p63.778, f32[16,16]{1,0} %multiply.781), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.754 = f32[16,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.783 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.782, f32[16,16]{1,0} %broadcast.754), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.784 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.782, f32[16,16]{1,0} %multiply.783), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p66.808 = f32[16]{0} parameter(66), frontend_attributes={neff_input_names="input66"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p65.804 = f32[16]{0} parameter(65), frontend_attributes={neff_input_names="input65"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.805 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.806 = f32[16]{0} multiply(f32[16]{0} %p65.804, f32[16]{0} %broadcast.805), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.18 = f32[] multiply(f32[] %select.533, f32[] %constant.153), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.10 = f32[16]{0} broadcast(f32[] %multiply.18), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.788 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.152, f32[16]{0} %broadcast.10), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.802 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.803 = f32[16]{0} multiply(f32[16]{0} %multiply.788, f32[16]{0} %broadcast.802), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.807 = f32[16]{0} add(f32[16]{0} %multiply.806, f32[16]{0} %multiply.803), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p64.789 = f32[16]{0} parameter(64), frontend_attributes={neff_input_names="input64"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.790 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.791 = f32[16]{0} multiply(f32[16]{0} %p64.789, f32[16]{0} %broadcast.790), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.792 = f32[16]{0} multiply(f32[16]{0} %multiply.788, f32[16]{0} %multiply.788), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.793 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.794 = f32[16]{0} multiply(f32[16]{0} %multiply.792, f32[16]{0} %broadcast.793), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.795 = f32[16]{0} add(f32[16]{0} %multiply.791, f32[16]{0} %multiply.794), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.796 = f32[16]{0} sqrt(f32[16]{0} %add.795), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.797 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.798 = f32[16]{0} add(f32[16]{0} %sqrt.796, f32[16]{0} %broadcast.797), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.809 = f32[16]{0} divide(f32[16]{0} %add.807, f32[16]{0} %add.798), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.810 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.811 = f32[16]{0} multiply(f32[16]{0} %divide.809, f32[16]{0} %broadcast.810), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.812 = f32[16]{0} add(f32[16]{0} %p66.808, f32[16]{0} %multiply.811), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p69.840 = f32[16,16]{1,0} parameter(69), frontend_attributes={neff_input_names="input69"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p68.836 = f32[16,16]{1,0} parameter(68), frontend_attributes={neff_input_names="input68"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.837 = f32[16,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.838 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p68.836, f32[16,16]{1,0} %broadcast.837), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.20 = f32[] multiply(f32[] %select.533, f32[] %constant.193), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.11 = f32[16,16]{1,0} broadcast(f32[] %multiply.20), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.820 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.192, f32[16,16]{1,0} %broadcast.11), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.834 = f32[16,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.835 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.820, f32[16,16]{1,0} %broadcast.834), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.839 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.838, f32[16,16]{1,0} %multiply.835), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p67.821 = f32[16,16]{1,0} parameter(67), frontend_attributes={neff_input_names="input67"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.822 = f32[16,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.823 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p67.821, f32[16,16]{1,0} %broadcast.822), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.824 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.820, f32[16,16]{1,0} %multiply.820), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.825 = f32[16,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.826 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.824, f32[16,16]{1,0} %broadcast.825), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.827 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.823, f32[16,16]{1,0} %multiply.826), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.828 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.827), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.829 = f32[16,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.830 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.828, f32[16,16]{1,0} %broadcast.829), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.841 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.839, f32[16,16]{1,0} %add.830), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.842 = f32[16,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.843 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.841, f32[16,16]{1,0} %broadcast.842), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.844 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p69.840, f32[16,16]{1,0} %multiply.843), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.816 = f32[16,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.845 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.844, f32[16,16]{1,0} %broadcast.816), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.846 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.844, f32[16,16]{1,0} %multiply.845), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p72.870 = f32[16]{0} parameter(72), frontend_attributes={neff_input_names="input72"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p71.866 = f32[16]{0} parameter(71), frontend_attributes={neff_input_names="input71"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.867 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.868 = f32[16]{0} multiply(f32[16]{0} %p71.866, f32[16]{0} %broadcast.867), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.22 = f32[] multiply(f32[] %select.533, f32[] %constant.149), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12 = f32[16]{0} broadcast(f32[] %multiply.22), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.850 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.148, f32[16]{0} %broadcast.12), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.864 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.865 = f32[16]{0} multiply(f32[16]{0} %multiply.850, f32[16]{0} %broadcast.864), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.869 = f32[16]{0} add(f32[16]{0} %multiply.868, f32[16]{0} %multiply.865), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p70.851 = f32[16]{0} parameter(70), frontend_attributes={neff_input_names="input70"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.852 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.853 = f32[16]{0} multiply(f32[16]{0} %p70.851, f32[16]{0} %broadcast.852), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.854 = f32[16]{0} multiply(f32[16]{0} %multiply.850, f32[16]{0} %multiply.850), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.855 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.856 = f32[16]{0} multiply(f32[16]{0} %multiply.854, f32[16]{0} %broadcast.855), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.857 = f32[16]{0} add(f32[16]{0} %multiply.853, f32[16]{0} %multiply.856), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.858 = f32[16]{0} sqrt(f32[16]{0} %add.857), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.859 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.860 = f32[16]{0} add(f32[16]{0} %sqrt.858, f32[16]{0} %broadcast.859), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.871 = f32[16]{0} divide(f32[16]{0} %add.869, f32[16]{0} %add.860), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.872 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.873 = f32[16]{0} multiply(f32[16]{0} %divide.871, f32[16]{0} %broadcast.872), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.874 = f32[16]{0} add(f32[16]{0} %p72.870, f32[16]{0} %multiply.873), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p75.902 = f32[16,16]{1,0} parameter(75), frontend_attributes={neff_input_names="input75"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p74.898 = f32[16,16]{1,0} parameter(74), frontend_attributes={neff_input_names="input74"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.899 = f32[16,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.900 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p74.898, f32[16,16]{1,0} %broadcast.899), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.24 = f32[] multiply(f32[] %select.533, f32[] %constant.189), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.13 = f32[16,16]{1,0} broadcast(f32[] %multiply.24), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.882 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.188, f32[16,16]{1,0} %broadcast.13), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.896 = f32[16,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.897 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.882, f32[16,16]{1,0} %broadcast.896), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.901 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.900, f32[16,16]{1,0} %multiply.897), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p73.883 = f32[16,16]{1,0} parameter(73), frontend_attributes={neff_input_names="input73"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.884 = f32[16,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.885 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p73.883, f32[16,16]{1,0} %broadcast.884), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.886 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.882, f32[16,16]{1,0} %multiply.882), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.887 = f32[16,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.888 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.886, f32[16,16]{1,0} %broadcast.887), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.889 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.885, f32[16,16]{1,0} %multiply.888), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.890 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.889), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.891 = f32[16,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.892 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.890, f32[16,16]{1,0} %broadcast.891), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.903 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.901, f32[16,16]{1,0} %add.892), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.904 = f32[16,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.905 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.903, f32[16,16]{1,0} %broadcast.904), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.906 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p75.902, f32[16,16]{1,0} %multiply.905), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.878 = f32[16,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.907 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.906, f32[16,16]{1,0} %broadcast.878), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.908 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.906, f32[16,16]{1,0} %multiply.907), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p78.932 = f32[16]{0} parameter(78), frontend_attributes={neff_input_names="input78"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p77.928 = f32[16]{0} parameter(77), frontend_attributes={neff_input_names="input77"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.929 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.930 = f32[16]{0} multiply(f32[16]{0} %p77.928, f32[16]{0} %broadcast.929), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.26 = f32[] multiply(f32[] %select.533, f32[] %constant.145), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.14 = f32[16]{0} broadcast(f32[] %multiply.26), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.912 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.144, f32[16]{0} %broadcast.14), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.926 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.927 = f32[16]{0} multiply(f32[16]{0} %multiply.912, f32[16]{0} %broadcast.926), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.931 = f32[16]{0} add(f32[16]{0} %multiply.930, f32[16]{0} %multiply.927), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p76.913 = f32[16]{0} parameter(76), frontend_attributes={neff_input_names="input76"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.914 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.915 = f32[16]{0} multiply(f32[16]{0} %p76.913, f32[16]{0} %broadcast.914), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.916 = f32[16]{0} multiply(f32[16]{0} %multiply.912, f32[16]{0} %multiply.912), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.917 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.918 = f32[16]{0} multiply(f32[16]{0} %multiply.916, f32[16]{0} %broadcast.917), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.919 = f32[16]{0} add(f32[16]{0} %multiply.915, f32[16]{0} %multiply.918), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.920 = f32[16]{0} sqrt(f32[16]{0} %add.919), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.921 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.922 = f32[16]{0} add(f32[16]{0} %sqrt.920, f32[16]{0} %broadcast.921), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.933 = f32[16]{0} divide(f32[16]{0} %add.931, f32[16]{0} %add.922), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.934 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.935 = f32[16]{0} multiply(f32[16]{0} %divide.933, f32[16]{0} %broadcast.934), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.936 = f32[16]{0} add(f32[16]{0} %p78.932, f32[16]{0} %multiply.935), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p81.960 = f32[16]{0} parameter(81), frontend_attributes={neff_input_names="input81"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p80.956 = f32[16]{0} parameter(80), frontend_attributes={neff_input_names="input80"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.957 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.958 = f32[16]{0} multiply(f32[16]{0} %p80.956, f32[16]{0} %broadcast.957), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.28 = f32[] multiply(f32[] %select.533, f32[] %constant.141), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.15 = f32[16]{0} broadcast(f32[] %multiply.28), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.940 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.140, f32[16]{0} %broadcast.15), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.954 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.955 = f32[16]{0} multiply(f32[16]{0} %multiply.940, f32[16]{0} %broadcast.954), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.959 = f32[16]{0} add(f32[16]{0} %multiply.958, f32[16]{0} %multiply.955), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p79.941 = f32[16]{0} parameter(79), frontend_attributes={neff_input_names="input79"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.942 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.943 = f32[16]{0} multiply(f32[16]{0} %p79.941, f32[16]{0} %broadcast.942), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.944 = f32[16]{0} multiply(f32[16]{0} %multiply.940, f32[16]{0} %multiply.940), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.945 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.946 = f32[16]{0} multiply(f32[16]{0} %multiply.944, f32[16]{0} %broadcast.945), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.947 = f32[16]{0} add(f32[16]{0} %multiply.943, f32[16]{0} %multiply.946), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.948 = f32[16]{0} sqrt(f32[16]{0} %add.947), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.949 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.950 = f32[16]{0} add(f32[16]{0} %sqrt.948, f32[16]{0} %broadcast.949), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.961 = f32[16]{0} divide(f32[16]{0} %add.959, f32[16]{0} %add.950), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.962 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.963 = f32[16]{0} multiply(f32[16]{0} %divide.961, f32[16]{0} %broadcast.962), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.964 = f32[16]{0} add(f32[16]{0} %p81.960, f32[16]{0} %multiply.963), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p84.988 = f32[16]{0} parameter(84), frontend_attributes={neff_input_names="input84"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p83.984 = f32[16]{0} parameter(83), frontend_attributes={neff_input_names="input83"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.985 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.986 = f32[16]{0} multiply(f32[16]{0} %p83.984, f32[16]{0} %broadcast.985), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.30 = f32[] multiply(f32[] %select.533, f32[] %constant.137), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.16 = f32[16]{0} broadcast(f32[] %multiply.30), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.968 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.136, f32[16]{0} %broadcast.16), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.982 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.983 = f32[16]{0} multiply(f32[16]{0} %multiply.968, f32[16]{0} %broadcast.982), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.987 = f32[16]{0} add(f32[16]{0} %multiply.986, f32[16]{0} %multiply.983), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p82.969 = f32[16]{0} parameter(82), frontend_attributes={neff_input_names="input82"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.970 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.971 = f32[16]{0} multiply(f32[16]{0} %p82.969, f32[16]{0} %broadcast.970), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.972 = f32[16]{0} multiply(f32[16]{0} %multiply.968, f32[16]{0} %multiply.968), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.973 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.974 = f32[16]{0} multiply(f32[16]{0} %multiply.972, f32[16]{0} %broadcast.973), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.975 = f32[16]{0} add(f32[16]{0} %multiply.971, f32[16]{0} %multiply.974), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.976 = f32[16]{0} sqrt(f32[16]{0} %add.975), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.977 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.978 = f32[16]{0} add(f32[16]{0} %sqrt.976, f32[16]{0} %broadcast.977), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.989 = f32[16]{0} divide(f32[16]{0} %add.987, f32[16]{0} %add.978), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.990 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.991 = f32[16]{0} multiply(f32[16]{0} %divide.989, f32[16]{0} %broadcast.990), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.992 = f32[16]{0} add(f32[16]{0} %p84.988, f32[16]{0} %multiply.991), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p87.1020 = f32[4096,16]{1,0} parameter(87), frontend_attributes={neff_input_names="input87"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p86.1016 = f32[4096,16]{1,0} parameter(86), frontend_attributes={neff_input_names="input86"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1017 = f32[4096,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1018 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %p86.1016, f32[4096,16]{1,0} %broadcast.1017), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.32 = f32[] multiply(f32[] %select.533, f32[] %constant.185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.18 = f32[4096,16]{1,0} broadcast(f32[] %multiply.32), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1000 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %get-tuple-element.184, f32[4096,16]{1,0} %broadcast.18), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1014 = f32[4096,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1015 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.1000, f32[4096,16]{1,0} %broadcast.1014), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1019 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %multiply.1018, f32[4096,16]{1,0} %multiply.1015), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p85.1001 = f32[4096,16]{1,0} parameter(85), frontend_attributes={neff_input_names="input85"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1002 = f32[4096,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1003 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %p85.1001, f32[4096,16]{1,0} %broadcast.1002), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1004 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.1000, f32[4096,16]{1,0} %multiply.1000), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1005 = f32[4096,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1006 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.1004, f32[4096,16]{1,0} %broadcast.1005), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1007 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %multiply.1003, f32[4096,16]{1,0} %multiply.1006), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1008 = f32[4096,16]{1,0} sqrt(f32[4096,16]{1,0} %add.1007), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1009 = f32[4096,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1010 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %sqrt.1008, f32[4096,16]{1,0} %broadcast.1009), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1021 = f32[4096,16]{1,0} divide(f32[4096,16]{1,0} %add.1019, f32[4096,16]{1,0} %add.1010), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1022 = f32[4096,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1023 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %divide.1021, f32[4096,16]{1,0} %broadcast.1022), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1024 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %p87.1020, f32[4096,16]{1,0} %multiply.1023), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.996 = f32[4096,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1025 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %add.1024, f32[4096,16]{1,0} %broadcast.996), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1026 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %add.1024, f32[4096,16]{1,0} %multiply.1025), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p90.1050 = f32[4096]{0} parameter(90), frontend_attributes={neff_input_names="input90"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p89.1046 = f32[4096]{0} parameter(89), frontend_attributes={neff_input_names="input89"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1047 = f32[4096]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1048 = f32[4096]{0} multiply(f32[4096]{0} %p89.1046, f32[4096]{0} %broadcast.1047), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.34 = f32[] multiply(f32[] %select.533, f32[] %constant.133), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.19 = f32[4096]{0} broadcast(f32[] %multiply.34), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1030 = f32[4096]{0} multiply(f32[4096]{0} %get-tuple-element.132, f32[4096]{0} %broadcast.19), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1044 = f32[4096]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1045 = f32[4096]{0} multiply(f32[4096]{0} %multiply.1030, f32[4096]{0} %broadcast.1044), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1049 = f32[4096]{0} add(f32[4096]{0} %multiply.1048, f32[4096]{0} %multiply.1045), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p88.1031 = f32[4096]{0} parameter(88), frontend_attributes={neff_input_names="input88"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1032 = f32[4096]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1033 = f32[4096]{0} multiply(f32[4096]{0} %p88.1031, f32[4096]{0} %broadcast.1032), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1034 = f32[4096]{0} multiply(f32[4096]{0} %multiply.1030, f32[4096]{0} %multiply.1030), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1035 = f32[4096]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1036 = f32[4096]{0} multiply(f32[4096]{0} %multiply.1034, f32[4096]{0} %broadcast.1035), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1037 = f32[4096]{0} add(f32[4096]{0} %multiply.1033, f32[4096]{0} %multiply.1036), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1038 = f32[4096]{0} sqrt(f32[4096]{0} %add.1037), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1039 = f32[4096]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1040 = f32[4096]{0} add(f32[4096]{0} %sqrt.1038, f32[4096]{0} %broadcast.1039), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1051 = f32[4096]{0} divide(f32[4096]{0} %add.1049, f32[4096]{0} %add.1040), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1052 = f32[4096]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1053 = f32[4096]{0} multiply(f32[4096]{0} %divide.1051, f32[4096]{0} %broadcast.1052), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1054 = f32[4096]{0} add(f32[4096]{0} %p90.1050, f32[4096]{0} %multiply.1053), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p93.1082 = f32[16,4096]{1,0} parameter(93), frontend_attributes={neff_input_names="input93"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p92.1078 = f32[16,4096]{1,0} parameter(92), frontend_attributes={neff_input_names="input92"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1079 = f32[16,4096]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1080 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %p92.1078, f32[16,4096]{1,0} %broadcast.1079), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.36 = f32[] multiply(f32[] %select.533, f32[] %constant.181), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.20 = f32[16,4096]{1,0} broadcast(f32[] %multiply.36), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1062 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %get-tuple-element.180, f32[16,4096]{1,0} %broadcast.20), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1076 = f32[16,4096]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1077 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.1062, f32[16,4096]{1,0} %broadcast.1076), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1081 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %multiply.1080, f32[16,4096]{1,0} %multiply.1077), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p91.1063 = f32[16,4096]{1,0} parameter(91), frontend_attributes={neff_input_names="input91"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1064 = f32[16,4096]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1065 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %p91.1063, f32[16,4096]{1,0} %broadcast.1064), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1066 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.1062, f32[16,4096]{1,0} %multiply.1062), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1067 = f32[16,4096]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1068 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.1066, f32[16,4096]{1,0} %broadcast.1067), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1069 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %multiply.1065, f32[16,4096]{1,0} %multiply.1068), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1070 = f32[16,4096]{1,0} sqrt(f32[16,4096]{1,0} %add.1069), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1071 = f32[16,4096]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1072 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %sqrt.1070, f32[16,4096]{1,0} %broadcast.1071), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1083 = f32[16,4096]{1,0} divide(f32[16,4096]{1,0} %add.1081, f32[16,4096]{1,0} %add.1072), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1084 = f32[16,4096]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1085 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %divide.1083, f32[16,4096]{1,0} %broadcast.1084), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1086 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %p93.1082, f32[16,4096]{1,0} %multiply.1085), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1058 = f32[16,4096]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1087 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %add.1086, f32[16,4096]{1,0} %broadcast.1058), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1088 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %add.1086, f32[16,4096]{1,0} %multiply.1087), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p96.1112 = f32[16]{0} parameter(96), frontend_attributes={neff_input_names="input96"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p95.1108 = f32[16]{0} parameter(95), frontend_attributes={neff_input_names="input95"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1109 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1110 = f32[16]{0} multiply(f32[16]{0} %p95.1108, f32[16]{0} %broadcast.1109), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.38 = f32[] multiply(f32[] %select.533, f32[] %constant.129), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.21 = f32[16]{0} broadcast(f32[] %multiply.38), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1092 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.128, f32[16]{0} %broadcast.21), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1106 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1107 = f32[16]{0} multiply(f32[16]{0} %multiply.1092, f32[16]{0} %broadcast.1106), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1111 = f32[16]{0} add(f32[16]{0} %multiply.1110, f32[16]{0} %multiply.1107), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p94.1093 = f32[16]{0} parameter(94), frontend_attributes={neff_input_names="input94"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1094 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1095 = f32[16]{0} multiply(f32[16]{0} %p94.1093, f32[16]{0} %broadcast.1094), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1096 = f32[16]{0} multiply(f32[16]{0} %multiply.1092, f32[16]{0} %multiply.1092), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1097 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1098 = f32[16]{0} multiply(f32[16]{0} %multiply.1096, f32[16]{0} %broadcast.1097), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1099 = f32[16]{0} add(f32[16]{0} %multiply.1095, f32[16]{0} %multiply.1098), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1100 = f32[16]{0} sqrt(f32[16]{0} %add.1099), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1101 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1102 = f32[16]{0} add(f32[16]{0} %sqrt.1100, f32[16]{0} %broadcast.1101), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1113 = f32[16]{0} divide(f32[16]{0} %add.1111, f32[16]{0} %add.1102), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1114 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1115 = f32[16]{0} multiply(f32[16]{0} %divide.1113, f32[16]{0} %broadcast.1114), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1116 = f32[16]{0} add(f32[16]{0} %p96.1112, f32[16]{0} %multiply.1115), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p99.1140 = f32[16]{0} parameter(99), frontend_attributes={neff_input_names="input99"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p98.1136 = f32[16]{0} parameter(98), frontend_attributes={neff_input_names="input98"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1137 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1138 = f32[16]{0} multiply(f32[16]{0} %p98.1136, f32[16]{0} %broadcast.1137), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.40 = f32[] multiply(f32[] %select.533, f32[] %constant.125), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.22 = f32[16]{0} broadcast(f32[] %multiply.40), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1120 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.124, f32[16]{0} %broadcast.22), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1134 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1135 = f32[16]{0} multiply(f32[16]{0} %multiply.1120, f32[16]{0} %broadcast.1134), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1139 = f32[16]{0} add(f32[16]{0} %multiply.1138, f32[16]{0} %multiply.1135), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p97.1121 = f32[16]{0} parameter(97), frontend_attributes={neff_input_names="input97"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1122 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1123 = f32[16]{0} multiply(f32[16]{0} %p97.1121, f32[16]{0} %broadcast.1122), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1124 = f32[16]{0} multiply(f32[16]{0} %multiply.1120, f32[16]{0} %multiply.1120), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1125 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1126 = f32[16]{0} multiply(f32[16]{0} %multiply.1124, f32[16]{0} %broadcast.1125), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1127 = f32[16]{0} add(f32[16]{0} %multiply.1123, f32[16]{0} %multiply.1126), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1128 = f32[16]{0} sqrt(f32[16]{0} %add.1127), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1129 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1130 = f32[16]{0} add(f32[16]{0} %sqrt.1128, f32[16]{0} %broadcast.1129), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1141 = f32[16]{0} divide(f32[16]{0} %add.1139, f32[16]{0} %add.1130), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1142 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1143 = f32[16]{0} multiply(f32[16]{0} %divide.1141, f32[16]{0} %broadcast.1142), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1144 = f32[16]{0} add(f32[16]{0} %p99.1140, f32[16]{0} %multiply.1143), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p102.1168 = f32[16]{0} parameter(102), frontend_attributes={neff_input_names="input102"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p101.1164 = f32[16]{0} parameter(101), frontend_attributes={neff_input_names="input101"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1165 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1166 = f32[16]{0} multiply(f32[16]{0} %p101.1164, f32[16]{0} %broadcast.1165), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.42 = f32[] multiply(f32[] %select.533, f32[] %constant.121), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.23 = f32[16]{0} broadcast(f32[] %multiply.42), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1148 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.120, f32[16]{0} %broadcast.23), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1162 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1163 = f32[16]{0} multiply(f32[16]{0} %multiply.1148, f32[16]{0} %broadcast.1162), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1167 = f32[16]{0} add(f32[16]{0} %multiply.1166, f32[16]{0} %multiply.1163), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p100.1149 = f32[16]{0} parameter(100), frontend_attributes={neff_input_names="input100"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1150 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1151 = f32[16]{0} multiply(f32[16]{0} %p100.1149, f32[16]{0} %broadcast.1150), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1152 = f32[16]{0} multiply(f32[16]{0} %multiply.1148, f32[16]{0} %multiply.1148), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1153 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1154 = f32[16]{0} multiply(f32[16]{0} %multiply.1152, f32[16]{0} %broadcast.1153), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1155 = f32[16]{0} add(f32[16]{0} %multiply.1151, f32[16]{0} %multiply.1154), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1156 = f32[16]{0} sqrt(f32[16]{0} %add.1155), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1157 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1158 = f32[16]{0} add(f32[16]{0} %sqrt.1156, f32[16]{0} %broadcast.1157), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1169 = f32[16]{0} divide(f32[16]{0} %add.1167, f32[16]{0} %add.1158), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1170 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1171 = f32[16]{0} multiply(f32[16]{0} %divide.1169, f32[16]{0} %broadcast.1170), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1172 = f32[16]{0} add(f32[16]{0} %p102.1168, f32[16]{0} %multiply.1171), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p105.1200 = f32[16,16]{1,0} parameter(105), frontend_attributes={neff_input_names="input105"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p104.1196 = f32[16,16]{1,0} parameter(104), frontend_attributes={neff_input_names="input104"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1197 = f32[16,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1198 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p104.1196, f32[16,16]{1,0} %broadcast.1197), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.44 = f32[] multiply(f32[] %select.533, f32[] %constant.177), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.24 = f32[16,16]{1,0} broadcast(f32[] %multiply.44), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1180 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.176, f32[16,16]{1,0} %broadcast.24), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1194 = f32[16,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1195 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1180, f32[16,16]{1,0} %broadcast.1194), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1199 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.1198, f32[16,16]{1,0} %multiply.1195), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p103.1181 = f32[16,16]{1,0} parameter(103), frontend_attributes={neff_input_names="input103"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1182 = f32[16,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1183 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p103.1181, f32[16,16]{1,0} %broadcast.1182), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1184 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1180, f32[16,16]{1,0} %multiply.1180), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1185 = f32[16,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1186 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1184, f32[16,16]{1,0} %broadcast.1185), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1187 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.1183, f32[16,16]{1,0} %multiply.1186), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1188 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.1187), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1189 = f32[16,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1190 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.1188, f32[16,16]{1,0} %broadcast.1189), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1201 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.1199, f32[16,16]{1,0} %add.1190), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1202 = f32[16,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1203 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.1201, f32[16,16]{1,0} %broadcast.1202), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1204 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p105.1200, f32[16,16]{1,0} %multiply.1203), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1176 = f32[16,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1205 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.1204, f32[16,16]{1,0} %broadcast.1176), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1206 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.1204, f32[16,16]{1,0} %multiply.1205), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p108.1230 = f32[16]{0} parameter(108), frontend_attributes={neff_input_names="input108"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p107.1226 = f32[16]{0} parameter(107), frontend_attributes={neff_input_names="input107"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1227 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1228 = f32[16]{0} multiply(f32[16]{0} %p107.1226, f32[16]{0} %broadcast.1227), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.46 = f32[] multiply(f32[] %select.533, f32[] %constant.117), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.25 = f32[16]{0} broadcast(f32[] %multiply.46), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1210 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.116, f32[16]{0} %broadcast.25), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1224 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1225 = f32[16]{0} multiply(f32[16]{0} %multiply.1210, f32[16]{0} %broadcast.1224), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1229 = f32[16]{0} add(f32[16]{0} %multiply.1228, f32[16]{0} %multiply.1225), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p106.1211 = f32[16]{0} parameter(106), frontend_attributes={neff_input_names="input106"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1212 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1213 = f32[16]{0} multiply(f32[16]{0} %p106.1211, f32[16]{0} %broadcast.1212), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1214 = f32[16]{0} multiply(f32[16]{0} %multiply.1210, f32[16]{0} %multiply.1210), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1215 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1216 = f32[16]{0} multiply(f32[16]{0} %multiply.1214, f32[16]{0} %broadcast.1215), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1217 = f32[16]{0} add(f32[16]{0} %multiply.1213, f32[16]{0} %multiply.1216), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1218 = f32[16]{0} sqrt(f32[16]{0} %add.1217), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1219 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1220 = f32[16]{0} add(f32[16]{0} %sqrt.1218, f32[16]{0} %broadcast.1219), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1231 = f32[16]{0} divide(f32[16]{0} %add.1229, f32[16]{0} %add.1220), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1232 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1233 = f32[16]{0} multiply(f32[16]{0} %divide.1231, f32[16]{0} %broadcast.1232), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1234 = f32[16]{0} add(f32[16]{0} %p108.1230, f32[16]{0} %multiply.1233), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p111.1262 = f32[16,16]{1,0} parameter(111), frontend_attributes={neff_input_names="input111"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p110.1258 = f32[16,16]{1,0} parameter(110), frontend_attributes={neff_input_names="input110"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1259 = f32[16,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1260 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p110.1258, f32[16,16]{1,0} %broadcast.1259), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.48 = f32[] multiply(f32[] %select.533, f32[] %constant.173), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.26 = f32[16,16]{1,0} broadcast(f32[] %multiply.48), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1242 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.172, f32[16,16]{1,0} %broadcast.26), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1256 = f32[16,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1257 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1242, f32[16,16]{1,0} %broadcast.1256), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1261 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.1260, f32[16,16]{1,0} %multiply.1257), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p109.1243 = f32[16,16]{1,0} parameter(109), frontend_attributes={neff_input_names="input109"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1244 = f32[16,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1245 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %p109.1243, f32[16,16]{1,0} %broadcast.1244), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1246 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1242, f32[16,16]{1,0} %multiply.1242), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1247 = f32[16,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1248 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1246, f32[16,16]{1,0} %broadcast.1247), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1249 = f32[16,16]{1,0} add(f32[16,16]{1,0} %multiply.1245, f32[16,16]{1,0} %multiply.1248), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1250 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.1249), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1251 = f32[16,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1252 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.1250, f32[16,16]{1,0} %broadcast.1251), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1263 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.1261, f32[16,16]{1,0} %add.1252), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1264 = f32[16,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1265 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.1263, f32[16,16]{1,0} %broadcast.1264), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1266 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p111.1262, f32[16,16]{1,0} %multiply.1265), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1238 = f32[16,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1267 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.1266, f32[16,16]{1,0} %broadcast.1238), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1268 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.1266, f32[16,16]{1,0} %multiply.1267), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p114.1292 = f32[16]{0} parameter(114), frontend_attributes={neff_input_names="input114"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p113.1288 = f32[16]{0} parameter(113), frontend_attributes={neff_input_names="input113"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1289 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1290 = f32[16]{0} multiply(f32[16]{0} %p113.1288, f32[16]{0} %broadcast.1289), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.50 = f32[] multiply(f32[] %select.533, f32[] %constant.109), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.27 = f32[16]{0} broadcast(f32[] %multiply.50), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1272 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.108, f32[16]{0} %broadcast.27), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1286 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1287 = f32[16]{0} multiply(f32[16]{0} %multiply.1272, f32[16]{0} %broadcast.1286), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1291 = f32[16]{0} add(f32[16]{0} %multiply.1290, f32[16]{0} %multiply.1287), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p112.1273 = f32[16]{0} parameter(112), frontend_attributes={neff_input_names="input112"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1274 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1275 = f32[16]{0} multiply(f32[16]{0} %p112.1273, f32[16]{0} %broadcast.1274), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1276 = f32[16]{0} multiply(f32[16]{0} %multiply.1272, f32[16]{0} %multiply.1272), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1277 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1278 = f32[16]{0} multiply(f32[16]{0} %multiply.1276, f32[16]{0} %broadcast.1277), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1279 = f32[16]{0} add(f32[16]{0} %multiply.1275, f32[16]{0} %multiply.1278), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1280 = f32[16]{0} sqrt(f32[16]{0} %add.1279), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1281 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1282 = f32[16]{0} add(f32[16]{0} %sqrt.1280, f32[16]{0} %broadcast.1281), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1293 = f32[16]{0} divide(f32[16]{0} %add.1291, f32[16]{0} %add.1282), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1294 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1295 = f32[16]{0} multiply(f32[16]{0} %divide.1293, f32[16]{0} %broadcast.1294), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1296 = f32[16]{0} add(f32[16]{0} %p114.1292, f32[16]{0} %multiply.1295), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p117.1320 = f32[16]{0} parameter(117), frontend_attributes={neff_input_names="input117"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p116.1316 = f32[16]{0} parameter(116), frontend_attributes={neff_input_names="input116"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1317 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1318 = f32[16]{0} multiply(f32[16]{0} %p116.1316, f32[16]{0} %broadcast.1317), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.52 = f32[] multiply(f32[] %select.533, f32[] %constant.105), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.28 = f32[16]{0} broadcast(f32[] %multiply.52), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1300 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.104, f32[16]{0} %broadcast.28), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1314 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1315 = f32[16]{0} multiply(f32[16]{0} %multiply.1300, f32[16]{0} %broadcast.1314), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1319 = f32[16]{0} add(f32[16]{0} %multiply.1318, f32[16]{0} %multiply.1315), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p115.1301 = f32[16]{0} parameter(115), frontend_attributes={neff_input_names="input115"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1302 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1303 = f32[16]{0} multiply(f32[16]{0} %p115.1301, f32[16]{0} %broadcast.1302), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1304 = f32[16]{0} multiply(f32[16]{0} %multiply.1300, f32[16]{0} %multiply.1300), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1305 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1306 = f32[16]{0} multiply(f32[16]{0} %multiply.1304, f32[16]{0} %broadcast.1305), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1307 = f32[16]{0} add(f32[16]{0} %multiply.1303, f32[16]{0} %multiply.1306), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1308 = f32[16]{0} sqrt(f32[16]{0} %add.1307), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1309 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1310 = f32[16]{0} add(f32[16]{0} %sqrt.1308, f32[16]{0} %broadcast.1309), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1321 = f32[16]{0} divide(f32[16]{0} %add.1319, f32[16]{0} %add.1310), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1322 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1323 = f32[16]{0} multiply(f32[16]{0} %divide.1321, f32[16]{0} %broadcast.1322), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1324 = f32[16]{0} add(f32[16]{0} %p117.1320, f32[16]{0} %multiply.1323), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p120.1348 = f32[16]{0} parameter(120), frontend_attributes={neff_input_names="input120"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p119.1344 = f32[16]{0} parameter(119), frontend_attributes={neff_input_names="input119"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1345 = f32[16]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1346 = f32[16]{0} multiply(f32[16]{0} %p119.1344, f32[16]{0} %broadcast.1345), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.54 = f32[] multiply(f32[] %select.533, f32[] %constant.101), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.29 = f32[16]{0} broadcast(f32[] %multiply.54), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1328 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.100, f32[16]{0} %broadcast.29), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1342 = f32[16]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1343 = f32[16]{0} multiply(f32[16]{0} %multiply.1328, f32[16]{0} %broadcast.1342), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1347 = f32[16]{0} add(f32[16]{0} %multiply.1346, f32[16]{0} %multiply.1343), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p118.1329 = f32[16]{0} parameter(118), frontend_attributes={neff_input_names="input118"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1330 = f32[16]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1331 = f32[16]{0} multiply(f32[16]{0} %p118.1329, f32[16]{0} %broadcast.1330), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1332 = f32[16]{0} multiply(f32[16]{0} %multiply.1328, f32[16]{0} %multiply.1328), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1333 = f32[16]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1334 = f32[16]{0} multiply(f32[16]{0} %multiply.1332, f32[16]{0} %broadcast.1333), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1335 = f32[16]{0} add(f32[16]{0} %multiply.1331, f32[16]{0} %multiply.1334), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1336 = f32[16]{0} sqrt(f32[16]{0} %add.1335), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1337 = f32[16]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1338 = f32[16]{0} add(f32[16]{0} %sqrt.1336, f32[16]{0} %broadcast.1337), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1349 = f32[16]{0} divide(f32[16]{0} %add.1347, f32[16]{0} %add.1338), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1350 = f32[16]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1351 = f32[16]{0} multiply(f32[16]{0} %divide.1349, f32[16]{0} %broadcast.1350), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1352 = f32[16]{0} add(f32[16]{0} %p120.1348, f32[16]{0} %multiply.1351), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p123.1376 = f32[30522]{0} parameter(123), frontend_attributes={neff_input_names="input123"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p122.1372 = f32[30522]{0} parameter(122), frontend_attributes={neff_input_names="input122"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1373 = f32[30522]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1374 = f32[30522]{0} multiply(f32[30522]{0} %p122.1372, f32[30522]{0} %broadcast.1373), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.56 = f32[] multiply(f32[] %select.533, f32[] %constant.113), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.30 = f32[30522]{0} broadcast(f32[] %multiply.56), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1356 = f32[30522]{0} multiply(f32[30522]{0} %get-tuple-element.112, f32[30522]{0} %broadcast.30), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1370 = f32[30522]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1371 = f32[30522]{0} multiply(f32[30522]{0} %multiply.1356, f32[30522]{0} %broadcast.1370), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1375 = f32[30522]{0} add(f32[30522]{0} %multiply.1374, f32[30522]{0} %multiply.1371), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p121.1357 = f32[30522]{0} parameter(121), frontend_attributes={neff_input_names="input121"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1358 = f32[30522]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1359 = f32[30522]{0} multiply(f32[30522]{0} %p121.1357, f32[30522]{0} %broadcast.1358), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1360 = f32[30522]{0} multiply(f32[30522]{0} %multiply.1356, f32[30522]{0} %multiply.1356), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1361 = f32[30522]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1362 = f32[30522]{0} multiply(f32[30522]{0} %multiply.1360, f32[30522]{0} %broadcast.1361), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1363 = f32[30522]{0} add(f32[30522]{0} %multiply.1359, f32[30522]{0} %multiply.1362), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1364 = f32[30522]{0} sqrt(f32[30522]{0} %add.1363), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1365 = f32[30522]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1366 = f32[30522]{0} add(f32[30522]{0} %sqrt.1364, f32[30522]{0} %broadcast.1365), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1377 = f32[30522]{0} divide(f32[30522]{0} %add.1375, f32[30522]{0} %add.1366), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1378 = f32[30522]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1379 = f32[30522]{0} multiply(f32[30522]{0} %divide.1377, f32[30522]{0} %broadcast.1378), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1380 = f32[30522]{0} add(f32[30522]{0} %p123.1376, f32[30522]{0} %multiply.1379), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p126.1408 = f32[2,16]{1,0} parameter(126), frontend_attributes={neff_input_names="input126"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p125.1404 = f32[2,16]{1,0} parameter(125), frontend_attributes={neff_input_names="input125"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1405 = f32[2,16]{1,0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1406 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %p125.1404, f32[2,16]{1,0} %broadcast.1405), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.58 = f32[] multiply(f32[] %select.533, f32[] %constant.169), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.31 = f32[2,16]{1,0} broadcast(f32[] %multiply.58), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1388 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.168, f32[2,16]{1,0} %broadcast.31), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1402 = f32[2,16]{1,0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1403 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.1388, f32[2,16]{1,0} %broadcast.1402), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1407 = f32[2,16]{1,0} add(f32[2,16]{1,0} %multiply.1406, f32[2,16]{1,0} %multiply.1403), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p124.1389 = f32[2,16]{1,0} parameter(124), frontend_attributes={neff_input_names="input124"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1390 = f32[2,16]{1,0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1391 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %p124.1389, f32[2,16]{1,0} %broadcast.1390), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1392 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.1388, f32[2,16]{1,0} %multiply.1388), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1393 = f32[2,16]{1,0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1394 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.1392, f32[2,16]{1,0} %broadcast.1393), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1395 = f32[2,16]{1,0} add(f32[2,16]{1,0} %multiply.1391, f32[2,16]{1,0} %multiply.1394), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1396 = f32[2,16]{1,0} sqrt(f32[2,16]{1,0} %add.1395), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1397 = f32[2,16]{1,0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1398 = f32[2,16]{1,0} add(f32[2,16]{1,0} %sqrt.1396, f32[2,16]{1,0} %broadcast.1397), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1409 = f32[2,16]{1,0} divide(f32[2,16]{1,0} %add.1407, f32[2,16]{1,0} %add.1398), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1410 = f32[2,16]{1,0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1411 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %divide.1409, f32[2,16]{1,0} %broadcast.1410), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1412 = f32[2,16]{1,0} add(f32[2,16]{1,0} %p126.1408, f32[2,16]{1,0} %multiply.1411), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1384 = f32[2,16]{1,0} broadcast(f32[] %p0.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1413 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %add.1412, f32[2,16]{1,0} %broadcast.1384), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1414 = f32[2,16]{1,0} add(f32[2,16]{1,0} %add.1412, f32[2,16]{1,0} %multiply.1413), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p129.1438 = f32[2]{0} parameter(129), frontend_attributes={neff_input_names="input129"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %p128.1434 = f32[2]{0} parameter(128), frontend_attributes={neff_input_names="input128"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.1435 = f32[2]{0} broadcast(f32[] %p40.553), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1436 = f32[2]{0} multiply(f32[2]{0} %p128.1434, f32[2]{0} %broadcast.1435), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.60 = f32[] multiply(f32[] %select.533, f32[] %constant.97), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.32 = f32[2]{0} broadcast(f32[] %multiply.60), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1418 = f32[2]{0} multiply(f32[2]{0} %get-tuple-element.96, f32[2]{0} %broadcast.32), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1432 = f32[2]{0} broadcast(f32[] %p39.547), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1433 = f32[2]{0} multiply(f32[2]{0} %multiply.1418, f32[2]{0} %broadcast.1432), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1437 = f32[2]{0} add(f32[2]{0} %multiply.1436, f32[2]{0} %multiply.1433), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %p127.1419 = f32[2]{0} parameter(127), frontend_attributes={neff_input_names="input127"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1420 = f32[2]{0} broadcast(f32[] %p37.536), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1421 = f32[2]{0} multiply(f32[2]{0} %p127.1419, f32[2]{0} %broadcast.1420), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1422 = f32[2]{0} multiply(f32[2]{0} %multiply.1418, f32[2]{0} %multiply.1418), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1423 = f32[2]{0} broadcast(f32[] %p3.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1424 = f32[2]{0} multiply(f32[2]{0} %multiply.1422, f32[2]{0} %broadcast.1423), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1425 = f32[2]{0} add(f32[2]{0} %multiply.1421, f32[2]{0} %multiply.1424), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1426 = f32[2]{0} sqrt(f32[2]{0} %add.1425), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1427 = f32[2]{0} broadcast(f32[] %p2.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1428 = f32[2]{0} add(f32[2]{0} %sqrt.1426, f32[2]{0} %broadcast.1427), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1439 = f32[2]{0} divide(f32[2]{0} %add.1437, f32[2]{0} %add.1428), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1440 = f32[2]{0} broadcast(f32[] %p1.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1441 = f32[2]{0} multiply(f32[2]{0} %divide.1439, f32[2]{0} %broadcast.1440), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1442 = f32[2]{0} add(f32[2]{0} %p129.1438, f32[2]{0} %multiply.1441), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1 = f32[1]{0} constant({0})
  %get-tuple-element.27 = f32[1]{0} get-tuple-element((f32[1]{0}, f32[]) %all-reduce.26), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %constant.2 = f32[1]{0} constant({0})
  ROOT %tuple.1449 = (f32[30522,16]{1,0}, f32[512,16]{1,0}, f32[2,16]{1,0}, f32[16]{0}, f32[16]{0}, /*index=5*/f32[16,16]{1,0}, f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, f32[16,16]{1,0}, /*index=10*/f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[4096,16]{1,0}, f32[4096]{0}, f32[16,4096]{1,0}, f32[16]{0}, f32[16]{0}, /*index=20*/f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, /*index=25*/f32[16]{0}, f32[16]{0}, f32[30522]{0}, f32[2,16]{1,0}, f32[2]{0}, /*index=30*/f32[1]{0}, f32[30522,16]{1,0}, f32[30522,16]{1,0}, f32[512,16]{1,0}, f32[512,16]{1,0}, /*index=35*/f32[2,16]{1,0}, f32[2,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=40*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=45*/f32[4096,16]{1,0}, f32[4096,16]{1,0}, f32[16,4096]{1,0}, f32[16,4096]{1,0}, f32[16,16]{1,0}, /*index=50*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[2,16]{1,0}, /*index=55*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=60*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=65*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=70*/f32[16]{0}, f32[4096]{0}, f32[4096]{0}, f32[16]{0}, f32[16]{0}, /*index=75*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=80*/f32[16]{0}, f32[30522]{0}, f32[30522]{0}, f32[16]{0}, f32[16]{0}, /*index=85*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2]{0}, /*index=90*/f32[2]{0}, f32[1]{0}, f32[1]{0}, f32[1]{0}) tuple(f32[30522,16]{1,0} %add.564, f32[512,16]{1,0} %add.598, f32[2,16]{1,0} %add.632, f32[16]{0} %add.660, f32[16]{0} %add.688, /*index=5*/f32[16,16]{1,0} %add.722, f32[16]{0} %add.750, f32[16,16]{1,0} %add.784, f32[16]{0} %add.812, f32[16,16]{1,0} %add.846, /*index=10*/f32[16]{0} %add.874, f32[16,16]{1,0} %add.908, f32[16]{0} %add.936, f32[16]{0} %add.964, f32[16]{0} %add.992, /*index=15*/f32[4096,16]{1,0} %add.1026, f32[4096]{0} %add.1054, f32[16,4096]{1,0} %add.1088, f32[16]{0} %add.1116, f32[16]{0} %add.1144, /*index=20*/f32[16]{0} %add.1172, f32[16,16]{1,0} %add.1206, f32[16]{0} %add.1234, f32[16,16]{1,0} %add.1268, f32[16]{0} %add.1296, /*index=25*/f32[16]{0} %add.1324, f32[16]{0} %add.1352, f32[30522]{0} %add.1380, f32[2,16]{1,0} %add.1414, f32[2]{0} %add.1442, /*index=30*/f32[1]{0} %constant.1, f32[30522,16]{1,0} %add.557, f32[30522,16]{1,0} %add.543, f32[512,16]{1,0} %add.591, f32[512,16]{1,0} %add.579, /*index=35*/f32[2,16]{1,0} %add.625, f32[2,16]{1,0} %add.613, f32[16,16]{1,0} %add.715, f32[16,16]{1,0} %add.703, f32[16,16]{1,0} %add.777, /*index=40*/f32[16,16]{1,0} %add.765, f32[16,16]{1,0} %add.839, f32[16,16]{1,0} %add.827, f32[16,16]{1,0} %add.901, f32[16,16]{1,0} %add.889, /*index=45*/f32[4096,16]{1,0} %add.1019, f32[4096,16]{1,0} %add.1007, f32[16,4096]{1,0} %add.1081, f32[16,4096]{1,0} %add.1069, f32[16,16]{1,0} %add.1199, /*index=50*/f32[16,16]{1,0} %add.1187, f32[16,16]{1,0} %add.1261, f32[16,16]{1,0} %add.1249, f32[2,16]{1,0} %add.1407, f32[2,16]{1,0} %add.1395, /*index=55*/f32[16]{0} %add.655, f32[16]{0} %add.643, f32[16]{0} %add.683, f32[16]{0} %add.671, f32[16]{0} %add.745, /*index=60*/f32[16]{0} %add.733, f32[16]{0} %add.807, f32[16]{0} %add.795, f32[16]{0} %add.869, f32[16]{0} %add.857, /*index=65*/f32[16]{0} %add.931, f32[16]{0} %add.919, f32[16]{0} %add.959, f32[16]{0} %add.947, f32[16]{0} %add.987, /*index=70*/f32[16]{0} %add.975, f32[4096]{0} %add.1049, f32[4096]{0} %add.1037, f32[16]{0} %add.1111, f32[16]{0} %add.1099, /*index=75*/f32[16]{0} %add.1139, f32[16]{0} %add.1127, f32[16]{0} %add.1167, f32[16]{0} %add.1155, f32[16]{0} %add.1229, /*index=80*/f32[16]{0} %add.1217, f32[30522]{0} %add.1375, f32[30522]{0} %add.1363, f32[16]{0} %add.1291, f32[16]{0} %add.1279, /*index=85*/f32[16]{0} %add.1319, f32[16]{0} %add.1307, f32[16]{0} %add.1347, f32[16]{0} %add.1335, f32[2]{0} %add.1437, /*index=90*/f32[2]{0} %add.1425, f32[1]{0} %divide.18, f32[1]{0} %get-tuple-element.27, f32[1]{0} %constant.2), frontend_attributes={neff_output_names="output0,output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12,output13,output14,output15,output16,output17,output18,output19,output20,output21,output22,output23,output24,output25,output26,output27,output28,output29,output30,output31,output32,output33,output34,output35,output36,output37,output38,output39,output40,output41,output42,output43,output44,output45,output46,output47,output48,output49,output50,output51,output52,output53,output54,output55,output56,output57,output58,output59,output60,output61,output62,output63,output64,output65,output66,output67,output68,output69,output70,output71,output72,output73,output74,output75,output76,output77,output78,output79,output80,output81,output82,output83,output84,output85,output86,output87,output88,output89,output90,output91,output92,output93"}
}

`

export default text;
