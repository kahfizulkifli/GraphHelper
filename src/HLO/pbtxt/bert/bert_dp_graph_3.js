const text = `
HloModule SyncTensorsGraph.1730, input_output_alias={ {0}: (38, {}, must-alias), {1}: (39, {}, must-alias), {2}: (40, {}, must-alias), {3}: (41, {}, must-alias), {4}: (42, {}, must-alias), {5}: (43, {}, must-alias), {6}: (44, {}, must-alias), {7}: (45, {}, must-alias), {8}: (46, {}, must-alias), {9}: (47, {}, must-alias), {10}: (48, {}, must-alias), {11}: (49, {}, must-alias), {12}: (50, {}, must-alias), {13}: (51, {}, must-alias), {14}: (52, {}, must-alias), {15}: (53, {}, must-alias), {16}: (54, {}, must-alias), {17}: (55, {}, must-alias), {18}: (56, {}, must-alias), {19}: (57, {}, must-alias), {20}: (58, {}, must-alias), {21}: (59, {}, must-alias), {22}: (60, {}, must-alias), {23}: (61, {}, must-alias), {24}: (62, {}, must-alias), {25}: (63, {}, must-alias), {26}: (64, {}, must-alias), {27}: (65, {}, must-alias), {28}: (66, {}, must-alias), {29}: (67, {}, must-alias), {30}: (4, {}, must-alias) }

%AddComputation.22 (x.23: f32[], y.24: f32[]) -> f32[] {
  %x.23 = f32[] parameter(0)
  %y.24 = f32[] parameter(1)
  ROOT %add.25 = f32[] add(f32[] %x.23, f32[] %y.24)
}

%AddComputation.91 (x.92: f32[], y.93: f32[]) -> f32[] {
  %x.92 = f32[] parameter(0)
  %y.93 = f32[] parameter(1)
  ROOT %add.94 = f32[] add(f32[] %x.92, f32[] %y.93)
}

%AddComputation.220 (x.221: f32[], y.222: f32[]) -> f32[] {
  %x.221 = f32[] parameter(0)
  %y.222 = f32[] parameter(1)
  ROOT %add.223 = f32[] add(f32[] %x.221, f32[] %y.222)
}

%AddComputation.229 (x.230: f32[], y.231: f32[]) -> f32[] {
  %x.230 = f32[] parameter(0)
  %y.231 = f32[] parameter(1)
  ROOT %add.232 = f32[] add(f32[] %x.230, f32[] %y.231)
}

%AddComputation.238 (x.239: f32[], y.240: f32[]) -> f32[] {
  %x.239 = f32[] parameter(0)
  %y.240 = f32[] parameter(1)
  ROOT %add.241 = f32[] add(f32[] %x.239, f32[] %y.240)
}

%AddComputation.247 (x.248: f32[], y.249: f32[]) -> f32[] {
  %x.248 = f32[] parameter(0)
  %y.249 = f32[] parameter(1)
  ROOT %add.250 = f32[] add(f32[] %x.248, f32[] %y.249)
}

%AddComputation.256 (x.257: f32[], y.258: f32[]) -> f32[] {
  %x.257 = f32[] parameter(0)
  %y.258 = f32[] parameter(1)
  ROOT %add.259 = f32[] add(f32[] %x.257, f32[] %y.258)
}

%AddComputation.265 (x.266: f32[], y.267: f32[]) -> f32[] {
  %x.266 = f32[] parameter(0)
  %y.267 = f32[] parameter(1)
  ROOT %add.268 = f32[] add(f32[] %x.266, f32[] %y.267)
}

%AddComputation.274 (x.275: f32[], y.276: f32[]) -> f32[] {
  %x.275 = f32[] parameter(0)
  %y.276 = f32[] parameter(1)
  ROOT %add.277 = f32[] add(f32[] %x.275, f32[] %y.276)
}

%AddComputation.283 (x.284: f32[], y.285: f32[]) -> f32[] {
  %x.284 = f32[] parameter(0)
  %y.285 = f32[] parameter(1)
  ROOT %add.286 = f32[] add(f32[] %x.284, f32[] %y.285)
}

%AddComputation.292 (x.293: f32[], y.294: f32[]) -> f32[] {
  %x.293 = f32[] parameter(0)
  %y.294 = f32[] parameter(1)
  ROOT %add.295 = f32[] add(f32[] %x.293, f32[] %y.294)
}

%AddComputation.301 (x.302: f32[], y.303: f32[]) -> f32[] {
  %x.302 = f32[] parameter(0)
  %y.303 = f32[] parameter(1)
  ROOT %add.304 = f32[] add(f32[] %x.302, f32[] %y.303)
}

%AddComputation.310 (x.311: f32[], y.312: f32[]) -> f32[] {
  %x.311 = f32[] parameter(0)
  %y.312 = f32[] parameter(1)
  ROOT %add.313 = f32[] add(f32[] %x.311, f32[] %y.312)
}

%AddComputation.319 (x.320: f32[], y.321: f32[]) -> f32[] {
  %x.320 = f32[] parameter(0)
  %y.321 = f32[] parameter(1)
  ROOT %add.322 = f32[] add(f32[] %x.320, f32[] %y.321)
}

%AddComputation.328 (x.329: f32[], y.330: f32[]) -> f32[] {
  %x.329 = f32[] parameter(0)
  %y.330 = f32[] parameter(1)
  ROOT %add.331 = f32[] add(f32[] %x.329, f32[] %y.330)
}

%AddComputation.337 (x.338: f32[], y.339: f32[]) -> f32[] {
  %x.338 = f32[] parameter(0)
  %y.339 = f32[] parameter(1)
  ROOT %add.340 = f32[] add(f32[] %x.338, f32[] %y.339)
}

%AddComputation.346 (x.347: f32[], y.348: f32[]) -> f32[] {
  %x.347 = f32[] parameter(0)
  %y.348 = f32[] parameter(1)
  ROOT %add.349 = f32[] add(f32[] %x.347, f32[] %y.348)
}

%AddComputation.355 (x.356: f32[], y.357: f32[]) -> f32[] {
  %x.356 = f32[] parameter(0)
  %y.357 = f32[] parameter(1)
  ROOT %add.358 = f32[] add(f32[] %x.356, f32[] %y.357)
}

%AddComputation.364 (x.365: f32[], y.366: f32[]) -> f32[] {
  %x.365 = f32[] parameter(0)
  %y.366 = f32[] parameter(1)
  ROOT %add.367 = f32[] add(f32[] %x.365, f32[] %y.366)
}

%AddComputation.373 (x.374: f32[], y.375: f32[]) -> f32[] {
  %x.374 = f32[] parameter(0)
  %y.375 = f32[] parameter(1)
  ROOT %add.376 = f32[] add(f32[] %x.374, f32[] %y.375)
}

%AddComputation.382 (x.383: f32[], y.384: f32[]) -> f32[] {
  %x.383 = f32[] parameter(0)
  %y.384 = f32[] parameter(1)
  ROOT %add.385 = f32[] add(f32[] %x.383, f32[] %y.384)
}

%AddComputation.391 (x.392: f32[], y.393: f32[]) -> f32[] {
  %x.392 = f32[] parameter(0)
  %y.393 = f32[] parameter(1)
  ROOT %add.394 = f32[] add(f32[] %x.392, f32[] %y.393)
}

%AddComputation.400 (x.401: f32[], y.402: f32[]) -> f32[] {
  %x.401 = f32[] parameter(0)
  %y.402 = f32[] parameter(1)
  ROOT %add.403 = f32[] add(f32[] %x.401, f32[] %y.402)
}

%AddComputation.409 (x.410: f32[], y.411: f32[]) -> f32[] {
  %x.410 = f32[] parameter(0)
  %y.411 = f32[] parameter(1)
  ROOT %add.412 = f32[] add(f32[] %x.410, f32[] %y.411)
}

%AddComputation.418 (x.419: f32[], y.420: f32[]) -> f32[] {
  %x.419 = f32[] parameter(0)
  %y.420 = f32[] parameter(1)
  ROOT %add.421 = f32[] add(f32[] %x.419, f32[] %y.420)
}

%AddComputation.427 (x.428: f32[], y.429: f32[]) -> f32[] {
  %x.428 = f32[] parameter(0)
  %y.429 = f32[] parameter(1)
  ROOT %add.430 = f32[] add(f32[] %x.428, f32[] %y.429)
}

%AddComputation.436 (x.437: f32[], y.438: f32[]) -> f32[] {
  %x.437 = f32[] parameter(0)
  %y.438 = f32[] parameter(1)
  ROOT %add.439 = f32[] add(f32[] %x.437, f32[] %y.438)
}

%AddComputation.445 (x.446: f32[], y.447: f32[]) -> f32[] {
  %x.446 = f32[] parameter(0)
  %y.447 = f32[] parameter(1)
  ROOT %add.448 = f32[] add(f32[] %x.446, f32[] %y.447)
}

%AddComputation.454 (x.455: f32[], y.456: f32[]) -> f32[] {
  %x.455 = f32[] parameter(0)
  %y.456 = f32[] parameter(1)
  ROOT %add.457 = f32[] add(f32[] %x.455, f32[] %y.456)
}

%AddComputation.463 (x.464: f32[], y.465: f32[]) -> f32[] {
  %x.464 = f32[] parameter(0)
  %y.465 = f32[] parameter(1)
  ROOT %add.466 = f32[] add(f32[] %x.464, f32[] %y.465)
}

%AddComputation.472 (x.473: f32[], y.474: f32[]) -> f32[] {
  %x.473 = f32[] parameter(0)
  %y.474 = f32[] parameter(1)
  ROOT %add.475 = f32[] add(f32[] %x.473, f32[] %y.474)
}

%AddComputation.481 (x.482: f32[], y.483: f32[]) -> f32[] {
  %x.482 = f32[] parameter(0)
  %y.483 = f32[] parameter(1)
  ROOT %add.484 = f32[] add(f32[] %x.482, f32[] %y.483)
}

%AddComputation.521 (x.522: f32[], y.523: f32[]) -> f32[] {
  %x.522 = f32[] parameter(0)
  %y.523 = f32[] parameter(1)
  ROOT %add.524 = f32[] add(f32[] %x.522, f32[] %y.523)
}

ENTRY %SyncTensorsGraph.1730 (p0.8: f32[], p1.10: f32[], p2.14: f32[], p3.15: f32[], p4.16: f32[1], p5.29: f32[30522,16], p6.30: f32[512,16], p7.31: f32[2,16], p8.32: f32[16,16], p9.33: f32[16,16], p10.34: f32[16,16], p11.35: f32[16,16], p12.36: f32[4096,16], p13.37: f32[16,4096], p14.38: f32[16,16], p15.39: f32[16,16], p16.40: f32[2,16], p17.41: f32[16], p18.42: f32[16], p19.43: f32[16], p20.44: f32[16], p21.45: f32[16], p22.46: f32[16], p23.47: f32[16], p24.48: f32[16], p25.49: f32[4096], p26.50: f32[16], p27.51: f32[16], p28.52: f32[16], p29.53: f32[16], p30.54: f32[30522], p31.55: f32[16], p32.56: f32[16], p33.57: f32[16], p34.58: f32[2], p35.536: f32[], p36.551: f32[], p37.557: f32[], p38.566: f32[30522,16], p39.610: f32[512,16], p40.654: f32[2,16], p41.693: f32[16], p42.730: f32[16], p43.772: f32[16,16], p44.811: f32[16], p45.853: f32[16,16], p46.892: f32[16], p47.934: f32[16,16], p48.973: f32[16], p49.1015: f32[16,16], p50.1054: f32[16], p51.1091: f32[16], p52.1128: f32[16], p53.1170: f32[4096,16], p54.1209: f32[4096], p55.1251: f32[16,4096], p56.1290: f32[16], p57.1327: f32[16], p58.1364: f32[16], p59.1406: f32[16,16], p60.1445: f32[16], p61.1487: f32[16,16], p62.1526: f32[16], p63.1563: f32[16], p64.1600: f32[16], p65.1637: f32[30522], p66.1679: f32[2,16], p67.1718: f32[2]) -> (f32[30522,16], f32[512,16], f32[2,16], f32[16], f32[16], /*index=5*/f32[16,16], f32[16], f32[16,16], f32[16], f32[16,16], /*index=10*/f32[16], f32[16,16], f32[16], f32[16], f32[16], /*index=15*/f32[4096,16], f32[4096], f32[16,4096], f32[16], f32[16], /*index=20*/f32[16], f32[16,16], f32[16], f32[16,16], f32[16], /*index=25*/f32[16], f32[16], f32[30522], f32[2,16], f32[2], /*index=30*/f32[1], f32[1], f32[1], f32[30522,16], f32[30522,16], /*index=35*/f32[512,16], f32[512,16], f32[2,16], f32[2,16], f32[16,16], /*index=40*/f32[16,16], f32[16,16], f32[16,16], f32[16,16], f32[16,16], /*index=45*/f32[16,16], f32[16,16], f32[4096,16], f32[4096,16], f32[16,4096], /*index=50*/f32[16,4096], f32[16,16], f32[16,16], f32[16,16], f32[16,16], /*index=55*/f32[2,16], f32[2,16], f32[16], f32[16], f32[16], /*index=60*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=65*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=70*/f32[16], f32[16], f32[16], f32[4096], f32[4096], /*index=75*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=80*/f32[16], f32[16], f32[16], f32[30522], f32[30522], /*index=85*/f32[16], f32[16], f32[16], f32[16], f32[16], /*index=90*/f32[16], f32[2], f32[2], f32[1]) {
  %p38.566 = f32[30522,16]{1,0} parameter(38), frontend_attributes={neff_input_names="input38"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant = f32[] constant(0)
  %p37.557 = f32[] parameter(37), frontend_attributes={neff_input_names="input37"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.0 = f32[] multiply(f32[] %constant, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.2 = f32[30522,16]{1,0} broadcast(f32[] %multiply.0), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %p34.58 = f32[2]{0} parameter(34), frontend_attributes={neff_input_names="input34"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p33.57 = f32[16]{0} parameter(33), frontend_attributes={neff_input_names="input33"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p32.56 = f32[16]{0} parameter(32), frontend_attributes={neff_input_names="input32"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p31.55 = f32[16]{0} parameter(31), frontend_attributes={neff_input_names="input31"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p30.54 = f32[30522]{0} parameter(30), frontend_attributes={neff_input_names="input30"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p29.53 = f32[16]{0} parameter(29), frontend_attributes={neff_input_names="input29"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p28.52 = f32[16]{0} parameter(28), frontend_attributes={neff_input_names="input28"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p27.51 = f32[16]{0} parameter(27), frontend_attributes={neff_input_names="input27"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p26.50 = f32[16]{0} parameter(26), frontend_attributes={neff_input_names="input26"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p25.49 = f32[4096]{0} parameter(25), frontend_attributes={neff_input_names="input25"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p24.48 = f32[16]{0} parameter(24), frontend_attributes={neff_input_names="input24"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p23.47 = f32[16]{0} parameter(23), frontend_attributes={neff_input_names="input23"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p22.46 = f32[16]{0} parameter(22), frontend_attributes={neff_input_names="input22"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p21.45 = f32[16]{0} parameter(21), frontend_attributes={neff_input_names="input21"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p20.44 = f32[16]{0} parameter(20), frontend_attributes={neff_input_names="input20"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p19.43 = f32[16]{0} parameter(19), frontend_attributes={neff_input_names="input19"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p18.42 = f32[16]{0} parameter(18), frontend_attributes={neff_input_names="input18"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p17.41 = f32[16]{0} parameter(17), frontend_attributes={neff_input_names="input17"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p16.40 = f32[2,16]{1,0} parameter(16), frontend_attributes={neff_input_names="input16"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p15.39 = f32[16,16]{1,0} parameter(15), frontend_attributes={neff_input_names="input15"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p14.38 = f32[16,16]{1,0} parameter(14), frontend_attributes={neff_input_names="input14"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p13.37 = f32[16,4096]{1,0} parameter(13), frontend_attributes={neff_input_names="input13"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p12.36 = f32[4096,16]{1,0} parameter(12), frontend_attributes={neff_input_names="input12"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p11.35 = f32[16,16]{1,0} parameter(11), frontend_attributes={neff_input_names="input11"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p10.34 = f32[16,16]{1,0} parameter(10), frontend_attributes={neff_input_names="input10"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p9.33 = f32[16,16]{1,0} parameter(9), frontend_attributes={neff_input_names="input9"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p8.32 = f32[16,16]{1,0} parameter(8), frontend_attributes={neff_input_names="input8"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p7.31 = f32[2,16]{1,0} parameter(7), frontend_attributes={neff_input_names="input7"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p6.30 = f32[512,16]{1,0} parameter(6), frontend_attributes={neff_input_names="input6"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p5.29 = f32[30522,16]{1,0} parameter(5), frontend_attributes={neff_input_names="input5"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %p4.16 = f32[1]{0} parameter(4), frontend_attributes={neff_input_names="input4"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %p3.15 = f32[] parameter(3), frontend_attributes={neff_input_names="input3"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %reshape = f32[1]{0} reshape(f32[] %p3.15), metadata={op_type="aten__div" op_name="aten__div" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %divide.18 = f32[1]{0} divide(f32[1]{0} %p4.16, f32[1]{0} %reshape), metadata={op_type="aten__div" op_name="aten__div" source_file="dp_bert_large_hf_pretrain_hdf5.py" source_line=356}
  %p2.14 = f32[] parameter(2), frontend_attributes={neff_input_names="input2"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %all-reduce.26 = (f32[1]{0}, f32[]) all-reduce(f32[1]{0} %divide.18, f32[] %p2.14), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.22, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %get-tuple-element.90 = f32[] get-tuple-element((f32[1]{0}, f32[]) %all-reduce.26), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %all-reduce.95 = (f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) all-reduce(f32[2]{0} %p34.58, f32[16]{0} %p33.57, f32[16]{0} %p32.56, f32[16]{0} %p31.55, f32[30522]{0} %p30.54, /*index=5*/f32[16]{0} %p29.53, f32[16]{0} %p28.52, f32[16]{0} %p27.51, f32[16]{0} %p26.50, f32[4096]{0} %p25.49, /*index=10*/f32[16]{0} %p24.48, f32[16]{0} %p23.47, f32[16]{0} %p22.46, f32[16]{0} %p21.45, f32[16]{0} %p20.44, /*index=15*/f32[16]{0} %p19.43, f32[16]{0} %p18.42, f32[16]{0} %p17.41, f32[2,16]{1,0} %p16.40, f32[16,16]{1,0} %p15.39, /*index=20*/f32[16,16]{1,0} %p14.38, f32[16,4096]{1,0} %p13.37, f32[4096,16]{1,0} %p12.36, f32[16,16]{1,0} %p11.35, f32[16,16]{1,0} %p10.34, /*index=25*/f32[16,16]{1,0} %p9.33, f32[16,16]{1,0} %p8.32, f32[2,16]{1,0} %p7.31, f32[512,16]{1,0} %p6.30, f32[30522,16]{1,0} %p5.29, /*index=30*/f32[] %get-tuple-element.90), replica_groups={}, constrain_layout=true, to_apply=%AddComputation.91, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %get-tuple-element.212 = f32[30522,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=29, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.528 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=49}
  %constant.213 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.214 = f32[30522,16]{1,0} broadcast(f32[] %constant.213), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.215 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %get-tuple-element.212, f32[30522,16]{1,0} %broadcast.214), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.478 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.215, f32[30522,16]{1,0} %multiply.215), metadata={op_type="aten__mul" op_name="aten__norm.1/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.479 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.1/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.485 = f32[] reduce(f32[30522,16]{1,0} %multiply.478, f32[] %constant.479), dimensions={0,1}, to_apply=%AddComputation.481, metadata={op_type="aten__sum" op_name="aten__norm.1/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.486 = f32[] sqrt(f32[] %reduce.485), metadata={op_type="aten__sqrt" op_name="aten__norm.1/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.487 = f32[1]{0} reshape(f32[] %sqrt.486), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.208 = f32[512,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=28, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.209 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.210 = f32[512,16]{1,0} broadcast(f32[] %constant.209), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.211 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %get-tuple-element.208, f32[512,16]{1,0} %broadcast.210), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.469 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.211, f32[512,16]{1,0} %multiply.211), metadata={op_type="aten__mul" op_name="aten__norm.2/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.470 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.2/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.476 = f32[] reduce(f32[512,16]{1,0} %multiply.469, f32[] %constant.470), dimensions={0,1}, to_apply=%AddComputation.472, metadata={op_type="aten__sum" op_name="aten__norm.2/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.477 = f32[] sqrt(f32[] %reduce.476), metadata={op_type="aten__sqrt" op_name="aten__norm.2/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.488 = f32[1]{0} reshape(f32[] %sqrt.477), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.204 = f32[2,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=27, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.205 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.206 = f32[2,16]{1,0} broadcast(f32[] %constant.205), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.207 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.204, f32[2,16]{1,0} %broadcast.206), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.460 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.207, f32[2,16]{1,0} %multiply.207), metadata={op_type="aten__mul" op_name="aten__norm.3/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.461 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.3/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.467 = f32[] reduce(f32[2,16]{1,0} %multiply.460, f32[] %constant.461), dimensions={0,1}, to_apply=%AddComputation.463, metadata={op_type="aten__sum" op_name="aten__norm.3/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.468 = f32[] sqrt(f32[] %reduce.467), metadata={op_type="aten__sqrt" op_name="aten__norm.3/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.489 = f32[1]{0} reshape(f32[] %sqrt.468), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.164 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=17, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.165 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.166 = f32[16]{0} broadcast(f32[] %constant.165), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.167 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.164, f32[16]{0} %broadcast.166), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.451 = f32[16]{0} multiply(f32[16]{0} %multiply.167, f32[16]{0} %multiply.167), metadata={op_type="aten__mul" op_name="aten__norm.4/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.452 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.4/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.458 = f32[] reduce(f32[16]{0} %multiply.451, f32[] %constant.452), dimensions={0}, to_apply=%AddComputation.454, metadata={op_type="aten__sum" op_name="aten__norm.4/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.459 = f32[] sqrt(f32[] %reduce.458), metadata={op_type="aten__sqrt" op_name="aten__norm.4/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.490 = f32[1]{0} reshape(f32[] %sqrt.459), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.160 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=16, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.161 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.162 = f32[16]{0} broadcast(f32[] %constant.161), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.163 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.160, f32[16]{0} %broadcast.162), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.442 = f32[16]{0} multiply(f32[16]{0} %multiply.163, f32[16]{0} %multiply.163), metadata={op_type="aten__mul" op_name="aten__norm.5/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.443 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.449 = f32[] reduce(f32[16]{0} %multiply.442, f32[] %constant.443), dimensions={0}, to_apply=%AddComputation.445, metadata={op_type="aten__sum" op_name="aten__norm.5/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.450 = f32[] sqrt(f32[] %reduce.449), metadata={op_type="aten__sqrt" op_name="aten__norm.5/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.491 = f32[1]{0} reshape(f32[] %sqrt.450), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.200 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=26, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.201 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.202 = f32[16,16]{1,0} broadcast(f32[] %constant.201), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.203 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.200, f32[16,16]{1,0} %broadcast.202), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.433 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.203, f32[16,16]{1,0} %multiply.203), metadata={op_type="aten__mul" op_name="aten__norm.6/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.434 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.440 = f32[] reduce(f32[16,16]{1,0} %multiply.433, f32[] %constant.434), dimensions={0,1}, to_apply=%AddComputation.436, metadata={op_type="aten__sum" op_name="aten__norm.6/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.441 = f32[] sqrt(f32[] %reduce.440), metadata={op_type="aten__sqrt" op_name="aten__norm.6/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.492 = f32[1]{0} reshape(f32[] %sqrt.441), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.156 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=15, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.157 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.158 = f32[16]{0} broadcast(f32[] %constant.157), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.159 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.156, f32[16]{0} %broadcast.158), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.424 = f32[16]{0} multiply(f32[16]{0} %multiply.159, f32[16]{0} %multiply.159), metadata={op_type="aten__mul" op_name="aten__norm.7/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.425 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.431 = f32[] reduce(f32[16]{0} %multiply.424, f32[] %constant.425), dimensions={0}, to_apply=%AddComputation.427, metadata={op_type="aten__sum" op_name="aten__norm.7/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.432 = f32[] sqrt(f32[] %reduce.431), metadata={op_type="aten__sqrt" op_name="aten__norm.7/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.493 = f32[1]{0} reshape(f32[] %sqrt.432), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.196 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=25, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.197 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.198 = f32[16,16]{1,0} broadcast(f32[] %constant.197), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.199 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.196, f32[16,16]{1,0} %broadcast.198), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.415 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.199, f32[16,16]{1,0} %multiply.199), metadata={op_type="aten__mul" op_name="aten__norm.8/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.416 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.422 = f32[] reduce(f32[16,16]{1,0} %multiply.415, f32[] %constant.416), dimensions={0,1}, to_apply=%AddComputation.418, metadata={op_type="aten__sum" op_name="aten__norm.8/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.423 = f32[] sqrt(f32[] %reduce.422), metadata={op_type="aten__sqrt" op_name="aten__norm.8/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.494 = f32[1]{0} reshape(f32[] %sqrt.423), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.152 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=14, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.153 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.154 = f32[16]{0} broadcast(f32[] %constant.153), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.155 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.152, f32[16]{0} %broadcast.154), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.406 = f32[16]{0} multiply(f32[16]{0} %multiply.155, f32[16]{0} %multiply.155), metadata={op_type="aten__mul" op_name="aten__norm.9/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.407 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.413 = f32[] reduce(f32[16]{0} %multiply.406, f32[] %constant.407), dimensions={0}, to_apply=%AddComputation.409, metadata={op_type="aten__sum" op_name="aten__norm.9/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.414 = f32[] sqrt(f32[] %reduce.413), metadata={op_type="aten__sqrt" op_name="aten__norm.9/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.495 = f32[1]{0} reshape(f32[] %sqrt.414), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.192 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=24, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.193 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.194 = f32[16,16]{1,0} broadcast(f32[] %constant.193), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.195 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.192, f32[16,16]{1,0} %broadcast.194), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.397 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.195, f32[16,16]{1,0} %multiply.195), metadata={op_type="aten__mul" op_name="aten__norm.10/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.398 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.404 = f32[] reduce(f32[16,16]{1,0} %multiply.397, f32[] %constant.398), dimensions={0,1}, to_apply=%AddComputation.400, metadata={op_type="aten__sum" op_name="aten__norm.10/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.405 = f32[] sqrt(f32[] %reduce.404), metadata={op_type="aten__sqrt" op_name="aten__norm.10/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.496 = f32[1]{0} reshape(f32[] %sqrt.405), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.148 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=13, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.149 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.150 = f32[16]{0} broadcast(f32[] %constant.149), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.151 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.148, f32[16]{0} %broadcast.150), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.388 = f32[16]{0} multiply(f32[16]{0} %multiply.151, f32[16]{0} %multiply.151), metadata={op_type="aten__mul" op_name="aten__norm.11/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.389 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.395 = f32[] reduce(f32[16]{0} %multiply.388, f32[] %constant.389), dimensions={0}, to_apply=%AddComputation.391, metadata={op_type="aten__sum" op_name="aten__norm.11/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.396 = f32[] sqrt(f32[] %reduce.395), metadata={op_type="aten__sqrt" op_name="aten__norm.11/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.497 = f32[1]{0} reshape(f32[] %sqrt.396), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.188 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=23, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.189 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.190 = f32[16,16]{1,0} broadcast(f32[] %constant.189), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.191 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.188, f32[16,16]{1,0} %broadcast.190), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.379 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.191, f32[16,16]{1,0} %multiply.191), metadata={op_type="aten__mul" op_name="aten__norm.12/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.380 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.386 = f32[] reduce(f32[16,16]{1,0} %multiply.379, f32[] %constant.380), dimensions={0,1}, to_apply=%AddComputation.382, metadata={op_type="aten__sum" op_name="aten__norm.12/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.387 = f32[] sqrt(f32[] %reduce.386), metadata={op_type="aten__sqrt" op_name="aten__norm.12/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.498 = f32[1]{0} reshape(f32[] %sqrt.387), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.144 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=12, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.145 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.146 = f32[16]{0} broadcast(f32[] %constant.145), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.147 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.144, f32[16]{0} %broadcast.146), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.370 = f32[16]{0} multiply(f32[16]{0} %multiply.147, f32[16]{0} %multiply.147), metadata={op_type="aten__mul" op_name="aten__norm.13/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.371 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.377 = f32[] reduce(f32[16]{0} %multiply.370, f32[] %constant.371), dimensions={0}, to_apply=%AddComputation.373, metadata={op_type="aten__sum" op_name="aten__norm.13/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.378 = f32[] sqrt(f32[] %reduce.377), metadata={op_type="aten__sqrt" op_name="aten__norm.13/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.499 = f32[1]{0} reshape(f32[] %sqrt.378), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.140 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=11, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.141 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.142 = f32[16]{0} broadcast(f32[] %constant.141), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.143 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.140, f32[16]{0} %broadcast.142), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.361 = f32[16]{0} multiply(f32[16]{0} %multiply.143, f32[16]{0} %multiply.143), metadata={op_type="aten__mul" op_name="aten__norm.14/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.362 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.368 = f32[] reduce(f32[16]{0} %multiply.361, f32[] %constant.362), dimensions={0}, to_apply=%AddComputation.364, metadata={op_type="aten__sum" op_name="aten__norm.14/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.369 = f32[] sqrt(f32[] %reduce.368), metadata={op_type="aten__sqrt" op_name="aten__norm.14/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.500 = f32[1]{0} reshape(f32[] %sqrt.369), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.136 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=10, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.137 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.138 = f32[16]{0} broadcast(f32[] %constant.137), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.139 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.136, f32[16]{0} %broadcast.138), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.352 = f32[16]{0} multiply(f32[16]{0} %multiply.139, f32[16]{0} %multiply.139), metadata={op_type="aten__mul" op_name="aten__norm.15/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.353 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.359 = f32[] reduce(f32[16]{0} %multiply.352, f32[] %constant.353), dimensions={0}, to_apply=%AddComputation.355, metadata={op_type="aten__sum" op_name="aten__norm.15/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.360 = f32[] sqrt(f32[] %reduce.359), metadata={op_type="aten__sqrt" op_name="aten__norm.15/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.501 = f32[1]{0} reshape(f32[] %sqrt.360), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.184 = f32[4096,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=22, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.185 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.186 = f32[4096,16]{1,0} broadcast(f32[] %constant.185), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.187 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %get-tuple-element.184, f32[4096,16]{1,0} %broadcast.186), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.343 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.187, f32[4096,16]{1,0} %multiply.187), metadata={op_type="aten__mul" op_name="aten__norm.16/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.344 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.350 = f32[] reduce(f32[4096,16]{1,0} %multiply.343, f32[] %constant.344), dimensions={0,1}, to_apply=%AddComputation.346, metadata={op_type="aten__sum" op_name="aten__norm.16/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.351 = f32[] sqrt(f32[] %reduce.350), metadata={op_type="aten__sqrt" op_name="aten__norm.16/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.502 = f32[1]{0} reshape(f32[] %sqrt.351), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.132 = f32[4096]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=9, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.133 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.134 = f32[4096]{0} broadcast(f32[] %constant.133), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.135 = f32[4096]{0} multiply(f32[4096]{0} %get-tuple-element.132, f32[4096]{0} %broadcast.134), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.334 = f32[4096]{0} multiply(f32[4096]{0} %multiply.135, f32[4096]{0} %multiply.135), metadata={op_type="aten__mul" op_name="aten__norm.17/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.335 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.341 = f32[] reduce(f32[4096]{0} %multiply.334, f32[] %constant.335), dimensions={0}, to_apply=%AddComputation.337, metadata={op_type="aten__sum" op_name="aten__norm.17/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.342 = f32[] sqrt(f32[] %reduce.341), metadata={op_type="aten__sqrt" op_name="aten__norm.17/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.503 = f32[1]{0} reshape(f32[] %sqrt.342), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.180 = f32[16,4096]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=21, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.181 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.182 = f32[16,4096]{1,0} broadcast(f32[] %constant.181), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.183 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %get-tuple-element.180, f32[16,4096]{1,0} %broadcast.182), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.325 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.183, f32[16,4096]{1,0} %multiply.183), metadata={op_type="aten__mul" op_name="aten__norm.18/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.326 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.332 = f32[] reduce(f32[16,4096]{1,0} %multiply.325, f32[] %constant.326), dimensions={0,1}, to_apply=%AddComputation.328, metadata={op_type="aten__sum" op_name="aten__norm.18/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.333 = f32[] sqrt(f32[] %reduce.332), metadata={op_type="aten__sqrt" op_name="aten__norm.18/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.504 = f32[1]{0} reshape(f32[] %sqrt.333), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.128 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=8, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.129 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.130 = f32[16]{0} broadcast(f32[] %constant.129), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.131 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.128, f32[16]{0} %broadcast.130), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.316 = f32[16]{0} multiply(f32[16]{0} %multiply.131, f32[16]{0} %multiply.131), metadata={op_type="aten__mul" op_name="aten__norm.19/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.317 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.323 = f32[] reduce(f32[16]{0} %multiply.316, f32[] %constant.317), dimensions={0}, to_apply=%AddComputation.319, metadata={op_type="aten__sum" op_name="aten__norm.19/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.324 = f32[] sqrt(f32[] %reduce.323), metadata={op_type="aten__sqrt" op_name="aten__norm.19/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.505 = f32[1]{0} reshape(f32[] %sqrt.324), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.124 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=7, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.125 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.126 = f32[16]{0} broadcast(f32[] %constant.125), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.127 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.124, f32[16]{0} %broadcast.126), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.307 = f32[16]{0} multiply(f32[16]{0} %multiply.127, f32[16]{0} %multiply.127), metadata={op_type="aten__mul" op_name="aten__norm.20/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.308 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.314 = f32[] reduce(f32[16]{0} %multiply.307, f32[] %constant.308), dimensions={0}, to_apply=%AddComputation.310, metadata={op_type="aten__sum" op_name="aten__norm.20/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.315 = f32[] sqrt(f32[] %reduce.314), metadata={op_type="aten__sqrt" op_name="aten__norm.20/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.506 = f32[1]{0} reshape(f32[] %sqrt.315), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.120 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=6, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.121 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.122 = f32[16]{0} broadcast(f32[] %constant.121), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.123 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.120, f32[16]{0} %broadcast.122), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.298 = f32[16]{0} multiply(f32[16]{0} %multiply.123, f32[16]{0} %multiply.123), metadata={op_type="aten__mul" op_name="aten__norm.21/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.299 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.305 = f32[] reduce(f32[16]{0} %multiply.298, f32[] %constant.299), dimensions={0}, to_apply=%AddComputation.301, metadata={op_type="aten__sum" op_name="aten__norm.21/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.306 = f32[] sqrt(f32[] %reduce.305), metadata={op_type="aten__sqrt" op_name="aten__norm.21/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.507 = f32[1]{0} reshape(f32[] %sqrt.306), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.176 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=20, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.177 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.178 = f32[16,16]{1,0} broadcast(f32[] %constant.177), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.179 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.176, f32[16,16]{1,0} %broadcast.178), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.289 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.179, f32[16,16]{1,0} %multiply.179), metadata={op_type="aten__mul" op_name="aten__norm.22/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.290 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.296 = f32[] reduce(f32[16,16]{1,0} %multiply.289, f32[] %constant.290), dimensions={0,1}, to_apply=%AddComputation.292, metadata={op_type="aten__sum" op_name="aten__norm.22/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.297 = f32[] sqrt(f32[] %reduce.296), metadata={op_type="aten__sqrt" op_name="aten__norm.22/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.508 = f32[1]{0} reshape(f32[] %sqrt.297), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.116 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=5, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.117 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.118 = f32[16]{0} broadcast(f32[] %constant.117), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.119 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.116, f32[16]{0} %broadcast.118), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.280 = f32[16]{0} multiply(f32[16]{0} %multiply.119, f32[16]{0} %multiply.119), metadata={op_type="aten__mul" op_name="aten__norm.23/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.281 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.287 = f32[] reduce(f32[16]{0} %multiply.280, f32[] %constant.281), dimensions={0}, to_apply=%AddComputation.283, metadata={op_type="aten__sum" op_name="aten__norm.23/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.288 = f32[] sqrt(f32[] %reduce.287), metadata={op_type="aten__sqrt" op_name="aten__norm.23/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.509 = f32[1]{0} reshape(f32[] %sqrt.288), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.112 = f32[30522]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=4, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.113 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.114 = f32[30522]{0} broadcast(f32[] %constant.113), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.115 = f32[30522]{0} multiply(f32[30522]{0} %get-tuple-element.112, f32[30522]{0} %broadcast.114), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.271 = f32[30522]{0} multiply(f32[30522]{0} %multiply.115, f32[30522]{0} %multiply.115), metadata={op_type="aten__mul" op_name="aten__norm.24/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.272 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.278 = f32[] reduce(f32[30522]{0} %multiply.271, f32[] %constant.272), dimensions={0}, to_apply=%AddComputation.274, metadata={op_type="aten__sum" op_name="aten__norm.24/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.279 = f32[] sqrt(f32[] %reduce.278), metadata={op_type="aten__sqrt" op_name="aten__norm.24/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.510 = f32[1]{0} reshape(f32[] %sqrt.279), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.172 = f32[16,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=19, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.173 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.174 = f32[16,16]{1,0} broadcast(f32[] %constant.173), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.175 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.172, f32[16,16]{1,0} %broadcast.174), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.262 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.175, f32[16,16]{1,0} %multiply.175), metadata={op_type="aten__mul" op_name="aten__norm.25/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.263 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.269 = f32[] reduce(f32[16,16]{1,0} %multiply.262, f32[] %constant.263), dimensions={0,1}, to_apply=%AddComputation.265, metadata={op_type="aten__sum" op_name="aten__norm.25/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.270 = f32[] sqrt(f32[] %reduce.269), metadata={op_type="aten__sqrt" op_name="aten__norm.25/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.511 = f32[1]{0} reshape(f32[] %sqrt.270), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.108 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=3, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.109 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.110 = f32[16]{0} broadcast(f32[] %constant.109), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.111 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.108, f32[16]{0} %broadcast.110), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.253 = f32[16]{0} multiply(f32[16]{0} %multiply.111, f32[16]{0} %multiply.111), metadata={op_type="aten__mul" op_name="aten__norm.26/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.254 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.260 = f32[] reduce(f32[16]{0} %multiply.253, f32[] %constant.254), dimensions={0}, to_apply=%AddComputation.256, metadata={op_type="aten__sum" op_name="aten__norm.26/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.261 = f32[] sqrt(f32[] %reduce.260), metadata={op_type="aten__sqrt" op_name="aten__norm.26/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.512 = f32[1]{0} reshape(f32[] %sqrt.261), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.104 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=2, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.105 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.106 = f32[16]{0} broadcast(f32[] %constant.105), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.107 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.104, f32[16]{0} %broadcast.106), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.244 = f32[16]{0} multiply(f32[16]{0} %multiply.107, f32[16]{0} %multiply.107), metadata={op_type="aten__mul" op_name="aten__norm.27/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.245 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.251 = f32[] reduce(f32[16]{0} %multiply.244, f32[] %constant.245), dimensions={0}, to_apply=%AddComputation.247, metadata={op_type="aten__sum" op_name="aten__norm.27/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.252 = f32[] sqrt(f32[] %reduce.251), metadata={op_type="aten__sqrt" op_name="aten__norm.27/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.513 = f32[1]{0} reshape(f32[] %sqrt.252), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.100 = f32[16]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=1, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.101 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.102 = f32[16]{0} broadcast(f32[] %constant.101), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.103 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.100, f32[16]{0} %broadcast.102), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.235 = f32[16]{0} multiply(f32[16]{0} %multiply.103, f32[16]{0} %multiply.103), metadata={op_type="aten__mul" op_name="aten__norm.28/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.236 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.242 = f32[] reduce(f32[16]{0} %multiply.235, f32[] %constant.236), dimensions={0}, to_apply=%AddComputation.238, metadata={op_type="aten__sum" op_name="aten__norm.28/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.243 = f32[] sqrt(f32[] %reduce.242), metadata={op_type="aten__sqrt" op_name="aten__norm.28/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.514 = f32[1]{0} reshape(f32[] %sqrt.243), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.168 = f32[2,16]{1,0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=18, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.169 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.170 = f32[2,16]{1,0} broadcast(f32[] %constant.169), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.171 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.168, f32[2,16]{1,0} %broadcast.170), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.226 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.171, f32[2,16]{1,0} %multiply.171), metadata={op_type="aten__mul" op_name="aten__norm.29/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.227 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.233 = f32[] reduce(f32[2,16]{1,0} %multiply.226, f32[] %constant.227), dimensions={0,1}, to_apply=%AddComputation.229, metadata={op_type="aten__sum" op_name="aten__norm.29/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.234 = f32[] sqrt(f32[] %reduce.233), metadata={op_type="aten__sqrt" op_name="aten__norm.29/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.515 = f32[1]{0} reshape(f32[] %sqrt.234), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %get-tuple-element.96 = f32[2]{0} get-tuple-element((f32[2]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, /*index=5*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, /*index=10*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=20*/f32[16,16]{1,0}, f32[16,4096]{1,0}, f32[4096,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=25*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[2,16]{1,0}, f32[512,16]{1,0}, f32[30522,16]{1,0}, /*index=30*/f32[]) %all-reduce.95), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %constant.97 = f32[] constant(0.5), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %broadcast.98 = f32[2]{0} broadcast(f32[] %constant.97), dimensions={}, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.99 = f32[2]{0} multiply(f32[2]{0} %get-tuple-element.96, f32[2]{0} %broadcast.98), metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_model.py" source_line=484}
  %multiply.217 = f32[2]{0} multiply(f32[2]{0} %multiply.99, f32[2]{0} %multiply.99), metadata={op_type="aten__mul" op_name="aten__norm.30/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.218 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.224 = f32[] reduce(f32[2]{0} %multiply.217, f32[] %constant.218), dimensions={0}, to_apply=%AddComputation.220, metadata={op_type="aten__sum" op_name="aten__norm.30/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.225 = f32[] sqrt(f32[] %reduce.224), metadata={op_type="aten__sqrt" op_name="aten__norm.30/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reshape.516 = f32[1]{0} reshape(f32[] %sqrt.225), metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %concatenate.517 = f32[30]{0} concatenate(f32[1]{0} %reshape.487, f32[1]{0} %reshape.488, f32[1]{0} %reshape.489, f32[1]{0} %reshape.490, f32[1]{0} %reshape.491, /*index=5*/f32[1]{0} %reshape.492, f32[1]{0} %reshape.493, f32[1]{0} %reshape.494, f32[1]{0} %reshape.495, f32[1]{0} %reshape.496, /*index=10*/f32[1]{0} %reshape.497, f32[1]{0} %reshape.498, f32[1]{0} %reshape.499, f32[1]{0} %reshape.500, f32[1]{0} %reshape.501, /*index=15*/f32[1]{0} %reshape.502, f32[1]{0} %reshape.503, f32[1]{0} %reshape.504, f32[1]{0} %reshape.505, f32[1]{0} %reshape.506, /*index=20*/f32[1]{0} %reshape.507, f32[1]{0} %reshape.508, f32[1]{0} %reshape.509, f32[1]{0} %reshape.510, f32[1]{0} %reshape.511, /*index=25*/f32[1]{0} %reshape.512, f32[1]{0} %reshape.513, f32[1]{0} %reshape.514, f32[1]{0} %reshape.515, f32[1]{0} %reshape.516), dimensions={0}, metadata={op_type="aten__stack" op_name="aten__stack" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=41}
  %multiply.518 = f32[30]{0} multiply(f32[30]{0} %concatenate.517, f32[30]{0} %concatenate.517), metadata={op_type="aten__mul" op_name="aten__norm.31/aten__mul" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %constant.519 = f32[] constant(0), metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %reduce.525 = f32[] reduce(f32[30]{0} %multiply.518, f32[] %constant.519), dimensions={0}, to_apply=%AddComputation.521, metadata={op_type="aten__sum" op_name="aten__norm.31/aten__sum" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %sqrt.526 = f32[] sqrt(f32[] %reduce.525), metadata={op_type="aten__sqrt" op_name="aten__norm.31/aten__sqrt" source_file="/home/ubuntu/kahfi/pytorch/torch/functional.py" source_line=1624}
  %p0.8 = f32[] parameter(0), frontend_attributes={neff_input_names="input0"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.527 = f32[] add(f32[] %sqrt.526, f32[] %p0.8), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=49}
  %divide.529 = f32[] divide(f32[] %constant.528, f32[] %add.527), metadata={op_type="aten__div" op_name="aten__div" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=49}
  %constant.2 = f32[] constant(1)
  %compare.532 = pred[] compare(f32[] %divide.529, f32[] %constant.2), direction=LT, metadata={op_type="aten__lt" op_name="aten__lt" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=50}
  %constant.11 = f32[] constant(1), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=51}
  %select.533 = f32[] select(pred[] %compare.532, f32[] %divide.529, f32[] %constant.11), metadata={op_type="aten__where" op_name="aten__where" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=50}
  %multiply.1 = f32[] multiply(f32[] %select.533, f32[] %constant.213), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.4 = f32[30522,16]{1,0} broadcast(f32[] %multiply.1), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.535 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %get-tuple-element.212, f32[30522,16]{1,0} %broadcast.4), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %p36.551 = f32[] parameter(36), frontend_attributes={neff_input_names="input36"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.555 = f32[30522,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.556 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.535, f32[30522,16]{1,0} %broadcast.555), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.565 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %broadcast.2, f32[30522,16]{1,0} %multiply.556), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.3 = f32[] constant(0)
  %p35.536 = f32[] parameter(35), frontend_attributes={neff_input_names="input35"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.3 = f32[] multiply(f32[] %constant.3, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.8 = f32[30522,16]{1,0} broadcast(f32[] %multiply.3), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.544 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.535, f32[30522,16]{1,0} %multiply.535), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %p1.10 = f32[] parameter(1), frontend_attributes={neff_input_names="input1"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.545 = f32[30522,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.546 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %multiply.544, f32[30522,16]{1,0} %broadcast.545), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.547 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %broadcast.8, f32[30522,16]{1,0} %multiply.546), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.548 = f32[30522,16]{1,0} sqrt(f32[30522,16]{1,0} %add.547), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.549 = f32[30522,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.550 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %sqrt.548, f32[30522,16]{1,0} %broadcast.549), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.567 = f32[30522,16]{1,0} divide(f32[30522,16]{1,0} %add.565, f32[30522,16]{1,0} %add.550), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.6 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.568 = f32[30522,16]{1,0} broadcast(f32[] %constant.6), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.569 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %divide.567, f32[30522,16]{1,0} %broadcast.568), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.570 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %p38.566, f32[30522,16]{1,0} %multiply.569), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.5 = f32[30522,16]{1,0} broadcast(f32[] %constant.1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.571 = f32[30522,16]{1,0} multiply(f32[30522,16]{1,0} %add.570, f32[30522,16]{1,0} %broadcast.5), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.572 = f32[30522,16]{1,0} add(f32[30522,16]{1,0} %add.570, f32[30522,16]{1,0} %multiply.571), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p39.610 = f32[512,16]{1,0} parameter(39), frontend_attributes={neff_input_names="input39"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.4 = f32[] constant(0)
  %multiply.4 = f32[] multiply(f32[] %constant.4, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.11 = f32[512,16]{1,0} broadcast(f32[] %multiply.4), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.5 = f32[] multiply(f32[] %select.533, f32[] %constant.209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.12 = f32[512,16]{1,0} broadcast(f32[] %multiply.5), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.582 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %get-tuple-element.208, f32[512,16]{1,0} %broadcast.12), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.600 = f32[512,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.601 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.582, f32[512,16]{1,0} %broadcast.600), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.609 = f32[512,16]{1,0} add(f32[512,16]{1,0} %broadcast.11, f32[512,16]{1,0} %multiply.601), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.5 = f32[] constant(0)
  %multiply.7 = f32[] multiply(f32[] %constant.5, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.15 = f32[512,16]{1,0} broadcast(f32[] %multiply.7), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.590 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.582, f32[512,16]{1,0} %multiply.582), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.591 = f32[512,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.592 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %multiply.590, f32[512,16]{1,0} %broadcast.591), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.593 = f32[512,16]{1,0} add(f32[512,16]{1,0} %broadcast.15, f32[512,16]{1,0} %multiply.592), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.594 = f32[512,16]{1,0} sqrt(f32[512,16]{1,0} %add.593), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.595 = f32[512,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.596 = f32[512,16]{1,0} add(f32[512,16]{1,0} %sqrt.594, f32[512,16]{1,0} %broadcast.595), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.611 = f32[512,16]{1,0} divide(f32[512,16]{1,0} %add.609, f32[512,16]{1,0} %add.596), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.578 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.612 = f32[512,16]{1,0} broadcast(f32[] %constant.578), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.613 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %divide.611, f32[512,16]{1,0} %broadcast.612), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.614 = f32[512,16]{1,0} add(f32[512,16]{1,0} %p39.610, f32[512,16]{1,0} %multiply.613), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.573 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.577 = f32[512,16]{1,0} broadcast(f32[] %constant.573), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.615 = f32[512,16]{1,0} multiply(f32[512,16]{1,0} %add.614, f32[512,16]{1,0} %broadcast.577), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.616 = f32[512,16]{1,0} add(f32[512,16]{1,0} %add.614, f32[512,16]{1,0} %multiply.615), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p40.654 = f32[2,16]{1,0} parameter(40), frontend_attributes={neff_input_names="input40"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.8 = f32[] constant(0)
  %multiply.8 = f32[] multiply(f32[] %constant.8, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.19 = f32[2,16]{1,0} broadcast(f32[] %multiply.8), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.10 = f32[] multiply(f32[] %select.533, f32[] %constant.205), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.20 = f32[2,16]{1,0} broadcast(f32[] %multiply.10), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.626 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.204, f32[2,16]{1,0} %broadcast.20), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.644 = f32[2,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.645 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.626, f32[2,16]{1,0} %broadcast.644), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.653 = f32[2,16]{1,0} add(f32[2,16]{1,0} %broadcast.19, f32[2,16]{1,0} %multiply.645), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.9 = f32[] constant(0)
  %multiply.12 = f32[] multiply(f32[] %constant.9, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.23 = f32[2,16]{1,0} broadcast(f32[] %multiply.12), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.634 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.626, f32[2,16]{1,0} %multiply.626), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.635 = f32[2,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.636 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.634, f32[2,16]{1,0} %broadcast.635), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.637 = f32[2,16]{1,0} add(f32[2,16]{1,0} %broadcast.23, f32[2,16]{1,0} %multiply.636), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.638 = f32[2,16]{1,0} sqrt(f32[2,16]{1,0} %add.637), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.639 = f32[2,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.640 = f32[2,16]{1,0} add(f32[2,16]{1,0} %sqrt.638, f32[2,16]{1,0} %broadcast.639), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.655 = f32[2,16]{1,0} divide(f32[2,16]{1,0} %add.653, f32[2,16]{1,0} %add.640), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.622 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.656 = f32[2,16]{1,0} broadcast(f32[] %constant.622), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.657 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %divide.655, f32[2,16]{1,0} %broadcast.656), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.658 = f32[2,16]{1,0} add(f32[2,16]{1,0} %p40.654, f32[2,16]{1,0} %multiply.657), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.617 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.621 = f32[2,16]{1,0} broadcast(f32[] %constant.617), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.659 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %add.658, f32[2,16]{1,0} %broadcast.621), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.660 = f32[2,16]{1,0} add(f32[2,16]{1,0} %add.658, f32[2,16]{1,0} %multiply.659), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p41.693 = f32[16]{0} parameter(41), frontend_attributes={neff_input_names="input41"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.10 = f32[] constant(0)
  %multiply.14 = f32[] multiply(f32[] %constant.10, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.26 = f32[16]{0} broadcast(f32[] %multiply.14), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.15 = f32[] multiply(f32[] %select.533, f32[] %constant.165), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.27 = f32[16]{0} broadcast(f32[] %multiply.15), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.665 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.164, f32[16]{0} %broadcast.27), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.683 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.684 = f32[16]{0} multiply(f32[16]{0} %multiply.665, f32[16]{0} %broadcast.683), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.692 = f32[16]{0} add(f32[16]{0} %broadcast.26, f32[16]{0} %multiply.684), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.13 = f32[] constant(0)
  %multiply.17 = f32[] multiply(f32[] %constant.13, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.30 = f32[16]{0} broadcast(f32[] %multiply.17), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.673 = f32[16]{0} multiply(f32[16]{0} %multiply.665, f32[16]{0} %multiply.665), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.674 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.675 = f32[16]{0} multiply(f32[16]{0} %multiply.673, f32[16]{0} %broadcast.674), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.676 = f32[16]{0} add(f32[16]{0} %broadcast.30, f32[16]{0} %multiply.675), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.677 = f32[16]{0} sqrt(f32[16]{0} %add.676), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.678 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.679 = f32[16]{0} add(f32[16]{0} %sqrt.677, f32[16]{0} %broadcast.678), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.694 = f32[16]{0} divide(f32[16]{0} %add.692, f32[16]{0} %add.679), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.661 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.695 = f32[16]{0} broadcast(f32[] %constant.661), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.696 = f32[16]{0} multiply(f32[16]{0} %divide.694, f32[16]{0} %broadcast.695), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.697 = f32[16]{0} add(f32[16]{0} %p41.693, f32[16]{0} %multiply.696), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p42.730 = f32[16]{0} parameter(42), frontend_attributes={neff_input_names="input42"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.14 = f32[] constant(0)
  %multiply.18 = f32[] multiply(f32[] %constant.14, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.33 = f32[16]{0} broadcast(f32[] %multiply.18), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.19 = f32[] multiply(f32[] %select.533, f32[] %constant.161), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.34 = f32[16]{0} broadcast(f32[] %multiply.19), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.702 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.160, f32[16]{0} %broadcast.34), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.720 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.721 = f32[16]{0} multiply(f32[16]{0} %multiply.702, f32[16]{0} %broadcast.720), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.729 = f32[16]{0} add(f32[16]{0} %broadcast.33, f32[16]{0} %multiply.721), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.15 = f32[] constant(0)
  %multiply.21 = f32[] multiply(f32[] %constant.15, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.37 = f32[16]{0} broadcast(f32[] %multiply.21), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.710 = f32[16]{0} multiply(f32[16]{0} %multiply.702, f32[16]{0} %multiply.702), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.711 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.712 = f32[16]{0} multiply(f32[16]{0} %multiply.710, f32[16]{0} %broadcast.711), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.713 = f32[16]{0} add(f32[16]{0} %broadcast.37, f32[16]{0} %multiply.712), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.714 = f32[16]{0} sqrt(f32[16]{0} %add.713), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.715 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.716 = f32[16]{0} add(f32[16]{0} %sqrt.714, f32[16]{0} %broadcast.715), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.731 = f32[16]{0} divide(f32[16]{0} %add.729, f32[16]{0} %add.716), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.698 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.732 = f32[16]{0} broadcast(f32[] %constant.698), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.733 = f32[16]{0} multiply(f32[16]{0} %divide.731, f32[16]{0} %broadcast.732), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.734 = f32[16]{0} add(f32[16]{0} %p42.730, f32[16]{0} %multiply.733), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p43.772 = f32[16,16]{1,0} parameter(43), frontend_attributes={neff_input_names="input43"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.16 = f32[] constant(0)
  %multiply.22 = f32[] multiply(f32[] %constant.16, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.40 = f32[16,16]{1,0} broadcast(f32[] %multiply.22), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.23 = f32[] multiply(f32[] %select.533, f32[] %constant.201), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.41 = f32[16,16]{1,0} broadcast(f32[] %multiply.23), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.744 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.200, f32[16,16]{1,0} %broadcast.41), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.762 = f32[16,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.763 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.744, f32[16,16]{1,0} %broadcast.762), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.771 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.40, f32[16,16]{1,0} %multiply.763), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.17 = f32[] constant(0)
  %multiply.25 = f32[] multiply(f32[] %constant.17, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.44 = f32[16,16]{1,0} broadcast(f32[] %multiply.25), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.752 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.744, f32[16,16]{1,0} %multiply.744), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.753 = f32[16,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.754 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.752, f32[16,16]{1,0} %broadcast.753), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.755 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.44, f32[16,16]{1,0} %multiply.754), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.756 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.755), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.757 = f32[16,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.758 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.756, f32[16,16]{1,0} %broadcast.757), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.773 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.771, f32[16,16]{1,0} %add.758), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.740 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.774 = f32[16,16]{1,0} broadcast(f32[] %constant.740), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.775 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.773, f32[16,16]{1,0} %broadcast.774), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.776 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p43.772, f32[16,16]{1,0} %multiply.775), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.735 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.739 = f32[16,16]{1,0} broadcast(f32[] %constant.735), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.777 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.776, f32[16,16]{1,0} %broadcast.739), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.778 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.776, f32[16,16]{1,0} %multiply.777), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p44.811 = f32[16]{0} parameter(44), frontend_attributes={neff_input_names="input44"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.18 = f32[] constant(0)
  %multiply.26 = f32[] multiply(f32[] %constant.18, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.47 = f32[16]{0} broadcast(f32[] %multiply.26), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.27 = f32[] multiply(f32[] %select.533, f32[] %constant.157), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.48 = f32[16]{0} broadcast(f32[] %multiply.27), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.783 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.156, f32[16]{0} %broadcast.48), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.801 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.802 = f32[16]{0} multiply(f32[16]{0} %multiply.783, f32[16]{0} %broadcast.801), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.810 = f32[16]{0} add(f32[16]{0} %broadcast.47, f32[16]{0} %multiply.802), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.19 = f32[] constant(0)
  %multiply.29 = f32[] multiply(f32[] %constant.19, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.51 = f32[16]{0} broadcast(f32[] %multiply.29), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.791 = f32[16]{0} multiply(f32[16]{0} %multiply.783, f32[16]{0} %multiply.783), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.792 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.793 = f32[16]{0} multiply(f32[16]{0} %multiply.791, f32[16]{0} %broadcast.792), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.794 = f32[16]{0} add(f32[16]{0} %broadcast.51, f32[16]{0} %multiply.793), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.795 = f32[16]{0} sqrt(f32[16]{0} %add.794), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.796 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.797 = f32[16]{0} add(f32[16]{0} %sqrt.795, f32[16]{0} %broadcast.796), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.812 = f32[16]{0} divide(f32[16]{0} %add.810, f32[16]{0} %add.797), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.779 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.813 = f32[16]{0} broadcast(f32[] %constant.779), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.814 = f32[16]{0} multiply(f32[16]{0} %divide.812, f32[16]{0} %broadcast.813), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.815 = f32[16]{0} add(f32[16]{0} %p44.811, f32[16]{0} %multiply.814), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p45.853 = f32[16,16]{1,0} parameter(45), frontend_attributes={neff_input_names="input45"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.20 = f32[] constant(0)
  %multiply.30 = f32[] multiply(f32[] %constant.20, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.54 = f32[16,16]{1,0} broadcast(f32[] %multiply.30), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.31 = f32[] multiply(f32[] %select.533, f32[] %constant.197), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.55 = f32[16,16]{1,0} broadcast(f32[] %multiply.31), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.825 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.196, f32[16,16]{1,0} %broadcast.55), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.843 = f32[16,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.844 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.825, f32[16,16]{1,0} %broadcast.843), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.852 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.54, f32[16,16]{1,0} %multiply.844), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.21 = f32[] constant(0)
  %multiply.33 = f32[] multiply(f32[] %constant.21, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.58 = f32[16,16]{1,0} broadcast(f32[] %multiply.33), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.833 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.825, f32[16,16]{1,0} %multiply.825), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.834 = f32[16,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.835 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.833, f32[16,16]{1,0} %broadcast.834), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.836 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.58, f32[16,16]{1,0} %multiply.835), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.837 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.836), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.838 = f32[16,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.839 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.837, f32[16,16]{1,0} %broadcast.838), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.854 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.852, f32[16,16]{1,0} %add.839), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.821 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.855 = f32[16,16]{1,0} broadcast(f32[] %constant.821), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.856 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.854, f32[16,16]{1,0} %broadcast.855), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.857 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p45.853, f32[16,16]{1,0} %multiply.856), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.816 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.820 = f32[16,16]{1,0} broadcast(f32[] %constant.816), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.858 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.857, f32[16,16]{1,0} %broadcast.820), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.859 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.857, f32[16,16]{1,0} %multiply.858), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p46.892 = f32[16]{0} parameter(46), frontend_attributes={neff_input_names="input46"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.22 = f32[] constant(0)
  %multiply.34 = f32[] multiply(f32[] %constant.22, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.61 = f32[16]{0} broadcast(f32[] %multiply.34), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.35 = f32[] multiply(f32[] %select.533, f32[] %constant.153), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.62 = f32[16]{0} broadcast(f32[] %multiply.35), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.864 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.152, f32[16]{0} %broadcast.62), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.882 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.883 = f32[16]{0} multiply(f32[16]{0} %multiply.864, f32[16]{0} %broadcast.882), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.891 = f32[16]{0} add(f32[16]{0} %broadcast.61, f32[16]{0} %multiply.883), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.23 = f32[] constant(0)
  %multiply.37 = f32[] multiply(f32[] %constant.23, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.65 = f32[16]{0} broadcast(f32[] %multiply.37), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.872 = f32[16]{0} multiply(f32[16]{0} %multiply.864, f32[16]{0} %multiply.864), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.873 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.874 = f32[16]{0} multiply(f32[16]{0} %multiply.872, f32[16]{0} %broadcast.873), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.875 = f32[16]{0} add(f32[16]{0} %broadcast.65, f32[16]{0} %multiply.874), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.876 = f32[16]{0} sqrt(f32[16]{0} %add.875), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.877 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.878 = f32[16]{0} add(f32[16]{0} %sqrt.876, f32[16]{0} %broadcast.877), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.893 = f32[16]{0} divide(f32[16]{0} %add.891, f32[16]{0} %add.878), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.860 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.894 = f32[16]{0} broadcast(f32[] %constant.860), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.895 = f32[16]{0} multiply(f32[16]{0} %divide.893, f32[16]{0} %broadcast.894), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.896 = f32[16]{0} add(f32[16]{0} %p46.892, f32[16]{0} %multiply.895), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p47.934 = f32[16,16]{1,0} parameter(47), frontend_attributes={neff_input_names="input47"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.24 = f32[] constant(0)
  %multiply.38 = f32[] multiply(f32[] %constant.24, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.68 = f32[16,16]{1,0} broadcast(f32[] %multiply.38), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.39 = f32[] multiply(f32[] %select.533, f32[] %constant.193), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.69 = f32[16,16]{1,0} broadcast(f32[] %multiply.39), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.906 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.192, f32[16,16]{1,0} %broadcast.69), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.924 = f32[16,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.925 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.906, f32[16,16]{1,0} %broadcast.924), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.933 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.68, f32[16,16]{1,0} %multiply.925), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.25 = f32[] constant(0)
  %multiply.41 = f32[] multiply(f32[] %constant.25, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.72 = f32[16,16]{1,0} broadcast(f32[] %multiply.41), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.914 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.906, f32[16,16]{1,0} %multiply.906), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.915 = f32[16,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.916 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.914, f32[16,16]{1,0} %broadcast.915), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.917 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.72, f32[16,16]{1,0} %multiply.916), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.918 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.917), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.919 = f32[16,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.920 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.918, f32[16,16]{1,0} %broadcast.919), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.935 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.933, f32[16,16]{1,0} %add.920), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.902 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.936 = f32[16,16]{1,0} broadcast(f32[] %constant.902), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.937 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.935, f32[16,16]{1,0} %broadcast.936), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.938 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p47.934, f32[16,16]{1,0} %multiply.937), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.897 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.901 = f32[16,16]{1,0} broadcast(f32[] %constant.897), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.939 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.938, f32[16,16]{1,0} %broadcast.901), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.940 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.938, f32[16,16]{1,0} %multiply.939), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p48.973 = f32[16]{0} parameter(48), frontend_attributes={neff_input_names="input48"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.26 = f32[] constant(0)
  %multiply.42 = f32[] multiply(f32[] %constant.26, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.75 = f32[16]{0} broadcast(f32[] %multiply.42), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.43 = f32[] multiply(f32[] %select.533, f32[] %constant.149), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.76 = f32[16]{0} broadcast(f32[] %multiply.43), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.945 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.148, f32[16]{0} %broadcast.76), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.963 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.964 = f32[16]{0} multiply(f32[16]{0} %multiply.945, f32[16]{0} %broadcast.963), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.972 = f32[16]{0} add(f32[16]{0} %broadcast.75, f32[16]{0} %multiply.964), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.27 = f32[] constant(0)
  %multiply.45 = f32[] multiply(f32[] %constant.27, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.79 = f32[16]{0} broadcast(f32[] %multiply.45), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.953 = f32[16]{0} multiply(f32[16]{0} %multiply.945, f32[16]{0} %multiply.945), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.954 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.955 = f32[16]{0} multiply(f32[16]{0} %multiply.953, f32[16]{0} %broadcast.954), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.956 = f32[16]{0} add(f32[16]{0} %broadcast.79, f32[16]{0} %multiply.955), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.957 = f32[16]{0} sqrt(f32[16]{0} %add.956), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.958 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.959 = f32[16]{0} add(f32[16]{0} %sqrt.957, f32[16]{0} %broadcast.958), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.974 = f32[16]{0} divide(f32[16]{0} %add.972, f32[16]{0} %add.959), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.941 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.975 = f32[16]{0} broadcast(f32[] %constant.941), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.976 = f32[16]{0} multiply(f32[16]{0} %divide.974, f32[16]{0} %broadcast.975), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.977 = f32[16]{0} add(f32[16]{0} %p48.973, f32[16]{0} %multiply.976), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p49.1015 = f32[16,16]{1,0} parameter(49), frontend_attributes={neff_input_names="input49"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.28 = f32[] constant(0)
  %multiply.46 = f32[] multiply(f32[] %constant.28, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.82 = f32[16,16]{1,0} broadcast(f32[] %multiply.46), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.47 = f32[] multiply(f32[] %select.533, f32[] %constant.189), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.83 = f32[16,16]{1,0} broadcast(f32[] %multiply.47), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.987 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.188, f32[16,16]{1,0} %broadcast.83), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1005 = f32[16,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1006 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.987, f32[16,16]{1,0} %broadcast.1005), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1014 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.82, f32[16,16]{1,0} %multiply.1006), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.29 = f32[] constant(0)
  %multiply.49 = f32[] multiply(f32[] %constant.29, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.86 = f32[16,16]{1,0} broadcast(f32[] %multiply.49), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.995 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.987, f32[16,16]{1,0} %multiply.987), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.996 = f32[16,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.997 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.995, f32[16,16]{1,0} %broadcast.996), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.998 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.86, f32[16,16]{1,0} %multiply.997), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.999 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.998), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1000 = f32[16,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1001 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.999, f32[16,16]{1,0} %broadcast.1000), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1016 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.1014, f32[16,16]{1,0} %add.1001), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.983 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1017 = f32[16,16]{1,0} broadcast(f32[] %constant.983), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1018 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.1016, f32[16,16]{1,0} %broadcast.1017), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1019 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p49.1015, f32[16,16]{1,0} %multiply.1018), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.978 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.982 = f32[16,16]{1,0} broadcast(f32[] %constant.978), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1020 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.1019, f32[16,16]{1,0} %broadcast.982), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1021 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.1019, f32[16,16]{1,0} %multiply.1020), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p50.1054 = f32[16]{0} parameter(50), frontend_attributes={neff_input_names="input50"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.30 = f32[] constant(0)
  %multiply.50 = f32[] multiply(f32[] %constant.30, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.89 = f32[16]{0} broadcast(f32[] %multiply.50), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.51 = f32[] multiply(f32[] %select.533, f32[] %constant.145), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.90 = f32[16]{0} broadcast(f32[] %multiply.51), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1026 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.144, f32[16]{0} %broadcast.90), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1044 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1045 = f32[16]{0} multiply(f32[16]{0} %multiply.1026, f32[16]{0} %broadcast.1044), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1053 = f32[16]{0} add(f32[16]{0} %broadcast.89, f32[16]{0} %multiply.1045), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.31 = f32[] constant(0)
  %multiply.53 = f32[] multiply(f32[] %constant.31, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.93 = f32[16]{0} broadcast(f32[] %multiply.53), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1034 = f32[16]{0} multiply(f32[16]{0} %multiply.1026, f32[16]{0} %multiply.1026), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1035 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1036 = f32[16]{0} multiply(f32[16]{0} %multiply.1034, f32[16]{0} %broadcast.1035), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1037 = f32[16]{0} add(f32[16]{0} %broadcast.93, f32[16]{0} %multiply.1036), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1038 = f32[16]{0} sqrt(f32[16]{0} %add.1037), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1039 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1040 = f32[16]{0} add(f32[16]{0} %sqrt.1038, f32[16]{0} %broadcast.1039), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1055 = f32[16]{0} divide(f32[16]{0} %add.1053, f32[16]{0} %add.1040), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1022 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1056 = f32[16]{0} broadcast(f32[] %constant.1022), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1057 = f32[16]{0} multiply(f32[16]{0} %divide.1055, f32[16]{0} %broadcast.1056), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1058 = f32[16]{0} add(f32[16]{0} %p50.1054, f32[16]{0} %multiply.1057), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p51.1091 = f32[16]{0} parameter(51), frontend_attributes={neff_input_names="input51"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.32 = f32[] constant(0)
  %multiply.54 = f32[] multiply(f32[] %constant.32, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.96 = f32[16]{0} broadcast(f32[] %multiply.54), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.55 = f32[] multiply(f32[] %select.533, f32[] %constant.141), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.97 = f32[16]{0} broadcast(f32[] %multiply.55), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1063 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.140, f32[16]{0} %broadcast.97), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1081 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1082 = f32[16]{0} multiply(f32[16]{0} %multiply.1063, f32[16]{0} %broadcast.1081), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1090 = f32[16]{0} add(f32[16]{0} %broadcast.96, f32[16]{0} %multiply.1082), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.33 = f32[] constant(0)
  %multiply.57 = f32[] multiply(f32[] %constant.33, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.101 = f32[16]{0} broadcast(f32[] %multiply.57), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1071 = f32[16]{0} multiply(f32[16]{0} %multiply.1063, f32[16]{0} %multiply.1063), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1072 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1073 = f32[16]{0} multiply(f32[16]{0} %multiply.1071, f32[16]{0} %broadcast.1072), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1074 = f32[16]{0} add(f32[16]{0} %broadcast.101, f32[16]{0} %multiply.1073), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1075 = f32[16]{0} sqrt(f32[16]{0} %add.1074), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1076 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1077 = f32[16]{0} add(f32[16]{0} %sqrt.1075, f32[16]{0} %broadcast.1076), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1092 = f32[16]{0} divide(f32[16]{0} %add.1090, f32[16]{0} %add.1077), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1059 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1093 = f32[16]{0} broadcast(f32[] %constant.1059), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1094 = f32[16]{0} multiply(f32[16]{0} %divide.1092, f32[16]{0} %broadcast.1093), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1095 = f32[16]{0} add(f32[16]{0} %p51.1091, f32[16]{0} %multiply.1094), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p52.1128 = f32[16]{0} parameter(52), frontend_attributes={neff_input_names="input52"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.34 = f32[] constant(0)
  %multiply.58 = f32[] multiply(f32[] %constant.34, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.105 = f32[16]{0} broadcast(f32[] %multiply.58), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.59 = f32[] multiply(f32[] %select.533, f32[] %constant.137), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.107 = f32[16]{0} broadcast(f32[] %multiply.59), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1100 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.136, f32[16]{0} %broadcast.107), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1118 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1119 = f32[16]{0} multiply(f32[16]{0} %multiply.1100, f32[16]{0} %broadcast.1118), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1127 = f32[16]{0} add(f32[16]{0} %broadcast.105, f32[16]{0} %multiply.1119), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.35 = f32[] constant(0)
  %multiply.61 = f32[] multiply(f32[] %constant.35, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.111 = f32[16]{0} broadcast(f32[] %multiply.61), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1108 = f32[16]{0} multiply(f32[16]{0} %multiply.1100, f32[16]{0} %multiply.1100), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1109 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1110 = f32[16]{0} multiply(f32[16]{0} %multiply.1108, f32[16]{0} %broadcast.1109), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1111 = f32[16]{0} add(f32[16]{0} %broadcast.111, f32[16]{0} %multiply.1110), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1112 = f32[16]{0} sqrt(f32[16]{0} %add.1111), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1113 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1114 = f32[16]{0} add(f32[16]{0} %sqrt.1112, f32[16]{0} %broadcast.1113), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1129 = f32[16]{0} divide(f32[16]{0} %add.1127, f32[16]{0} %add.1114), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1096 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1130 = f32[16]{0} broadcast(f32[] %constant.1096), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1131 = f32[16]{0} multiply(f32[16]{0} %divide.1129, f32[16]{0} %broadcast.1130), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1132 = f32[16]{0} add(f32[16]{0} %p52.1128, f32[16]{0} %multiply.1131), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p53.1170 = f32[4096,16]{1,0} parameter(53), frontend_attributes={neff_input_names="input53"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.36 = f32[] constant(0)
  %multiply.62 = f32[] multiply(f32[] %constant.36, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.115 = f32[4096,16]{1,0} broadcast(f32[] %multiply.62), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.63 = f32[] multiply(f32[] %select.533, f32[] %constant.185), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.116 = f32[4096,16]{1,0} broadcast(f32[] %multiply.63), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1142 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %get-tuple-element.184, f32[4096,16]{1,0} %broadcast.116), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1160 = f32[4096,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1161 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.1142, f32[4096,16]{1,0} %broadcast.1160), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1169 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %broadcast.115, f32[4096,16]{1,0} %multiply.1161), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.37 = f32[] constant(0)
  %multiply.65 = f32[] multiply(f32[] %constant.37, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.120 = f32[4096,16]{1,0} broadcast(f32[] %multiply.65), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1150 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.1142, f32[4096,16]{1,0} %multiply.1142), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1151 = f32[4096,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1152 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %multiply.1150, f32[4096,16]{1,0} %broadcast.1151), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1153 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %broadcast.120, f32[4096,16]{1,0} %multiply.1152), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1154 = f32[4096,16]{1,0} sqrt(f32[4096,16]{1,0} %add.1153), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1155 = f32[4096,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1156 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %sqrt.1154, f32[4096,16]{1,0} %broadcast.1155), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1171 = f32[4096,16]{1,0} divide(f32[4096,16]{1,0} %add.1169, f32[4096,16]{1,0} %add.1156), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1138 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1172 = f32[4096,16]{1,0} broadcast(f32[] %constant.1138), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1173 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %divide.1171, f32[4096,16]{1,0} %broadcast.1172), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1174 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %p53.1170, f32[4096,16]{1,0} %multiply.1173), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1133 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.1137 = f32[4096,16]{1,0} broadcast(f32[] %constant.1133), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1175 = f32[4096,16]{1,0} multiply(f32[4096,16]{1,0} %add.1174, f32[4096,16]{1,0} %broadcast.1137), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1176 = f32[4096,16]{1,0} add(f32[4096,16]{1,0} %add.1174, f32[4096,16]{1,0} %multiply.1175), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p54.1209 = f32[4096]{0} parameter(54), frontend_attributes={neff_input_names="input54"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.38 = f32[] constant(0)
  %multiply.66 = f32[] multiply(f32[] %constant.38, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.124 = f32[4096]{0} broadcast(f32[] %multiply.66), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.67 = f32[] multiply(f32[] %select.533, f32[] %constant.133), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.125 = f32[4096]{0} broadcast(f32[] %multiply.67), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1181 = f32[4096]{0} multiply(f32[4096]{0} %get-tuple-element.132, f32[4096]{0} %broadcast.125), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1199 = f32[4096]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1200 = f32[4096]{0} multiply(f32[4096]{0} %multiply.1181, f32[4096]{0} %broadcast.1199), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1208 = f32[4096]{0} add(f32[4096]{0} %broadcast.124, f32[4096]{0} %multiply.1200), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.39 = f32[] constant(0)
  %multiply.69 = f32[] multiply(f32[] %constant.39, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.129 = f32[4096]{0} broadcast(f32[] %multiply.69), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1189 = f32[4096]{0} multiply(f32[4096]{0} %multiply.1181, f32[4096]{0} %multiply.1181), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1190 = f32[4096]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1191 = f32[4096]{0} multiply(f32[4096]{0} %multiply.1189, f32[4096]{0} %broadcast.1190), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1192 = f32[4096]{0} add(f32[4096]{0} %broadcast.129, f32[4096]{0} %multiply.1191), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1193 = f32[4096]{0} sqrt(f32[4096]{0} %add.1192), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1194 = f32[4096]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1195 = f32[4096]{0} add(f32[4096]{0} %sqrt.1193, f32[4096]{0} %broadcast.1194), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1210 = f32[4096]{0} divide(f32[4096]{0} %add.1208, f32[4096]{0} %add.1195), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1177 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1211 = f32[4096]{0} broadcast(f32[] %constant.1177), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1212 = f32[4096]{0} multiply(f32[4096]{0} %divide.1210, f32[4096]{0} %broadcast.1211), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1213 = f32[4096]{0} add(f32[4096]{0} %p54.1209, f32[4096]{0} %multiply.1212), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p55.1251 = f32[16,4096]{1,0} parameter(55), frontend_attributes={neff_input_names="input55"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.40 = f32[] constant(0)
  %multiply.70 = f32[] multiply(f32[] %constant.40, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.133 = f32[16,4096]{1,0} broadcast(f32[] %multiply.70), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.71 = f32[] multiply(f32[] %select.533, f32[] %constant.181), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.135 = f32[16,4096]{1,0} broadcast(f32[] %multiply.71), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1223 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %get-tuple-element.180, f32[16,4096]{1,0} %broadcast.135), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1241 = f32[16,4096]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1242 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.1223, f32[16,4096]{1,0} %broadcast.1241), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1250 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %broadcast.133, f32[16,4096]{1,0} %multiply.1242), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.41 = f32[] constant(0)
  %multiply.73 = f32[] multiply(f32[] %constant.41, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.139 = f32[16,4096]{1,0} broadcast(f32[] %multiply.73), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1231 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.1223, f32[16,4096]{1,0} %multiply.1223), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1232 = f32[16,4096]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1233 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %multiply.1231, f32[16,4096]{1,0} %broadcast.1232), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1234 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %broadcast.139, f32[16,4096]{1,0} %multiply.1233), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1235 = f32[16,4096]{1,0} sqrt(f32[16,4096]{1,0} %add.1234), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1236 = f32[16,4096]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1237 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %sqrt.1235, f32[16,4096]{1,0} %broadcast.1236), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1252 = f32[16,4096]{1,0} divide(f32[16,4096]{1,0} %add.1250, f32[16,4096]{1,0} %add.1237), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1219 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1253 = f32[16,4096]{1,0} broadcast(f32[] %constant.1219), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1254 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %divide.1252, f32[16,4096]{1,0} %broadcast.1253), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1255 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %p55.1251, f32[16,4096]{1,0} %multiply.1254), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1214 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.1218 = f32[16,4096]{1,0} broadcast(f32[] %constant.1214), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1256 = f32[16,4096]{1,0} multiply(f32[16,4096]{1,0} %add.1255, f32[16,4096]{1,0} %broadcast.1218), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1257 = f32[16,4096]{1,0} add(f32[16,4096]{1,0} %add.1255, f32[16,4096]{1,0} %multiply.1256), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p56.1290 = f32[16]{0} parameter(56), frontend_attributes={neff_input_names="input56"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.42 = f32[] constant(0)
  %multiply.74 = f32[] multiply(f32[] %constant.42, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.143 = f32[16]{0} broadcast(f32[] %multiply.74), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.75 = f32[] multiply(f32[] %select.533, f32[] %constant.129), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.144 = f32[16]{0} broadcast(f32[] %multiply.75), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1262 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.128, f32[16]{0} %broadcast.144), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1280 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1281 = f32[16]{0} multiply(f32[16]{0} %multiply.1262, f32[16]{0} %broadcast.1280), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1289 = f32[16]{0} add(f32[16]{0} %broadcast.143, f32[16]{0} %multiply.1281), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.43 = f32[] constant(0)
  %multiply.77 = f32[] multiply(f32[] %constant.43, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.148 = f32[16]{0} broadcast(f32[] %multiply.77), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1270 = f32[16]{0} multiply(f32[16]{0} %multiply.1262, f32[16]{0} %multiply.1262), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1271 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1272 = f32[16]{0} multiply(f32[16]{0} %multiply.1270, f32[16]{0} %broadcast.1271), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1273 = f32[16]{0} add(f32[16]{0} %broadcast.148, f32[16]{0} %multiply.1272), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1274 = f32[16]{0} sqrt(f32[16]{0} %add.1273), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1275 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1276 = f32[16]{0} add(f32[16]{0} %sqrt.1274, f32[16]{0} %broadcast.1275), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1291 = f32[16]{0} divide(f32[16]{0} %add.1289, f32[16]{0} %add.1276), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1258 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1292 = f32[16]{0} broadcast(f32[] %constant.1258), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1293 = f32[16]{0} multiply(f32[16]{0} %divide.1291, f32[16]{0} %broadcast.1292), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1294 = f32[16]{0} add(f32[16]{0} %p56.1290, f32[16]{0} %multiply.1293), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p57.1327 = f32[16]{0} parameter(57), frontend_attributes={neff_input_names="input57"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.44 = f32[] constant(0)
  %multiply.78 = f32[] multiply(f32[] %constant.44, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.152 = f32[16]{0} broadcast(f32[] %multiply.78), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.79 = f32[] multiply(f32[] %select.533, f32[] %constant.125), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.153 = f32[16]{0} broadcast(f32[] %multiply.79), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1299 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.124, f32[16]{0} %broadcast.153), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1317 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1318 = f32[16]{0} multiply(f32[16]{0} %multiply.1299, f32[16]{0} %broadcast.1317), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1326 = f32[16]{0} add(f32[16]{0} %broadcast.152, f32[16]{0} %multiply.1318), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.45 = f32[] constant(0)
  %multiply.81 = f32[] multiply(f32[] %constant.45, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.157 = f32[16]{0} broadcast(f32[] %multiply.81), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1307 = f32[16]{0} multiply(f32[16]{0} %multiply.1299, f32[16]{0} %multiply.1299), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1308 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1309 = f32[16]{0} multiply(f32[16]{0} %multiply.1307, f32[16]{0} %broadcast.1308), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1310 = f32[16]{0} add(f32[16]{0} %broadcast.157, f32[16]{0} %multiply.1309), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1311 = f32[16]{0} sqrt(f32[16]{0} %add.1310), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1312 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1313 = f32[16]{0} add(f32[16]{0} %sqrt.1311, f32[16]{0} %broadcast.1312), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1328 = f32[16]{0} divide(f32[16]{0} %add.1326, f32[16]{0} %add.1313), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1295 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1329 = f32[16]{0} broadcast(f32[] %constant.1295), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1330 = f32[16]{0} multiply(f32[16]{0} %divide.1328, f32[16]{0} %broadcast.1329), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1331 = f32[16]{0} add(f32[16]{0} %p57.1327, f32[16]{0} %multiply.1330), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p58.1364 = f32[16]{0} parameter(58), frontend_attributes={neff_input_names="input58"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.46 = f32[] constant(0)
  %multiply.82 = f32[] multiply(f32[] %constant.46, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.161 = f32[16]{0} broadcast(f32[] %multiply.82), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.83 = f32[] multiply(f32[] %select.533, f32[] %constant.121), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.163 = f32[16]{0} broadcast(f32[] %multiply.83), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1336 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.120, f32[16]{0} %broadcast.163), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1354 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1355 = f32[16]{0} multiply(f32[16]{0} %multiply.1336, f32[16]{0} %broadcast.1354), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1363 = f32[16]{0} add(f32[16]{0} %broadcast.161, f32[16]{0} %multiply.1355), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.47 = f32[] constant(0)
  %multiply.85 = f32[] multiply(f32[] %constant.47, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.167 = f32[16]{0} broadcast(f32[] %multiply.85), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1344 = f32[16]{0} multiply(f32[16]{0} %multiply.1336, f32[16]{0} %multiply.1336), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1345 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1346 = f32[16]{0} multiply(f32[16]{0} %multiply.1344, f32[16]{0} %broadcast.1345), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1347 = f32[16]{0} add(f32[16]{0} %broadcast.167, f32[16]{0} %multiply.1346), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1348 = f32[16]{0} sqrt(f32[16]{0} %add.1347), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1349 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1350 = f32[16]{0} add(f32[16]{0} %sqrt.1348, f32[16]{0} %broadcast.1349), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1365 = f32[16]{0} divide(f32[16]{0} %add.1363, f32[16]{0} %add.1350), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1332 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1366 = f32[16]{0} broadcast(f32[] %constant.1332), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1367 = f32[16]{0} multiply(f32[16]{0} %divide.1365, f32[16]{0} %broadcast.1366), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1368 = f32[16]{0} add(f32[16]{0} %p58.1364, f32[16]{0} %multiply.1367), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p59.1406 = f32[16,16]{1,0} parameter(59), frontend_attributes={neff_input_names="input59"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.48 = f32[] constant(0)
  %multiply.86 = f32[] multiply(f32[] %constant.48, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.171 = f32[16,16]{1,0} broadcast(f32[] %multiply.86), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.87 = f32[] multiply(f32[] %select.533, f32[] %constant.177), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.172 = f32[16,16]{1,0} broadcast(f32[] %multiply.87), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1378 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.176, f32[16,16]{1,0} %broadcast.172), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1396 = f32[16,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1397 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1378, f32[16,16]{1,0} %broadcast.1396), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1405 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.171, f32[16,16]{1,0} %multiply.1397), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.49 = f32[] constant(0)
  %multiply.89 = f32[] multiply(f32[] %constant.49, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.176 = f32[16,16]{1,0} broadcast(f32[] %multiply.89), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1386 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1378, f32[16,16]{1,0} %multiply.1378), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1387 = f32[16,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1388 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1386, f32[16,16]{1,0} %broadcast.1387), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1389 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.176, f32[16,16]{1,0} %multiply.1388), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1390 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.1389), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1391 = f32[16,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1392 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.1390, f32[16,16]{1,0} %broadcast.1391), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1407 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.1405, f32[16,16]{1,0} %add.1392), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1374 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1408 = f32[16,16]{1,0} broadcast(f32[] %constant.1374), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1409 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.1407, f32[16,16]{1,0} %broadcast.1408), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1410 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p59.1406, f32[16,16]{1,0} %multiply.1409), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1369 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.1373 = f32[16,16]{1,0} broadcast(f32[] %constant.1369), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1411 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.1410, f32[16,16]{1,0} %broadcast.1373), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1412 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.1410, f32[16,16]{1,0} %multiply.1411), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p60.1445 = f32[16]{0} parameter(60), frontend_attributes={neff_input_names="input60"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.50 = f32[] constant(0)
  %multiply.90 = f32[] multiply(f32[] %constant.50, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.180 = f32[16]{0} broadcast(f32[] %multiply.90), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.91 = f32[] multiply(f32[] %select.533, f32[] %constant.117), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.181 = f32[16]{0} broadcast(f32[] %multiply.91), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1417 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.116, f32[16]{0} %broadcast.181), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1435 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1436 = f32[16]{0} multiply(f32[16]{0} %multiply.1417, f32[16]{0} %broadcast.1435), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1444 = f32[16]{0} add(f32[16]{0} %broadcast.180, f32[16]{0} %multiply.1436), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.51 = f32[] constant(0)
  %multiply.93 = f32[] multiply(f32[] %constant.51, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.185 = f32[16]{0} broadcast(f32[] %multiply.93), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1425 = f32[16]{0} multiply(f32[16]{0} %multiply.1417, f32[16]{0} %multiply.1417), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1426 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1427 = f32[16]{0} multiply(f32[16]{0} %multiply.1425, f32[16]{0} %broadcast.1426), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1428 = f32[16]{0} add(f32[16]{0} %broadcast.185, f32[16]{0} %multiply.1427), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1429 = f32[16]{0} sqrt(f32[16]{0} %add.1428), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1430 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1431 = f32[16]{0} add(f32[16]{0} %sqrt.1429, f32[16]{0} %broadcast.1430), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1446 = f32[16]{0} divide(f32[16]{0} %add.1444, f32[16]{0} %add.1431), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1413 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1447 = f32[16]{0} broadcast(f32[] %constant.1413), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1448 = f32[16]{0} multiply(f32[16]{0} %divide.1446, f32[16]{0} %broadcast.1447), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1449 = f32[16]{0} add(f32[16]{0} %p60.1445, f32[16]{0} %multiply.1448), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p61.1487 = f32[16,16]{1,0} parameter(61), frontend_attributes={neff_input_names="input61"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.52 = f32[] constant(0)
  %multiply.94 = f32[] multiply(f32[] %constant.52, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.189 = f32[16,16]{1,0} broadcast(f32[] %multiply.94), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.95 = f32[] multiply(f32[] %select.533, f32[] %constant.173), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.191 = f32[16,16]{1,0} broadcast(f32[] %multiply.95), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1459 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %get-tuple-element.172, f32[16,16]{1,0} %broadcast.191), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1477 = f32[16,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1478 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1459, f32[16,16]{1,0} %broadcast.1477), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1486 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.189, f32[16,16]{1,0} %multiply.1478), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.53 = f32[] constant(0)
  %multiply.97 = f32[] multiply(f32[] %constant.53, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.195 = f32[16,16]{1,0} broadcast(f32[] %multiply.97), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1467 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1459, f32[16,16]{1,0} %multiply.1459), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1468 = f32[16,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1469 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %multiply.1467, f32[16,16]{1,0} %broadcast.1468), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1470 = f32[16,16]{1,0} add(f32[16,16]{1,0} %broadcast.195, f32[16,16]{1,0} %multiply.1469), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1471 = f32[16,16]{1,0} sqrt(f32[16,16]{1,0} %add.1470), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1472 = f32[16,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1473 = f32[16,16]{1,0} add(f32[16,16]{1,0} %sqrt.1471, f32[16,16]{1,0} %broadcast.1472), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1488 = f32[16,16]{1,0} divide(f32[16,16]{1,0} %add.1486, f32[16,16]{1,0} %add.1473), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1455 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1489 = f32[16,16]{1,0} broadcast(f32[] %constant.1455), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1490 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %divide.1488, f32[16,16]{1,0} %broadcast.1489), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1491 = f32[16,16]{1,0} add(f32[16,16]{1,0} %p61.1487, f32[16,16]{1,0} %multiply.1490), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1450 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.1454 = f32[16,16]{1,0} broadcast(f32[] %constant.1450), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1492 = f32[16,16]{1,0} multiply(f32[16,16]{1,0} %add.1491, f32[16,16]{1,0} %broadcast.1454), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1493 = f32[16,16]{1,0} add(f32[16,16]{1,0} %add.1491, f32[16,16]{1,0} %multiply.1492), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p62.1526 = f32[16]{0} parameter(62), frontend_attributes={neff_input_names="input62"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.54 = f32[] constant(0)
  %multiply.98 = f32[] multiply(f32[] %constant.54, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.199 = f32[16]{0} broadcast(f32[] %multiply.98), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.100 = f32[] multiply(f32[] %select.533, f32[] %constant.109), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.200 = f32[16]{0} broadcast(f32[] %multiply.100), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1498 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.108, f32[16]{0} %broadcast.200), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1516 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1517 = f32[16]{0} multiply(f32[16]{0} %multiply.1498, f32[16]{0} %broadcast.1516), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1525 = f32[16]{0} add(f32[16]{0} %broadcast.199, f32[16]{0} %multiply.1517), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.55 = f32[] constant(0)
  %multiply.102 = f32[] multiply(f32[] %constant.55, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.204 = f32[16]{0} broadcast(f32[] %multiply.102), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1506 = f32[16]{0} multiply(f32[16]{0} %multiply.1498, f32[16]{0} %multiply.1498), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1507 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1508 = f32[16]{0} multiply(f32[16]{0} %multiply.1506, f32[16]{0} %broadcast.1507), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1509 = f32[16]{0} add(f32[16]{0} %broadcast.204, f32[16]{0} %multiply.1508), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1510 = f32[16]{0} sqrt(f32[16]{0} %add.1509), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1511 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1512 = f32[16]{0} add(f32[16]{0} %sqrt.1510, f32[16]{0} %broadcast.1511), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1527 = f32[16]{0} divide(f32[16]{0} %add.1525, f32[16]{0} %add.1512), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1494 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1528 = f32[16]{0} broadcast(f32[] %constant.1494), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1529 = f32[16]{0} multiply(f32[16]{0} %divide.1527, f32[16]{0} %broadcast.1528), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1530 = f32[16]{0} add(f32[16]{0} %p62.1526, f32[16]{0} %multiply.1529), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p63.1563 = f32[16]{0} parameter(63), frontend_attributes={neff_input_names="input63"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.56 = f32[] constant(0)
  %multiply.104 = f32[] multiply(f32[] %constant.56, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.208 = f32[16]{0} broadcast(f32[] %multiply.104), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.105 = f32[] multiply(f32[] %select.533, f32[] %constant.105), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.209 = f32[16]{0} broadcast(f32[] %multiply.105), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1535 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.104, f32[16]{0} %broadcast.209), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1553 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1554 = f32[16]{0} multiply(f32[16]{0} %multiply.1535, f32[16]{0} %broadcast.1553), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1562 = f32[16]{0} add(f32[16]{0} %broadcast.208, f32[16]{0} %multiply.1554), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.57 = f32[] constant(0)
  %multiply.108 = f32[] multiply(f32[] %constant.57, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.213 = f32[16]{0} broadcast(f32[] %multiply.108), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1543 = f32[16]{0} multiply(f32[16]{0} %multiply.1535, f32[16]{0} %multiply.1535), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1544 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1545 = f32[16]{0} multiply(f32[16]{0} %multiply.1543, f32[16]{0} %broadcast.1544), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1546 = f32[16]{0} add(f32[16]{0} %broadcast.213, f32[16]{0} %multiply.1545), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1547 = f32[16]{0} sqrt(f32[16]{0} %add.1546), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1548 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1549 = f32[16]{0} add(f32[16]{0} %sqrt.1547, f32[16]{0} %broadcast.1548), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1564 = f32[16]{0} divide(f32[16]{0} %add.1562, f32[16]{0} %add.1549), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1531 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1565 = f32[16]{0} broadcast(f32[] %constant.1531), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1566 = f32[16]{0} multiply(f32[16]{0} %divide.1564, f32[16]{0} %broadcast.1565), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1567 = f32[16]{0} add(f32[16]{0} %p63.1563, f32[16]{0} %multiply.1566), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p64.1600 = f32[16]{0} parameter(64), frontend_attributes={neff_input_names="input64"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.58 = f32[] constant(0)
  %multiply.109 = f32[] multiply(f32[] %constant.58, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.217 = f32[16]{0} broadcast(f32[] %multiply.109), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.110 = f32[] multiply(f32[] %select.533, f32[] %constant.101), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.218 = f32[16]{0} broadcast(f32[] %multiply.110), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1572 = f32[16]{0} multiply(f32[16]{0} %get-tuple-element.100, f32[16]{0} %broadcast.218), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1590 = f32[16]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1591 = f32[16]{0} multiply(f32[16]{0} %multiply.1572, f32[16]{0} %broadcast.1590), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1599 = f32[16]{0} add(f32[16]{0} %broadcast.217, f32[16]{0} %multiply.1591), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.59 = f32[] constant(0)
  %multiply.113 = f32[] multiply(f32[] %constant.59, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.221 = f32[16]{0} broadcast(f32[] %multiply.113), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1580 = f32[16]{0} multiply(f32[16]{0} %multiply.1572, f32[16]{0} %multiply.1572), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1581 = f32[16]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1582 = f32[16]{0} multiply(f32[16]{0} %multiply.1580, f32[16]{0} %broadcast.1581), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1583 = f32[16]{0} add(f32[16]{0} %broadcast.221, f32[16]{0} %multiply.1582), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1584 = f32[16]{0} sqrt(f32[16]{0} %add.1583), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1585 = f32[16]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1586 = f32[16]{0} add(f32[16]{0} %sqrt.1584, f32[16]{0} %broadcast.1585), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1601 = f32[16]{0} divide(f32[16]{0} %add.1599, f32[16]{0} %add.1586), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1568 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1602 = f32[16]{0} broadcast(f32[] %constant.1568), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1603 = f32[16]{0} multiply(f32[16]{0} %divide.1601, f32[16]{0} %broadcast.1602), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1604 = f32[16]{0} add(f32[16]{0} %p64.1600, f32[16]{0} %multiply.1603), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p65.1637 = f32[30522]{0} parameter(65), frontend_attributes={neff_input_names="input65"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.60 = f32[] constant(0)
  %multiply.114 = f32[] multiply(f32[] %constant.60, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.224 = f32[30522]{0} broadcast(f32[] %multiply.114), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.116 = f32[] multiply(f32[] %select.533, f32[] %constant.113), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.225 = f32[30522]{0} broadcast(f32[] %multiply.116), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1609 = f32[30522]{0} multiply(f32[30522]{0} %get-tuple-element.112, f32[30522]{0} %broadcast.225), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1627 = f32[30522]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1628 = f32[30522]{0} multiply(f32[30522]{0} %multiply.1609, f32[30522]{0} %broadcast.1627), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1636 = f32[30522]{0} add(f32[30522]{0} %broadcast.224, f32[30522]{0} %multiply.1628), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.61 = f32[] constant(0)
  %multiply.118 = f32[] multiply(f32[] %constant.61, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.228 = f32[30522]{0} broadcast(f32[] %multiply.118), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1617 = f32[30522]{0} multiply(f32[30522]{0} %multiply.1609, f32[30522]{0} %multiply.1609), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1618 = f32[30522]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1619 = f32[30522]{0} multiply(f32[30522]{0} %multiply.1617, f32[30522]{0} %broadcast.1618), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1620 = f32[30522]{0} add(f32[30522]{0} %broadcast.228, f32[30522]{0} %multiply.1619), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1621 = f32[30522]{0} sqrt(f32[30522]{0} %add.1620), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1622 = f32[30522]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1623 = f32[30522]{0} add(f32[30522]{0} %sqrt.1621, f32[30522]{0} %broadcast.1622), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1638 = f32[30522]{0} divide(f32[30522]{0} %add.1636, f32[30522]{0} %add.1623), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1605 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1639 = f32[30522]{0} broadcast(f32[] %constant.1605), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1640 = f32[30522]{0} multiply(f32[30522]{0} %divide.1638, f32[30522]{0} %broadcast.1639), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1641 = f32[30522]{0} add(f32[30522]{0} %p65.1637, f32[30522]{0} %multiply.1640), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %p66.1679 = f32[2,16]{1,0} parameter(66), frontend_attributes={neff_input_names="input66"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.62 = f32[] constant(0)
  %multiply.120 = f32[] multiply(f32[] %constant.62, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.231 = f32[2,16]{1,0} broadcast(f32[] %multiply.120), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.121 = f32[] multiply(f32[] %select.533, f32[] %constant.169), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.232 = f32[2,16]{1,0} broadcast(f32[] %multiply.121), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1651 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %get-tuple-element.168, f32[2,16]{1,0} %broadcast.232), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1669 = f32[2,16]{1,0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1670 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.1651, f32[2,16]{1,0} %broadcast.1669), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1678 = f32[2,16]{1,0} add(f32[2,16]{1,0} %broadcast.231, f32[2,16]{1,0} %multiply.1670), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.63 = f32[] constant(0)
  %multiply.124 = f32[] multiply(f32[] %constant.63, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.235 = f32[2,16]{1,0} broadcast(f32[] %multiply.124), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1659 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.1651, f32[2,16]{1,0} %multiply.1651), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1660 = f32[2,16]{1,0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1661 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %multiply.1659, f32[2,16]{1,0} %broadcast.1660), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1662 = f32[2,16]{1,0} add(f32[2,16]{1,0} %broadcast.235, f32[2,16]{1,0} %multiply.1661), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1663 = f32[2,16]{1,0} sqrt(f32[2,16]{1,0} %add.1662), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1664 = f32[2,16]{1,0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1665 = f32[2,16]{1,0} add(f32[2,16]{1,0} %sqrt.1663, f32[2,16]{1,0} %broadcast.1664), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1680 = f32[2,16]{1,0} divide(f32[2,16]{1,0} %add.1678, f32[2,16]{1,0} %add.1665), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1647 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1681 = f32[2,16]{1,0} broadcast(f32[] %constant.1647), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1682 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %divide.1680, f32[2,16]{1,0} %broadcast.1681), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1683 = f32[2,16]{1,0} add(f32[2,16]{1,0} %p66.1679, f32[2,16]{1,0} %multiply.1682), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1642 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %broadcast.1646 = f32[2,16]{1,0} broadcast(f32[] %constant.1642), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %multiply.1684 = f32[2,16]{1,0} multiply(f32[2,16]{1,0} %add.1683, f32[2,16]{1,0} %broadcast.1646), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %add.1685 = f32[2,16]{1,0} add(f32[2,16]{1,0} %add.1683, f32[2,16]{1,0} %multiply.1684), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=145}
  %p67.1718 = f32[2]{0} parameter(67), frontend_attributes={neff_input_names="input67"}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/core/xla_op_registry.py" source_line=44}
  %constant.64 = f32[] constant(0)
  %multiply.125 = f32[] multiply(f32[] %constant.64, f32[] %p37.557), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %broadcast.238 = f32[2]{0} broadcast(f32[] %multiply.125), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=113}
  %multiply.126 = f32[] multiply(f32[] %select.533, f32[] %constant.97), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.239 = f32[2]{0} broadcast(f32[] %multiply.126), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %multiply.1690 = f32[2]{0} multiply(f32[2]{0} %get-tuple-element.96, f32[2]{0} %broadcast.239), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/xla_modified/pytorch/xla/torch_xla/_patched_functions.py" source_line=53}
  %broadcast.1708 = f32[2]{0} broadcast(f32[] %p36.551), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %multiply.1709 = f32[2]{0} multiply(f32[2]{0} %multiply.1690, f32[2]{0} %broadcast.1708), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %add.1717 = f32[2]{0} add(f32[2]{0} %broadcast.238, f32[2]{0} %multiply.1709), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=124}
  %constant.65 = f32[] constant(0)
  %multiply.129 = f32[] multiply(f32[] %constant.65, f32[] %p35.536), metadata={op_type="aten__mul" op_name="aten__mul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.242 = f32[2]{0} broadcast(f32[] %multiply.129), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=115}
  %multiply.1698 = f32[2]{0} multiply(f32[2]{0} %multiply.1690, f32[2]{0} %multiply.1690), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %broadcast.1699 = f32[2]{0} broadcast(f32[] %p1.10), dimensions={}, metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %multiply.1700 = f32[2]{0} multiply(f32[2]{0} %multiply.1698, f32[2]{0} %broadcast.1699), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %add.1701 = f32[2]{0} add(f32[2]{0} %broadcast.242, f32[2]{0} %multiply.1700), metadata={op_type="aten__addcmul" op_name="aten__addcmul" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=125}
  %sqrt.1702 = f32[2]{0} sqrt(f32[2]{0} %add.1701), metadata={op_type="aten__sqrt" op_name="aten__sqrt" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %broadcast.1703 = f32[2]{0} broadcast(f32[] %p0.8), dimensions={}, metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %add.1704 = f32[2]{0} add(f32[2]{0} %sqrt.1702, f32[2]{0} %broadcast.1703), metadata={op_type="aten__add" op_name="aten__add" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=126}
  %divide.1719 = f32[2]{0} divide(f32[2]{0} %add.1717, f32[2]{0} %add.1704), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.1686 = f32[] constant(-0), metadata={op_type="prim__Constant" op_name="prim__Constant" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %broadcast.1720 = f32[2]{0} broadcast(f32[] %constant.1686), dimensions={}, metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %multiply.1721 = f32[2]{0} multiply(f32[2]{0} %divide.1719, f32[2]{0} %broadcast.1720), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %add.1722 = f32[2]{0} add(f32[2]{0} %p67.1718, f32[2]{0} %multiply.1721), metadata={op_type="aten__addcdiv" op_name="aten__addcdiv" source_file="/home/ubuntu/kahfi/dp_bert_hf_pretrain/adamw_fp32_optim_params.py" source_line=134}
  %constant.66 = f32[1]{0} constant({0})
  %get-tuple-element.27 = f32[1]{0} get-tuple-element((f32[1]{0}, f32[]) %all-reduce.26), index=0, metadata={op_type="xla__cross_replica_sum" op_name="xla__cross_replica_sum" source_file="/home/ubuntu/kahfi/pytorch/torch/_ops.py" source_line=692}
  %constant.67 = f32[1]{0} constant({0})
  ROOT %tuple.1729 = (f32[30522,16]{1,0}, f32[512,16]{1,0}, f32[2,16]{1,0}, f32[16]{0}, f32[16]{0}, /*index=5*/f32[16,16]{1,0}, f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, f32[16,16]{1,0}, /*index=10*/f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=15*/f32[4096,16]{1,0}, f32[4096]{0}, f32[16,4096]{1,0}, f32[16]{0}, f32[16]{0}, /*index=20*/f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, f32[16,16]{1,0}, f32[16]{0}, /*index=25*/f32[16]{0}, f32[16]{0}, f32[30522]{0}, f32[2,16]{1,0}, f32[2]{0}, /*index=30*/f32[1]{0}, f32[1]{0}, f32[1]{0}, f32[30522,16]{1,0}, f32[30522,16]{1,0}, /*index=35*/f32[512,16]{1,0}, f32[512,16]{1,0}, f32[2,16]{1,0}, f32[2,16]{1,0}, f32[16,16]{1,0}, /*index=40*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=45*/f32[16,16]{1,0}, f32[16,16]{1,0}, f32[4096,16]{1,0}, f32[4096,16]{1,0}, f32[16,4096]{1,0}, /*index=50*/f32[16,4096]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, f32[16,16]{1,0}, /*index=55*/f32[2,16]{1,0}, f32[2,16]{1,0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=60*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=65*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=70*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[4096]{0}, f32[4096]{0}, /*index=75*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=80*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[30522]{0}, f32[30522]{0}, /*index=85*/f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, f32[16]{0}, /*index=90*/f32[16]{0}, f32[2]{0}, f32[2]{0}, f32[1]{0}) tuple(f32[30522,16]{1,0} %add.572, f32[512,16]{1,0} %add.616, f32[2,16]{1,0} %add.660, f32[16]{0} %add.697, f32[16]{0} %add.734, /*index=5*/f32[16,16]{1,0} %add.778, f32[16]{0} %add.815, f32[16,16]{1,0} %add.859, f32[16]{0} %add.896, f32[16,16]{1,0} %add.940, /*index=10*/f32[16]{0} %add.977, f32[16,16]{1,0} %add.1021, f32[16]{0} %add.1058, f32[16]{0} %add.1095, f32[16]{0} %add.1132, /*index=15*/f32[4096,16]{1,0} %add.1176, f32[4096]{0} %add.1213, f32[16,4096]{1,0} %add.1257, f32[16]{0} %add.1294, f32[16]{0} %add.1331, /*index=20*/f32[16]{0} %add.1368, f32[16,16]{1,0} %add.1412, f32[16]{0} %add.1449, f32[16,16]{1,0} %add.1493, f32[16]{0} %add.1530, /*index=25*/f32[16]{0} %add.1567, f32[16]{0} %add.1604, f32[30522]{0} %add.1641, f32[2,16]{1,0} %add.1685, f32[2]{0} %add.1722, /*index=30*/f32[1]{0} %constant.66, f32[1]{0} %divide.18, f32[1]{0} %get-tuple-element.27, f32[30522,16]{1,0} %add.565, f32[30522,16]{1,0} %add.547, /*index=35*/f32[512,16]{1,0} %add.609, f32[512,16]{1,0} %add.593, f32[2,16]{1,0} %add.653, f32[2,16]{1,0} %add.637, f32[16,16]{1,0} %add.771, /*index=40*/f32[16,16]{1,0} %add.755, f32[16,16]{1,0} %add.852, f32[16,16]{1,0} %add.836, f32[16,16]{1,0} %add.933, f32[16,16]{1,0} %add.917, /*index=45*/f32[16,16]{1,0} %add.1014, f32[16,16]{1,0} %add.998, f32[4096,16]{1,0} %add.1169, f32[4096,16]{1,0} %add.1153, f32[16,4096]{1,0} %add.1250, /*index=50*/f32[16,4096]{1,0} %add.1234, f32[16,16]{1,0} %add.1405, f32[16,16]{1,0} %add.1389, f32[16,16]{1,0} %add.1486, f32[16,16]{1,0} %add.1470, /*index=55*/f32[2,16]{1,0} %add.1678, f32[2,16]{1,0} %add.1662, f32[16]{0} %add.692, f32[16]{0} %add.676, f32[16]{0} %add.729, /*index=60*/f32[16]{0} %add.713, f32[16]{0} %add.810, f32[16]{0} %add.794, f32[16]{0} %add.891, f32[16]{0} %add.875, /*index=65*/f32[16]{0} %add.972, f32[16]{0} %add.956, f32[16]{0} %add.1053, f32[16]{0} %add.1037, f32[16]{0} %add.1090, /*index=70*/f32[16]{0} %add.1074, f32[16]{0} %add.1127, f32[16]{0} %add.1111, f32[4096]{0} %add.1208, f32[4096]{0} %add.1192, /*index=75*/f32[16]{0} %add.1289, f32[16]{0} %add.1273, f32[16]{0} %add.1326, f32[16]{0} %add.1310, f32[16]{0} %add.1363, /*index=80*/f32[16]{0} %add.1347, f32[16]{0} %add.1444, f32[16]{0} %add.1428, f32[30522]{0} %add.1636, f32[30522]{0} %add.1620, /*index=85*/f32[16]{0} %add.1525, f32[16]{0} %add.1509, f32[16]{0} %add.1562, f32[16]{0} %add.1546, f32[16]{0} %add.1599, /*index=90*/f32[16]{0} %add.1583, f32[2]{0} %add.1717, f32[2]{0} %add.1701, f32[1]{0} %constant.67), frontend_attributes={neff_output_names="output0,output1,output2,output3,output4,output5,output6,output7,output8,output9,output10,output11,output12,output13,output14,output15,output16,output17,output18,output19,output20,output21,output22,output23,output24,output25,output26,output27,output28,output29,output30,output31,output32,output33,output34,output35,output36,output37,output38,output39,output40,output41,output42,output43,output44,output45,output46,output47,output48,output49,output50,output51,output52,output53,output54,output55,output56,output57,output58,output59,output60,output61,output62,output63,output64,output65,output66,output67,output68,output69,output70,output71,output72,output73,output74,output75,output76,output77,output78,output79,output80,output81,output82,output83,output84,output85,output86,output87,output88,output89,output90,output91,output92,output93"}
}


`

export default text;
